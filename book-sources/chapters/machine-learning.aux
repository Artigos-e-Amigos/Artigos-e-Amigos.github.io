\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Basics of Machine Learning}{193}{chapter.12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Unsupervised Learning}{193}{section.12.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}Dimensionality Reduction and PCA}{193}{subsection.12.1.1}}
\@writefile{toc}{\contentsline {paragraph}{Presentation of the method.}{193}{section*.112}}
\newlabel{eq-emp-cov}{{12.1}{193}{Presentation of the method}{equation.12.1.1}{}}
\newlabel{eq-cov-approx}{{12.2}{193}{Presentation of the method}{equation.12.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces  Empirical covariance of the data and its associated singular values. }}{194}{figure.12.1}}
\newlabel{fig-cov}{{12.1}{194}{Empirical covariance of the data and its associated singular values}{figure.12.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces PCA main axes capture variance}}{194}{figure.12.2}}
\newlabel{eq-pca-1}{{12.3}{194}{Presentation of the method}{equation.12.1.3}{}}
\newlabel{eq-pca-2}{{12.4}{194}{Presentation of the method}{equation.12.1.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimality analysis.}{194}{section*.113}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces  2-D and 3-D PCA vizualization of the input clouds. }}{195}{figure.12.3}}
\newlabel{fig-pca}{{12.3}{195}{2-D and 3-D PCA vizualization of the input clouds}{figure.12.3}{}}
\newlabel{eq-pca-nonconvex}{{12.5}{195}{Optimality analysis}{equation.12.1.5}{}}
\newlabel{thm-pca-optim}{{20}{195}{}{thm.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces  Left: proof of the rightmost inequality in\nobreakspace  {}\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq-variational-pca}\unskip \@@italiccorr )}}. Middle: matrix $B$, right: matrix $\mathaccentV {tilde}07EB$. }}{196}{figure.12.4}}
\newlabel{fig-pca-var-proof}{{12.4}{196}{Left: proof of the rightmost inequality in~\eqref {eq-variational-pca}. Middle: matrix $B$, right: matrix $\tilde B$}{figure.12.4}{}}
\newlabel{lem-upper-bound-pca}{{9}{196}{}{lem.9}{}}
\newlabel{eq-variational-pca}{{12.6}{196}{}{equation.12.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}Clustering and $k$-means}{197}{subsection.12.1.2}}
\@writefile{toc}{\contentsline {paragraph}{$k$-means}{197}{section*.114}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.5}{\ignorespaces $k$-means clusters according to Vornoi cells.}}{197}{figure.12.5}}
\newlabel{eq-kmeans-1}{{12.7}{197}{$k$-means}{equation.12.1.7}{}}
\newlabel{eq-kmeans-2}{{12.8}{197}{$k$-means}{equation.12.1.8}{}}
\@writefile{toc}{\contentsline {paragraph}{$k$-means++}{197}{section*.115}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces  Left: iteration of $k$-means algorithm. Right: histogram of points belonging to each class after the $k$-means optimization. }}{198}{figure.12.6}}
\newlabel{fig-kmeans}{{12.6}{198}{Left: iteration of $k$-means algorithm. Right: histogram of points belonging to each class after the $k$-means optimization}{figure.12.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Lloyd algorithm and continuous densities.}{198}{section*.116}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Empirical Risk Minimization}{198}{section.12.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.7}{\ignorespaces  Iteration of $k$-means algorithm (Lloyd algorithm) on continuous densities $\mu $. Top: uniform. Bottom: non-uniform (the densities of $\mu $ with respect to the Lebesgue measure is displayed as a grayscale image in the background). }}{199}{figure.12.7}}
\newlabel{fig-lloyd}{{12.7}{199}{Iteration of $k$-means algorithm (Lloyd algorithm) on continuous densities $\mu $. Top: uniform. Bottom: non-uniform (the densities of $\mu $ with respect to the Lebesgue measure is displayed as a grayscale image in the background)}{figure.12.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Empirical Risk}{199}{subsection.12.2.1}}
\newlabel{eq-erm-1}{{12.9}{199}{Empirical Risk}{equation.12.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Prediction and Consistency}{199}{subsection.12.2.2}}
\newlabel{eq-consistency-estim}{{12.10}{199}{Prediction and Consistency}{equation.12.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}Parametric Approaches and Regularization}{200}{subsection.12.2.3}}
\newlabel{eq-erm-param}{{12.11}{200}{Parametric Approaches and Regularization}{equation.12.2.11}{}}
\newlabel{eq-consistency-param}{{12.12}{200}{Parametric Approaches and Regularization}{equation.12.2.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Prediction vs. estimation risks.}{200}{section*.117}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.4}Testing Set and Cross-validation}{200}{subsection.12.2.4}}
\newlabel{eq-valid-risk}{{12.13}{200}{Testing Set and Cross-validation}{equation.12.2.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Supervised Learning: Regression}{200}{section.12.3}}
\newlabel{sec-regression}{{12.3}{200}{Supervised Learning: Regression}{section.12.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.9}{\ignorespaces  Conditional expectation. }}{201}{figure.12.9}}
\newlabel{fig-bound-regul}{{12.9}{201}{Conditional expectation}{figure.12.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.8}{\ignorespaces Probabilistic modelling.}}{201}{figure.12.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}Linear Regression}{201}{subsection.12.3.1}}
\newlabel{sec-linear-models}{{12.3.1}{201}{Linear Regression}{subsection.12.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Least square and conditional expectation.}{201}{section*.118}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.10}{\ignorespaces Linear regression.}}{202}{figure.12.10}}
\@writefile{toc}{\contentsline {paragraph}{Penalized linear models.}{202}{figure.12.10}}
\newlabel{eq-erm-lin}{{12.14}{202}{Penalized linear models}{equation.12.3.14}{}}
\newlabel{eq-empirical-conver}{{12.15}{202}{Penalized linear models}{equation.12.3.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Ridge regression (quadratic penalization).}{203}{section*.120}}
\newlabel{eq-linest-std}{{12.16}{203}{Ridge regression (quadratic penalization)}{equation.12.3.16}{}}
\newlabel{eq-linest-woodbury}{{12.17}{203}{Ridge regression (quadratic penalization)}{equation.12.3.17}{}}
\newlabel{eq-sc-stat}{{12.18}{203}{}{equation.12.3.18}{}}
\newlabel{eq-rate-estim}{{12.19}{203}{}{equation.12.3.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Supervised Learning: Classification}{203}{section.12.4}}
\newlabel{sec-classif}{{12.4}{203}{Supervised Learning: Classification}{section.12.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.1}Nearest Neighbors Classification}{203}{subsection.12.4.1}}
\newlabel{sec-nn-classif}{{12.4.1}{203}{Nearest Neighbors Classification}{subsection.12.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.11}{\ignorespaces Nearest neighbors.}}{203}{figure.12.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.12}{\ignorespaces  $k$-nearest-neighbor classification boundary function. }}{204}{figure.12.12}}
\newlabel{fig-hist-classif}{{12.12}{204}{$k$-nearest-neighbor classification boundary function}{figure.12.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.2}Two Classes Logistic Classification}{204}{subsection.12.4.2}}
\newlabel{sec-two-class-logit}{{12.4.2}{204}{Two Classes Logistic Classification}{subsection.12.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Approximate risk minimization.}{204}{section*.121}}
\newlabel{eq-ideal-classif}{{12.20}{204}{Approximate risk minimization}{equation.12.4.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.13}{\ignorespaces  1-D and 2-D logistic classification, showing the impact of $|\tmspace  -\thinmuskip {.1667em}| \beta  |\tmspace  -\thinmuskip {.1667em}|$ on the sharpness of the classification boundary. }}{205}{figure.12.13}}
\newlabel{fig-losses}{{12.13}{205}{1-D and 2-D logistic classification, showing the impact of $\norm {\be }$ on the sharpness of the classification boundary}{figure.12.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Logistic loss probabilistic interpretation.}{205}{section*.122}}
\newlabel{eq-two-class-logit-model}{{12.21}{205}{Logistic loss probabilistic interpretation}{equation.12.4.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.14}{\ignorespaces  Comparison of loss functions. \textbf  {\leavevmode {\color  {red}[ToDo: Re-do the figure, it is not correct, they should upper bound $\ell _0$]}} }}{206}{figure.12.14}}
\newlabel{fig-losses}{{12.14}{206}{Comparison of loss functions. \todo {Re-do the figure, it is not correct, they should upper bound $\ell _0$}}{figure.12.14}{}}
\newlabel{eq-logistic-optim}{{12.22}{206}{Logistic loss probabilistic interpretation}{equation.12.4.22}{}}
\newlabel{eq-logistic-loss}{{12.23}{206}{Logistic loss probabilistic interpretation}{equation.12.4.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Gradient descent method.}{206}{section*.123}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.15}{\ignorespaces  Influence on the separation distance between the class on the classification probability. }}{207}{figure.12.15}}
\newlabel{fig-separation-influ}{{12.15}{207}{Influence on the separation distance between the class on the classification probability}{figure.12.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.3}Multi-Classes Logistic Classification}{207}{subsection.12.4.3}}
\newlabel{sec-multiclass-logit}{{12.4.3}{207}{Multi-Classes Logistic Classification}{subsection.12.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.16}{\ignorespaces  2-D and 3-D PCA vizualization of the digits images. }}{208}{figure.12.16}}
\newlabel{fig-digits}{{12.16}{208}{2-D and 3-D PCA vizualization of the digits images}{figure.12.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.17}{\ignorespaces  Results of digit classification Left: probability $h(x)_\ell $ of belonging to each of the 9 first classes (displayed over a 2-D PCA space). Right: colors reflect probability $h(x)$ of belonging to classes. }}{208}{figure.12.17}}
\newlabel{fig-digits-classes}{{12.17}{208}{Results of digit classification Left: probability $h(x)_\ell $ of belonging to each of the 9 first classes (displayed over a 2-D PCA space). Right: colors reflect probability $h(x)$ of belonging to classes}{figure.12.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Kernel Methods}{208}{section.12.5}}
\newlabel{sec-kernel-methods}{{12.5}{208}{Kernel Methods}{section.12.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.1}Feature Map and Kernels}{209}{subsection.12.5.1}}
\newlabel{eq-kernel-generic}{{12.24}{209}{Feature Map and Kernels}{equation.12.5.24}{}}
\newlabel{eq-prediction-evaluation}{{12.25}{210}{Feature Map and Kernels}{equation.12.5.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.2}Kernel Design}{210}{subsection.12.5.2}}
\newlabel{eq-gauss-kernel}{{12.26}{210}{Kernel Design}{equation.12.5.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.18}{\ignorespaces  Regression using a Gaussian kernel. }}{211}{figure.12.18}}
\newlabel{fig-kernel}{{12.18}{211}{Regression using a Gaussian kernel}{figure.12.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Kernel on non-Euclidean spaces.}{211}{section*.124}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.3}General Case}{211}{subsection.12.5.3}}
\newlabel{eq-kernel-generic}{{12.27}{211}{}{equation.12.5.27}{}}
\newlabel{eq-rkhs-representer}{{12.28}{211}{}{equation.12.5.28}{}}
\newlabel{eq-rkhs-variational}{{12.29}{211}{}{equation.12.5.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.19}{\ignorespaces  Non-linear classification using a Gaussian kernel. }}{212}{figure.12.19}}
\newlabel{fig-classes-kernel}{{12.19}{212}{Non-linear classification using a Gaussian kernel}{figure.12.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.6}Probably approximately correct learning theory}{212}{section.12.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.1}Non parametric setup and calibration}{213}{subsection.12.6.1}}
\@writefile{toc}{\contentsline {paragraph}{Risk decomposition}{213}{section*.125}}
\@writefile{toc}{\contentsline {paragraph}{Calibration in the classification setup}{214}{section*.126}}
\newlabel{eq-calibration-control}{{12.30}{214}{Calibration in the classification setup}{equation.12.6.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.2}PAC bounds}{214}{subsection.12.6.2}}
\@writefile{toc}{\contentsline {paragraph}{Bias-variance decomposition.}{214}{section*.127}}
\newlabel{eq-bias-var-split}{{12.31}{214}{Bias-variance decomposition}{equation.12.6.31}{}}
\@writefile{toc}{\contentsline {paragraph}{Approximation error.}{214}{section*.128}}
\@writefile{toc}{\contentsline {paragraph}{Estimation error.}{215}{section*.129}}
\newlabel{eq-estimation-split}{{12.32}{215}{Estimation error}{equation.12.6.32}{}}
\newlabel{prop-mc-diarmid}{{40}{215}{McDiarmid control to the mean}{prop.40}{}}
\newlabel{eq-mc-diarmid}{{12.33}{215}{McDiarmid control to the mean}{equation.12.6.33}{}}
\newlabel{prop-control-rademacher}{{41}{215}{}{prop.41}{}}
\newlabel{eq-final-pac}{{12.34}{215}{}{equation.12.6.34}{}}
\newlabel{eq-svm-optim}{{12.35}{216}{Application to SVM classification}{equation.12.6.35}{}}
\newlabel{eq-kernel-method}{{12.36}{216}{Resolution using the kernel trick}{equation.12.6.36}{}}
\@setckpt{chapters/machine-learning}{
\setcounter{page}{217}
\setcounter{equation}{36}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{12}
\setcounter{section}{6}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{19}
\setcounter{table}{0}
\setcounter{Item}{3}
\setcounter{Hfootnote}{3}
\setcounter{bookmark@seq@number}{195}
\setcounter{parentequation}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{thm}{23}
\setcounter{prop}{41}
\setcounter{defn}{1}
\setcounter{cor}{0}
\setcounter{alg}{0}
\setcounter{lem}{9}
\setcounter{rem}{3}
\setcounter{exmp}{6}
\setcounter{float@type}{32}
\setcounter{listing}{0}
\setcounter{lstnumber}{1}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{AM@survey}{0}
\setcounter{section@level}{4}
\setcounter{lstlisting}{0}
}
