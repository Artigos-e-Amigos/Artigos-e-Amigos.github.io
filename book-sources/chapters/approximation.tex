% !TEX root = ../FundationsDataScience.tex

\chapter{Linear and Non-linear Approximation}
\label{chap-approx}

This chapter studies the theory of signal and image approximation, and gives an application to lossy compression. This theoritical analysis is performed for continuous functions $f \in \Ldeux([0,1]^d)$ for $d=1,2$. This analysis is important to studies the performance of compression, denoising, and super-resolution applications.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approximation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Approximation in an Ortho-basis}

We consider an orthogonal basis $\Bb = \{ \psi_m \}_m$ of $\Ldeux([0,1]^d)$, with for instance $d=1$ (signals) or $d=2$ (images).
We recall that the decomposition of a signal in an orthonormal basis
\eq{
	f = \sum_{m \in \ZZ} \dotp{f}{\psi_m} \psi_m
}
gives back the original signal and thus produces no error. Processing algorithms modify the coefficients $\dotp{f}{\psi_m}$ and introduce some error.  

The simplest processing computes an approximation by considering only a sub-set $I_M \subset \ZZ$ of $M$ coefficients and performing the reconstruction from this subset 
\eq{
	f_M \eqdef \sum_{m \in I_M} \dotp{f}{\psi_m} \psi_m, 
	\qwhereq M = |I_M|.
}
The reconstructed signal $f_M$ is the orthogonal projection of $f$ onto the space
\eq{
	V_M \eqdef \Span\enscond{\psi_m}{m \in I_M}.
}
Since $V_M$ might depend on $f$, this projection $f \mapsto f_M$ might be non-linear.

Since the basis is orthogonal, the approximation error is 
\eq{
	\norm{f-f_M}^2 = \sum_{m \notin I_M} |\dotp{f}{\psi_m}|^2.
}
The important question is now to choose the set $I_M$, which might depend on the signal $f$ itself. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear Approximation}

Linear approximation is obtained by fixing once for all $I_M$, and thus using the same set of coefficients for all $f$. The mapping $f \mapsto f_M$ is a linear orthogonal projection on $V_M$, and it satisfies
\eq{
	(f+g)_M = f_M + g_M
}
For the Fourier basis, one usually selects the low-frequency atoms
\eq{
	I_M = \{-M/2+1,\ldots,M/2\}.
}
For a 1-D wavelet basis, one usually selects the coarse wavelets
\eq{
	I_M= \enscond{m=(j,m)}{ j \geq j_0}
}
where $j_0$ is selected such that $|I_M|=M$.

\myfigure{
\tabtrois{
\image{approximation}{.32}{approximation-original}&
\image{approximation}{.32}{approximation-linear}&
\image{approximation}{.32}{approximation-non-linear}\\
Original $f$ & Linear $f_M^\ell$ & Non-linear $f_M^n$
}
}{%
	Linear versus non-linear wavelet approximation. %	
}{fig-approximation}

Figure \ref{fig-approximation}, center, shows an example of such a linear approximation with wavelets. Linear approximation tends to performs poorly near singularities, because they introduce some blurring.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Non-linear Approximation}

A non-linear approximation is obtained by choosing $I_M$ depending on $f$. In particular, one would like to choose $I_M$ to minimize the approximation error $\norm{f-f_M}$. Since the basis is orthogonal, this is achieved by selecting the $M$ largest coefficients in magnitude 
\eq{
	I_M= \{ M \text{ largest coefficients } |\dotp{f}{\psi_m}| \}.
}
This can be equivalently obtained using a thresholding 
\eq{
	I_M = \enscond{m}{ |\dotp{f}{\psi_m}|>T }
}
where $T$ depends on the number of coefficients $M$, 
\eq{
	M = \#\enscond{m}{|\dotp{f}{\psi_m}|>T}.
}

%%
\paragraph{Computation of the threshold.}

There is a bijective 1:1 mapping between $T$ and $M$ obtained by ordering the coefficient magnitudes $|\dotp{f}{\psi_m}|$ by decaying order, 
\eql{\label{eq-dfn-ordered-coefs}
	T = d_{M} \qwhereq
	\{ d_m \}_{m=0}^{N-1} = \{ |\dotp{f}{\psi_m}| \}_{0}^{N-1}
	\qandq
	d_m \geq d_{m+1}.
}
Figure \ref{fig-decay-coefs} shows this mapping between $M$ and $T$.

The following proposition show that the decay of the ordered coefficients is linked to the non-linear approximation decay.

\begin{prop}\label{prop-equiv-comp-decay}
One has
\eql{\label{eq-decay-coef-approx}
	d_m = O( m^{-\frac{\al+1}{2}} )
	\quad\Longleftrightarrow\quad
	\norm{f-f_M}^2 = O(M^{-\al}).
}
\end{prop}

\begin{proof}
One has
\eq{
	\norm{f-f_M}^2 = \sum_{m>M} d_m^2
}
and 
\eq{
	d_M^2 \leq \frac{2}{M} \sum_{m=M/2+1}^{M} d_m^2 \leq \frac{2}{M} \sum_{m > M/2} d_m^2 = \frac{2}{M} \norm{f-f_{M/2}}^2.
}
\end{proof}



\myfigure{
\image{approximation}{.3}{approximation-decay-coefs}
}{%
	Decay of the ordered coefficients and determination of the threshold for non-linear approxiation. %	
}{fig-decay-coefs}

%%
\paragraph{Hard thresholding.}
 
The non-linear approximation is re-written as
\eql{\label{eq-nonlinear-approx}
	f_M = \sum_{ |\dotp{f}{\psi_m}|>T } \dotp{f}{\psi_m} \psi_m
	=  \sum_m S_T(\dotp{f}{\psi_m}) \psi_m,
}
where 
\eql{\label{eq-hard-thresh}
	S_T(x) = 
	\choice{ 
		x \qifq |x|>T \\ 
		0 \qifq |x| \leq T 
	}
}
is the hard thresholding, that is displayed in Figure \ref{fig-thresholding-hard}. 

\myfigure{
\image{approximation}{.3}{thresholding-hard}
}{%
	Hard thresholding.%	
}{fig-thresholding-hard}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Signal and Image Modeling}
\label{sec-signal-models}

A signal model is a constraint $f \in \Theta$, where $\Theta \subset \Ldeux([0,1]^d)$ is a set of signals one is interested in.
Figure \ref{fig-approximation-class} shows different class of models for images, that we describe in the following paragraph.

\myfigure{
\tabquatre{
\image{approximation}{.24}{approximation-class-regular}&
\image{approximation}{.24}{approximation-class-bv}&
\image{approximation}{.24}{approximation-class-cartoon}&
\image{approximation}{.24}{approximation-class-natural}\\
Regular & Bounded variations & Cartoon & Natural
}
%\image{approximation}{.3}{approximation-class-pieceregular}
}{%
	Examples of different kinds of image models. %	
}{fig-approximation-class}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uniformly Smooth Signals and Images}
\label{subsec-smooth-class}

%%
\paragraph{Signals with derivatives.}

The simplest model is made of uniformly smooth signals, that have bounded derivatives
\eql{\label{eq-smooth-model} 
	\Th = \enscond{ f \in \Ldeux([0,1]^d) }{ \norm{f}_{\Cal} \leq C },
}
where $C>0$ is a fixed constant, and where in 1-D
\eq{
	\norm{f}_{\Cal} \eqdef \umax{k \leq \al} \normb{ \frac{\d^k f}{\d t^k} }_{\infty}.
}
This extends to higher dimensional signals by considering partial derivatives along each direction.

%%
\paragraph{Sobolev smooth signals and images.}

A smooth $\Cal$ signals in \eqref{eq-smooth-model} has derivatives with bounded energy, so that 
\eq{
	\frac{\d^\al f}{\d t^\al}(t) = f^{(\al)}(t) \in \Ldeux([0,1]). 
}	
Using the fact that 
\eq{
	\hat f^{(\al)}_m = (2i\pi m)^\al \hat f_m
}
where $\hat f$ is the Fourier coefficient defined in \eqref{eq-defn-fourier-coeffs}, except we are here on $\RR/\ZZ$ in place of $\RR/2\pi\ZZ$, 
\eq{
	\hat f_n \eqdef \int_0^1 e^{-2\imath\pi n x} f(x) \d x, 
}
one defines a so-called Sobolev functional 
\eql{\label{eq-sobolev-norm}
	\norm{f}_{\text{Sob}(\al)}^2 = \sum_{m \in \ZZ} |2\pi m|^{2\al} |\dotp{f}{e_m}|^2,
}
that satisfies $\norm{f}_{\text{Sob}(\al)} = \norm{f^{(\al)}}$ for smooth functions. This Sobolev functional is extended to signals that have derivatives in the sense of distribution in $\Ldeux([0,1])$.

This definition extends to distributions and signals $f \in \Ldeux([0,1]^d)$ of arbitrary dimension $d$ as
\eql{\label{eq-sobolev-norm-highdim}
	\norm{f}_{\text{Sob}(\al)}^2 = \sum_{m \in \ZZ^d} (2\pi \norm{m})^{2\al} |\dotp{f}{e_m}|^2,
}

The $\Cal$-Sobolev model
\eql{\label{eq-sobolev-model}
	\Th = \enscond{f \in \ldeux([0,1]^d)}{ \umax{k \leq \al} \norm{f}_{\text{Sob}(k)}^2 \leq C }
}
generalizes the $\Cal$ smooth image model \eqref{eq-smooth-model}.

Figure \ref{fig-signal-model-sobolev} shows images with an increasing Sobolev norm for $\al=2$.

\myfigure{
\image{approximation}{.5}{signal-model-sobolev}
}{%
	Images with increasing Sobolev norm.%	
}{fig-signal-model-sobolev}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Piecewise Regular Signals and Images}

%%
\paragraph{Piecewise smooth signals.}

Piecewise smooth signals in 1-D are functions $f \in \Ldeux([0,1])$ that are $\Cal$ smooth outside a set of less than $K$ pointwise discontinuities
\eql{\label{eq-model-piece-smooth}
	\Th = \enscond{f \in \Ldeux([0,1])}{ \exists \, (t_i)_{i=0}^{K-1}, \; 
	\norm{f_{(t_i,t_{i+1})}}_{\Cal} \leq C }
}
where $f_{(t_i,t_{i+1})}$ is the restriction of $f$ to the open interval $(t_i,t_{i+1})$.

%%
\paragraph{Piecewise smooth images.}

Piecewise smooth images are 2-D functions $f \in \Ldeux([0,1]^2)$ that are $\Cal$ regular outside a set of less than $K$ curves that have a finite perimeter
\eql{\label{eq-model-piece-smooth-2d}
	\Th = \enscond{f \in \Ldeux([0,1]^2)}{ \exists \, \Ga = (\ga_i)_{i=0}^{K-1}, \; 
	\norm{f}_{\Cal(\Ga^c)} \leq C_1 \qandq
	|\ga_i| \leq C_2
	 }	 
}
where $|\ga_i|$ is the length of the curve $\ga_i$ and where $\norm{f}_{\Cal(\Ga^c)}$ is the maximum norm of the derivatives of $f$ outside the set of curves $\Ga$. 

Segmentation methods such as the one proposed by Mumford and Shah \cite{mumford-shah} implicitly assume such a piecewise smooth image modem. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bounded Variation Signals and Images}
\label{subsec-bv-model}

Signals with edges are obtained by considering functions with bounded variations
\eql{\label{eq-model-tv}
	\Th = \enscond{f \in \Ldeux(\RR^d)}{ \normi{f} \leq C_1 \qandq \normTV{f} \leq C_2 }.
}
For $d=1$ and $d=2$, this model generalizes the model of piecewise smooth signals \eqref{eq-model-piece-smooth} and images \eqref{eq-model-piece-smooth-2d}.

The total variation of a smooth function is 
\eq{
	\int \norm{\nabla f(x)} \d x
} 
where 
\eq{
	\nabla f(x) = \pa{ \frac{\partial f}{\partial x_i}  }_{i=0}^{d-1} \in \RR^d
} 
is the gradient vector at $x$. The total variation is extended to discontinuous images, that might for instance exhibit jumps across singular curves (the edges of the image). In particular, the total variation of a piecewise smooth image is the sum of the lengths of its level sets
\eql{\label{eq-tv-coaera}
	\normTV{f} = \int_{-\infty}^\infty |\Ll_t(f)| \d t < +\infty
	\qwhereq
	\Ll_t(f) = \enscond{x}{f(x)=t},
}	
and where $|\Ll_t(f)|$ is the length of $\Ll_t(f)$. For a set $\Om \subset \RR^2$ with finite perimeter $|\partial \Om|$, then
\eq{
	\normTV{1_\Om} = |\partial \Om|.
}
The model of bounded variation was introduced in image processing by Rudin, Osher and Fatemi \cite{rudin-tv}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cartoon Images}
\label{subsec-cartoon-images}

The bounded variation image model \eqref{eq-model-tv} does not constrain the smoothness of the level set curves $\Ll_t(f)$. Geometrical images have smooth contour curves, which should be taken into account to improve the result of processing methods. 

The model of $\Cal$ cartoon images is composed of 2-D functions that are $\Cal$ regular outside a set of less that $K$ regular edge curves $\ga_i$
\eql{\label{eq-cartoon-model}
	\Th = \enscond{f \in \Ldeux([0,1]^2)}{ \exists \, \Ga = (\ga_i)_{i=0}^{K-1}, \; 
	\norm{f}_{\Cal(\Ga^c)} \leq C_1 \qandq
	\norm{\ga_i}_{\Cal} \leq C_2
	 }
}
where each $\ga_i$ is a arc-length parameterization of the curve $\ga_i : [0,A] \mapsto [0,1]^2$.
Figure \ref{fig-image-model-cartoon} shows cartoon images with increasing total variation $\normTV{f}$.

\myfigure{
\image{approximation}{.5}{image-model-cartoon}
}{%
	Cartoon image with increasing total variation.%	
}{fig-image-model-cartoon}

Typical images might also be slightly blurred by optical diffraction, so that one might consider a blurred cartoon image model 
\eql{\label{eq-cartoon-model-smooth}
	\tilde\Th = \enscond{\tilde f = f \star h \in \Ldeux([0,1]^2)}{ f \in \Th \qandq h \in \Hh	}
}
where $\Th$ is the model of sharp (unblurred) images \eqref{eq-cartoon-model} and $\Hh$ is a set of constraints on the blurring kernel, for instance $h \geq 0$ should be smooth, localized in space and frequency.
This unknown blurring makes difficult brute force approaches that detects the edges location $\Ga$ and then process the regular parts in $[0,1]^2 \backslash \Ga$.

Figure \ref{fig-sample-cartoon} shows examples of images in $\Th$ and $\tilde\Th$.

\myfigure{
\image{approximation}{.25}{sample-cartoon-1}
\image{approximation}{.25}{sample-cartoon-2}
\image{approximation}{.25}{sample-cartoon-3}
}{%
	Examples of cartoon images: sharp discontinuities (left and center) and smooth discontinuities (right). %	
}{fig-sample-cartoon}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Efficient approximation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Decay of Approximation Error}
\label{sec-linear-approx-error}

To perform an efficient processing of signals or images in $\Th$, the goal is to design an orthogonal basis such that the non-linear approximation error $\norm{f-f_M}$ decays as fast as possible to $0$ when $M$ increases. 


%%
\paragraph{Polynomial error decay.}

This error decay measured using a power law 
\eql{\label{eq-approx-powerlaw}
	\foralls f \in \Th, \; \foralls M, \quad \norm{f-f_M}^2 \leq C_f M^{-\al}
}
where $\al$ is independent of $f$ and should be as large as possible. The parameter $\al$ depends on the basis and on $\Th$. It is a class-complexity parameter that describes the overall complexity of signals in $\Th$ with respect to the orthogonal basis one considers for approximation. The parameter $C_f$ depends on $f$ and describes the complexity of the signal $f$ within its class $\Th$.

%%
\paragraph{Relevance for compression, denoising and inverse problems.}

Monitoring the decay of approximation error is not only interesting from a mathematical point of view. Section \ref{sec-transform-coding} shows that the compression error is close to the non-linear approximation error. Bases that are efficient for approximation are thus also efficient for compression. 

Chapter \ref{chap-denoising} shows that a similar conclusion holds for non-linear denoising with thresholding. Efficient denoisers are obtained by performing a non-linear approximation of the noisy image in a well chosen basis. The average denoising error with respect to a random noise is closely related to the approximation error.

Chapter \ref{chap-sparsity} shows that ill-posed inverse problems such as super-resolution of missing information can be solved by taking advantage of the compressibility of the signal or the image in a well chosen basis. A basis that is efficient for approximation of the high resolution signal is needed to recover efficiently missing information. The performance of these schemes is difficult to analyze, and the basis atoms must also be far enough from the kernel of the operator that removes information.  



%%%%
\paragraph{Comparison of signals.}

For a fixed basis (for instance wavelets), the decay of $\norm{f-f_M}$ allows one to compare the complexity of different images.  Figure \ref{fig-approx-speed} shows that natural images with complicated geometric structures and textures are more difficult to approximate using wavelets.

Since the approximation error often decays in a power-low fashion \eqref{eq-approx-powerlaw}, the curves are displayed in a log-log plot, so that
\eq{
	\log(\norm{f-f_M}^2) = \text{cst} - \al \log(M)
}
and hence one should expect an affine curve with slope $-\al$. Due to discretization issue, this is only the case for value of $M \ll N$, since the error quickly drops to zero for $M \approx N$.


\myfigure{
\tabquatre{
\image{approximation}{.24}{approx-speed-1}&
\image{approximation}{.24}{approx-speed-2}&
\image{approximation}{.24}{approx-speed-3}&
\image{approximation}{.24}{approx-speed-4}\\
Smooth & Cartoon & Natural \#1 & Natural \#2
}
}{%
	Several different test images.%	
}{fig-approx-speed-1}



\myfigure{
\image{approximation}{.4}{approx-speed-curves}
}{%
	Comparison of approximation error decay in wavelets for different images shown in Figure \ref{fig-approx-speed-1}.%	
}{fig-approx-speed}

%%%
\subsection{Comparison of bases.}

For a given image $f$, one can compare different ortho-bases using the decay of $\norm{f-f_M}$. Figure \ref{fig-approx-effi} shows the efficiency of several bases to approximate a fixed natural image with contours and textures. The Fourier basis described in Section \ref{sec-2d-fourier} is highly innefficient because of periodic boundary artifact and the global support of its atoms that fail to capture contours. The cosine basis uses symmetric boundary conditions and thus removes the boundary artifacts, but it still not able to resolve efficiently localized features. The local DCT basis corresponds to the union of local cosine bases defined on small square patches. It is more efficient since its atoms have a local support. However, it gives bad approximation for a small number $M$ of kept coefficients, because of blocking artifacts. The isotropic wavelet basis detailed in Section \ref{sec-isotropic-wav} gives the best approximation results because its is both composed of localized atoms and does not have a block structure but rather a multiresolution structure. 

\myfigure{
\tabquatre{
\image{approximation}{.24}{approx-effi-fft}&
\image{approximation}{.24}{approx-effi-dct}&
\image{approximation}{.24}{approx-effi-locdct}&
\image{approximation}{.24}{approx-effi-wav}\\
Fourier & Cosine & Local DCT & Wavelets\\
SNR=17.1dB & SNR=17.5dB & SNR=18.4dB & SNR=19.3dB
}
}{%
	Comparison of approximation errors for different bases using the same number $M=N/50$ of coefficients.%	
}{fig-approx-effi-1}

\myfigure{
	\image{approximation}{.4}{approx-effi-curves}
}{%
	Comparison of approximation error decay for different bases.%	
}{fig-approx-effi}



%%
\if 0
\paragraph{Compressibility measures.}

We consider $(u_n)_n \in \CC^\NN$ a sequence of coefficients, which typically corresponds to inner products $u_n = \dotp{f}{\phi_n}$ (for linear approximation) or sorted inner products (for non-linear approximation). There are three main ways to control its ``compressibility'' assuming polynomial decay rates.

\begin{itemize}
	\item Coefficient decay	
		\eql{\label{eq-model-coef}\tag{$\Dd(\al,C_u^{\text{coef}})$}
			|u_n| \leq C_u^{\text{coef}} n^{-\al} .
		}
	\item Approximation error decay	
		\eql{\label{eq-model-error}\tag{$\Cc(\be, C_u^{\text{err}})$}
			\sum_{n > M} |u_n|^2 \leq C_u^{\text{err}} M^{-2\be} .
		}
	\item Sobolev/Besov-type spaces 
		\eql{\label{eq-model-sob}\tag{$\Ss(\ga,C_u^{\text{sob}} )$}
			\sum_{n} (1+n^{2\ga}) |u_n|^2  \leq C_u^{\text{sob}} .
		}
		Sobolev spaces corresponds to the case where $u_n$ are un-sorted.
\end{itemize}

When $u$ are the sorted coefficients in some basis,  then condition~\eqref{eq-model-error} corresponds to controlling the non-linear approximation error

The following proposition shows the relationship between these models.

\begin{prop}\label{prop-relation-compressibility}
	One has
\end{prop}

\begin{proof}
	%%
	If $u$ satisfies~\eqref{eq-model-coef}, then 
	\eq{
		\sum_{n > M} |u_n|^2 \leq (C_u^{\text{coef}})^2 \sum_{n > M} n^{-2\al} = C_\al (C_u^{\text{coef}})^2 M^{-2\al+1}
	}
	for some $C_\al=??$, hence $u$ satisfies~\eqref{eq-model-error} with $\be=\al-1/2$ and $C_u^{\text{err}} \leq C_\al (C_u^{\text{coef}})^2$.
	
	If $u$ satisfies~\eqref{eq-model-sob}, then
	\eq{
		C_u^{\text{sob}}  \geq \sum_n n^{2\ga} u_n^2
		\geq \sum_{n>M} n^{2\ga} u_n^2
		\geq M^{2\ga} \sum_{m > M} u_n^2 
	}
	so that $u$ safisfies~\eqref{eq-model-error} with $\be=\al$ and $C_u^{\text{err}}=C_u^{\text{sob}}$.
	
	One has, if the coefficients are sorted
	\eq{
		u_M^2 \leq \frac{2}{M} \sum_{m=M/2+1}^{M} u_m^2 \leq \frac{2}{M} \sum_{n > M/2} u_n^2 = \frac{2}{M} \norm{f-f_{M/2}}^2.
	}
	This proves that
	\eql{\label{eq-decay-coef-approx}
		u_n = O( n^{-\frac{\al+1}{2}} )
		\quad\Longleftrightarrow\quad
		\norm{f-f_M}^2 = O(M^{-\al}).
	}

\end{proof}
\fi



Figure~\ref{fig-approx-comparaison} summarizes the different type of approximation decays for various class of data, which are detailed in the following section.

\myfigure{
\image{approximation}{1}{approx-comparaison}
}{%
	Summary of linear and non-linear approximation rates 
	$\norm{f-f_M}^2$ for different classes of 1-D signals and images. %	
}{fig-approx-comparaison}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fourier Linear Approximation of Smooth Functions}

The smooth signal and image model \eqref{eq-smooth-model} assumed that the analog function have bounded $\al$ continuous derivatives. A function $f$ with a large $\al$ has more smoothness, and is thus simpler to approximate. Figure \ref{fig-signal-model-sobolev} shows images with increasing smoothness.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1-D Fourier Approximation}

A 1-D signal $f \in \Ldeux([0,1])$ is associated to a 1-periodic function $f(t+1)=f(t)$ defined for $t \in \RR$.

%%
\paragraph{Low pass approximation.}

We consider a linear Fourier approximation, that only retains low frequencies
\eq{
	f_M^{\text{lin}} = \sum_{m=-M/2}^{M/2} \dotp{f}{e_m}e_m
}
where we use the 1-D Fourier atoms
\eq{
	\foralls n \in \ZZ, \quad e_m(t) \eqdef e^{2 i \pi m t}.
}
We note that $f_M$ actually requires $M+1$ Fourier atoms.

Figure \ref{fig-ourier-approx-1d} shows examples of such linear approximation for an increasing value of $M$. Since the original function $f$ is singular (no derivative), this produces a large error and one observe ringing artifacts near singularities.

% Code \ref{approx-fourier-linear} implement this low pass linear approximation and code \ref{approx-fourier-nonlinear} implement the non-linear approximation.

\myfigure{
\tabdeux{
\image{approximation}{.48}{fourier-approx-1d-0}&
\image{approximation}{.48}{fourier-approx-1d-1}\\
\image{approximation}{.48}{fourier-approx-1d-2}&
\image{approximation}{.48}{fourier-approx-1d-3}
}
}{%
	Fourier approximation of a signal.%	
}{fig-ourier-approx-1d}

%\matlab{matlab/approx-fourier-linear.m
%}{2-D Fourier linear approximation. Input: image \matvar{f}, parameter \matvar{m}, output: approximation \matvar{f1}. The number of coefficients is $M=(2$\matvar{m}$+1)^2$. }{approx-fourier-linear}

%\matlab{matlab/approx-fourier-nonlinear.m
%}{2-D Fourier linear approximation. Input: image \matvar{f} and number of coefficients \matvar{M}, output: approximation \matvar{f1}.  }{approx-fourier-nonlinear}


This low pass approximation corresponds to a filtering, since 
\eq{
	f_M = \sum_{m=-M/2}^{M/2} \dotp{f}{e_m} e_m = f \star h_M
	\qwhereq
	\hat h_{M} \eqdef 1_{[-M/2,M/2]}.
}
Here, $h_M$ is the so-called Dirichlet kernel (see also the chapter on Fourier).

The following proposition shows that this approximation decays fast for $\Cc^\al$ signals.

\begin{prop}
	One has for $f \in \Cc^\al(\RR/\ZZ)$
	\eq{
		\norm{ f-f_M^{\text{lin}} }^2 = O(M^{-2\al+1}).
	}
\end{prop}

\begin{proof}
Using integration by part, one shows that for a $\Cal$ function $f$ in the smooth signal model~\eqref{eq-smooth-model}, 
$\Ff(f^{(\ell)})_m = (2\imath m \pi)^\ell \hat f_m$, so that 
one has using Cauchy-Schwartz
\eq{
	|\dotp{f}{e_m}| \leq |2\pi m|^{-\al} \norm{f^{(\al)}}_2.
}
This implies
\eq{
	\norm{ f-f_M^{\text{lin}} }^2 = \sum_{|m|>M/2} |\dotp{f}{e_m}|^2
	\leq \norm{f^{(\al)}}^2_2 \sum_{|m|>M/2} |2\pi m|^{-2\al} = O(M^{-2\al+1}).
}
\end{proof}

We show next that a slightly modified proof gives a better decay (assuming $f^{(\al)}$ is in $L^2(\RR/\ZZ)$) and that this conclusion is valid for a larger class of Sobolev functions.

\begin{prop}\label{prop-sobol-fourier-lin}
	For signal $f$ in the Sobolev model~\eqref{eq-sobolev-model}, i.e. $f^{(\al)} \in L^2(\RR)$, one has
	\eql{\label{eq-sovolev-signal-decay}
		\norm{f-f^{\text{lin}}_M}^2 \leq C \norm{f^{(\al)}}^2  M^{-2\al}.
	}
\end{prop}
\begin{proof}
	One has
	\eqml{
		\norm{f^{(\al)}}^2 &= \sum_m |2 \pi m|^{2\al} |\dotp{f}{e_m}|^2
		\geq \sum_{|m|>M/2} |2 \pi m|^{2\al} |\dotp{f}{e_m}|^2 \\ 
		& \geq (\pi M)^{2\al} \sum_{m > M/2}
		|\dotp{f}{e_m}|^2 \geq (\pi M)^{2\al} \norm{f-f_M^{\text{lin}}}^2.
	}
\end{proof}

One can also shows that this asymptotic error decay is optimal, and that the non-linear approximation error in Fourier also decays like $O(M^{-2\al})$.

For a signal in the piecewise smooth model \eqref{eq-model-piece-smooth}, such as the one shows in Figure \ref{fig-ourier-approx-1d}, one only has a slow decay of the linear and non-linear approximation error
\eql{\label{eq-fourier-piecewisesmooth}
	\norm{f-f_M}^2 \leq C_f M^{-1}
}
and Fourier atoms are not anymore optimal for approximation.
%
A typical example is $f=1_[a,b]$ the indicator of a step, for which $|\hat f_m| \sim 1/m$, and thus
$\norm{f-f_M}^2 \sim \sum_{|m|>M} 1/|m|^2 \sim  1/M$ for both linear and non-linear approximations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sobolev Images}

This analysis caries over to images and higher dimensional datasets by considering a Sobolev functional \eqref{eq-sobolev-norm-highdim} for $d>1$.

The linear and non-linear approximation of an $\al$-regular Sobolev image then satisfy
\eq{
	 \norm{f-f_M}^2 \leq C \norm{f^{\al}}^2 M^{-\al}.
}
For $d$-dimensional data $f : [0,1]^d \rightarrow \RR$, one would have an error decay of $O(M^{-2\al/d})$. 

For an image in the piecewise smooth model \eqref{eq-model-piece-smooth-2d}, the linear and non-linear error decays are slow, 
\eql{\label{eq-fourier-decay-2d}
	\norm{f-f_M}^2 \leq C_f M^{-1/2},
}
and Fourier atoms are not anymore optimal for approximation.

\myfigure{
\tabquatre{
\image{approximation}{.24}{fourier-approx-2d-0}&
\image{approximation}{.24}{fourier-approx-2d-1}&
\image{approximation}{.24}{fourier-approx-2d-2}&
\image{approximation}{.24}{fourier-approx-2d-3}\\
&
\image{approximation}{.24}{approx-2d-fourier-1}&
\image{approximation}{.24}{approx-2d-fourier-2}&
\image{approximation}{.24}{approx-2d-fourier-3}
}
}{%
	Linear (top row) and non-linear (bottom row) Fourier approximation.%	
}{fig-fourier-approx-2d-linnonlin}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wavelet Approximation of Piecewise Smooth Functions}

Wavelet approximation improve significantly over Fourier approximation to capture singularities. This is due to the localized support of wavelets. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Decay of Wavelet Coefficients}
\label{subsect-wavdecay}

To efficiently approximate regular parts of signals and images, one uses wavelet with a large enough number $p$ of vanishing moments
\eq{
	\foralls k<p, \; \int \psi(x) x^k \d x = 0.
}
This number $p$ should be larger than the regularity $\al$ of the signal outside singularities (for instance jumps or kinks).

\myfigure{
\image{approximation}{.55}{singular-part-1d}
\image{approximation}{.25}{singular-part-2d}
}{%
	Singular par of signals (left) and image (right).%	
}{fig-singular-part}

To quantify the approximation error decay for piecewise smooth signals \eqref{eq-model-piece-smooth} and images \eqref{eq-model-piece-smooth-2d}, one needs to treat differently wavelets that are in regular and singular areas. Figure \ref{fig-singular-part} shows for a signal and an image the localization of singular and regular parts.

\begin{prop}
If $f$ is $\Cal$ on $\supp(\psi_{j,n})$, with $p \geq \al$, one has
\eql{\label{eq-wavcoef-decay-1}
	|\dotp{f}{\psi_{j,n}}| \leq C_f \norm{\psi}_1 2^{j(\al+d/2)}.
}
In general, one always has for bounded $f$
\eq{\label{eq-wavcoef-decay-2}
	|\dotp{f}{\psi_{j,n}}| \leq \norm{f}_\infty \norm{\psi}_1 2^{j \frac{d}{2}}.
}
\end{prop}

\begin{proof}
If $f$ is $\Cal$ on $\supp(\psi_{j,n})$, with $p \geq \al$, then one can perform a Taylor expansion of $f$ around the point $2^j n$
\eq{
	f(x) = P(x-2^j n) + R(x-2^jn) = P(2^j t) + R(2^j t)
}
where $\deg(P)<\al$ and 
\eq{
	|R(x)|\leq C_f \norm{x}^\al.
}
One then bounds the wavelet coefficient
\eq{
	\dotp{f}{\psi_{j,n}} = \frac{1}{2^{j \frac{d}{2}}} \int f(x) \psi\pa{ \frac{x-2^j n}{2^j} } \d x 
		= 2^{j \frac{d}{2}} \int R(2^j t) \psi(t) \d t
}
where we have performed the change of variable $t = \frac{x-2^j n}{2^j}$. This shows that~\eqref{eq-wavcoef-decay-1} holds. Property~\eqref{eq-wavcoef-decay-2} is straightforward. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1-D Piecewise Smooth Approximation}

For 1-D signal in the piecewise regular model \eqref{eq-model-piece-smooth}, large wavelets coefficients $\dotp{f}{\psi_{j,n}}$ are clustered around the singularities of the signal. We call $\Ss \subset [0,1]$ the finite set of singular points.


\begin{thm}\label{thm-approx-piecesmooth-1d}
$f$ is in the piecewise smooth signal model \eqref{eq-model-piece-smooth}, the non-linear approximation error in wavelet obeys
\eql{\label{eq-decay-piecewav}
	\norm{f-f_M}^2 = O(M^{-2\al}).
}
\end{thm}

\begin{proof}
The proof is split in several parts.

%%
\paragraph{Step 1. Coefficient segmentation.}

The singular support at scale $2^j$ is the set of coefficients corresponding to wavelets that are crossing a singularity
\eql{\label{eq-singsupport-1d}
	\Cc_j = \enscond{n}{ \supp(\psi_{j,n}) \cap \Ss \neq \emptyset }
}
It has a constant size because of the dyadic translation of wavelets
\eq{
	|\Cc_j| \leq K |\Ss| = \text{constant}.
}
Using \eqref{eq-wavcoef-decay-1} for $d=1$, the decay of regular coefficients is bounded as
\eq{
	\foralls n \in \Cc_j^c, \quad \abs{\dotp{f}{\psi_{j,n}}} \leq  C2^{j(\alpha+1/2)}.
}
Using \eqref{eq-wavcoef-decay-2} for $d=1$, the decay of singular coefficients is bounded as
\eq{
	\foralls n \in \Cc_j, \quad \abs{\dotp{f}{\psi_{j,n}}} \leq  C 2^{j/2}.
}
Once a fixed threshold $T$ is fixed to compute the non-linear approximation, one defines cut-off scales for regular and singular coefficients that depend on $T$
\eq{
	2^{j_1} = (T/C)^{\frac{1}{\al+1/2}}
	\qandq
	2^{j_2} = (T/C)^{2}.
}
Figure \ref{fig-singular-support-1d} shows a schematic segmentation of the set of wavelet coefficients into regular and singular parts, and also using the cut-off scales.

\myfigure{
\image{approximation}{.6}{singular-support-1d}
\image{approximation}{.3}{singular-support-legend}
}{%
	Segmentation of the wavelet coefficients into regular and singular parts.%	
}{fig-singular-support-1d}

%%
\paragraph{Step 2. Counting the error.}
These cut-off scales allow us to define a hand-crafted approximation signal
\eql{\label{eq-hand-made-approx}
	\tilde f_M = \sum_{j \geq j_2} \sum_{n \in \Cc_j} \dotp{f}{\psi_{j,n}} \psi_{j,n} +  \sum_{j \geq j_1} \sum_{n \in \Cc_j^c} \dotp{f}{\psi_{j,n}} \psi_{j,n}.
}
The approximation error generated by this $M$-term approximation $\tilde f_M$ is larger than the best $M$-term approximation error, and hence
\eqml{\label{eq-wavapprox-1d-1}
	\norm{f-f_M}^2 &\leq \norm{f-\tilde f_M}^2 \leq \sum_{j<j_2, n \in \Cc_j} |\dotp{f}{\psi_{j,n}}|^2 + 
\sum_{j<j_1, n \in \Cc_j^c} |\dotp{f}{\psi_{j,n}}|^2 \\
	& \leq \sum_{j < j_2} (K |\Ss|) \times C^2 2^{j} + \sum_{j < j_1} 2^{-j} \times  C^2 2^{j(2\al+1)} \\
	& = O( 2^{j_2} + 2^{2\al j_1} ) = O( T^2 + T^{ \frac{2\al}{\al+1/2} } ) = O( T^{ \frac{2\al}{\al+1/2} } ).
}

%%
\paragraph{Step 3. Counting the number of measurements.}
The number of coefficients needed to build the approximating signal $\tilde f_M$ is
\eqml{\label{eq-wavapprox-1d-2}
	M & \leq \sum_{j \geq j_2} |\Cc_j| +
\sum_{j \geq j_1} |\Cc_j^c| \leq
\sum_{j \geq j_2} K |\Ss| +
\sum_{j \geq j_1} 2^{-j} \\
	&= O( |\log(T)| + T^{\frac{-1}{\al+1/2}} ) 
=  O(T^{\frac{-1}{\al+1/2}} ).
}
%%
\paragraph{Step 3.  Putting everything together.}
Putting equations \eqref{eq-wavapprox-1d-1} and \eqref{eq-wavapprox-1d-2} together gives the desired result.
\end{proof}

This theorem improves significantly over the $O(M^{-1})$ decay of Fourier approximation \eqref{eq-fourier-piecewisesmooth}. Furthermore, this decay is the same as the error decay of uniformly smooth signal \eqref{eq-sovolev-signal-decay}. In 1-D, wavelet approximations do not ``see'' the singularities. The error decay \eqref{eq-decay-piecewav} can be shown to be asymptotically optimal.

\myfigure{
\image{approximation}{.6}{wav-approx-1d-1}\\
\image{approximation}{.6}{wav-approx-1d-2}\\
\image{approximation}{.6}{wav-approx-1d-3}
}{%
	1-D wavelet approximation.%	
}{fig-wav-approx-1d}

Figure \ref{fig-wav-approx-1d} shows examples of wavelet approximation of singular signals. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2-D Piecewise Smooth Approximation}

We now give the equivalent of Theorem~\ref{thm-approx-piecesmooth-1d} but for 2-D functions.

\begin{thm}\label{thm-approx-piecesmooth-2d}
$f$ is in the piecewise smooth signal model \eqref{eq-model-piece-smooth-2d}, the non-linear approximation error in wavelet obeys
\eql{\label{eq-approximation-error-wav-2d}
	\norm{f-f_M}^2 = O(M^{-1}).
}
\end{thm}

\begin{proof}
For an image in the piecewise smooth model \eqref{eq-model-piece-smooth-2d}, we define the singular support $\Cc_j$ as in \eqref{eq-singsupport-1d}. The major difference with the 1-D setting, is that for 2-D images, the size of the singular support grows when the scale $2^j$ goes to zero
\eq{
	|\Cc_j^\om| \leq 2^{-j} K |\Ss|,
}
where $|\Ss|$ is the perimeter of the singular curves $\Ss$, and $\om \in \{V,H,D\}$ is the wavelet orientation. 
Using \eqref{eq-wavcoef-decay-1} for $d=2$, the decay of regular coefficients is bounded as
\eq{
	\foralls n \in (\Cc_j^{\om})^c, \quad \abs{\dotp{f}{\psi_{j,n}^\om}} \leq  C2^{j(\alpha+1)}.
}
Using \eqref{eq-wavcoef-decay-2} for $d=2$, the decay of singular coefficients is bounded as
\eq{
	\foralls n \in \Cc_j^\om, \quad \abs{\dotp{f}{\psi_{j,n}^\om}} \leq  C 2^{j}.
}
After fixing $T$, the cut-off scales are defined as 
\eq{
	2^{j_1} = (T/C)^{\frac{1}{\al+1}}
	\qandq
	2^{j_2} = T/C.
}
We define similarly to \eqref{eq-hand-made-approx} a hand-made approximation.
Similarly to \eqref{eq-wavapprox-1d-1}, we bound the approximation error as
\eq{
	\norm{f-f_M}^2 \leq \norm{f-\tilde f_M}^2 
	= O( 2^{j_2} + 2^{2\al j_1} ) = O( T + T^{ \frac{2\al}{\al+1} } ) = O( T )
}
and the number of coefficients as 
\eq{
	M = O( T^{-1} + T^{\frac{-1}{\al+1}} ) = O(T^{-1} ).
}
This leads to the announced decay of the non-linear wavelet approximation error.
\end{proof}

This improves significantly over the $O(M^{-1/2})$ decay of Fourier approximation \eqref{eq-fourier-decay-2d}. This result is however deceiving, since it does not take advantage of the $\Cal$ regularity of the image outside the edge curves. 

This error decay is still valid for the more general model of images with bounded variations \eqref{eq-model-tv}. One can shows that wavelets are asymptotically optimal to approximate images with bounded variations. 

\myfigure{
\tabtrois{
\image{approximation}{.32}{approx-2d-wav-1}&
\image{approximation}{.32}{approx-2d-wav-2}&
\image{approximation}{.32}{approx-2d-wav-3}
}
}{%
	2-D wavelet approximation.%	
}{fig-approx-2d-wav}

Figure \ref{fig-approx-2d-wav} shows wavelet approximations of a bounded variation image. 

% Code \ref{approx-wavelets-linear} implements the linear wavelet approximation and code \ref{approx-wavelets-nonlinear} implements the non-linear approximation.


%\matlab{matlab/approx-wavelets-linear.m
%}{2-D wavelets linear approximation. Input: image \matvar{f}, parameter \matvar{m}, coarse scale \matvar{j0}, output: approximation \matvar{f1}. The number of coefficients is $M=$\matvar{m}$^2$. }{approx-wavelets-linear}

%\matlab{matlab/approx-wavelets-nonlinear.m
%}{2-D wavelets linear approximation. Input: image \matvar{f} and number of coefficients \matvar{M}, coarse scale \matvar{j0}, output: approximation \matvar{f1}.  }{approx-wavelets-nonlinear}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cartoon Images Approximation}

The square support of wavelet makes them inefficient to approximate geometric images \eqref{eq-cartoon-model}, whose edges are more regular than the level set of bounded variation images \eqref{eq-model-tv}, which are only assumed to be of finite length. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Wavelet Approximation of Cartoon Images}

Result \eqref{eq-approximation-error-wav-2d} shows that wavelet approximation of images in the cartoon models \eqref{eq-cartoon-model} decays at least like $O(M^{-1})$. One can show that simple cartoon images like $f=1_\Om$ where $\Om$ is a disk reach this low decay speed. This is because the square support of wavelets forbid them to take advantage of the regularity of edge curves. The approximation error for the smoothed cartoon model \eqref{eq-cartoon-model-smooth} is also slow if the width of the blurring kernel is small with respect to the number $M$ of coefficients. 

Figure \ref{fig-cartoon-wav} shows that many large coefficients are located near edge curves, and retaining only a small number leads to a bad approximation with visually unpleasant artifacts. 


\myfigure{
\tabtrois{
\image{approximation}{.32}{cartoon-wav-image}&
\image{approximation}{.32}{cartoon-wav-approx}&
\image{approximation}{.32}{cartoon-wav-coefs}\\
$f$ & $\{\dotp{f}{\psi_{j,n}^\om}\}$ & $f_M$
}
}{%
	Wavelet approximation of a cartoon image. %	
}{fig-cartoon-wav}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Finite Element Approximation}

To improve over the wavelet approximation, one can design an adapted triangulation that is highly anisotropic near edges. 
Figure \ref{fig-triangulation-image} shows an example of such a triangulation.

\myfigure{
\image{approximation}{.3}{triangulation-image}
\image{approximation}{.3}{triangulation-triangles}
}{%
	Left: cartoon image, right: adaptive triangulation. %	
}{fig-triangulation-image}

A triangulation is obtain by sampling $M$ points over the image domain $[0,1]^2$ and then connecting them using triangles. One then defines a piecewise linear interpolation $\tilde f_M$ over these triangles. 

\myfigure{
\image{approximation}{.3}{triangulation-aniso}\quad
\image{approximation}{.3}{triangulation-iso}
}{%
	Aspect ratio of triangle away from edges (left) and near an edge (right). %	
}{fig-triangulation-anisoiso}

As shown in Figure \ref{fig-triangulation-anisoiso}, an efficient approximation of a $\Cdeux$-cartoon image \eqref{eq-cartoon-model} for $\al=2$ is obtained by seeding $\approx M/2$ approximately equilateral triangles of width $\approx M^{-1/2}$ in the areas where the image is regular. Near the edges, using the $\Cdeux$ regularity of the singular curve, one can seed $\approx M/2$ anisotropic triangles of length $M^{-1}$ and width $\approx M^{-1/2}$. One can show that such an adaptive triangulation leads to an approximation error 
\eql{\label{eq-error-triangulation}
	\norm{f-f_M}^2 = O(M^{-2}),
}
which improves over the wavelet approximation error decay \eqref{eq-approximation-error-wav-2d}. 

This scheme is however difficult to implement in practice, since the edge curves are not known and difficult to find. This is in particular the case for smooth cartoon images when the smoothing kernel $h$ is unknown. 

There is currrently no known algorithm that can automatically produces the error decay \eqref{eq-error-triangulation}. 
One thus has to use heuristics and greedy algorithm to find the location of the sampling points and computes the triangles. 
Figure \eqref{fig-triangulation-peppers} shows an example of compression using such a greedy seeding algorithm, that works well in practice. 

\myfigure{
\tabtrois{
\image{approximation}{.32}{triangulation-peppers}&
\image{approximation}{.32}{triangulation-peppers-triangles}&
\image{approximation}{.32}{triangulation-peppers-jpeg2k}\\
Adpated triangulation & $\tilde f_M$ & JPEG-2000
}
}{%
	Comparison of adaptive triangulation and JPEG-2000, with the same number of bits. %	
}{fig-triangulation-peppers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Curvelets Approximation}

Instead of using an adaptive approximation scheme such as finite elements, one can replace the wavelet basis by a set of oriented anisotropic atoms. The curvelet frame was proposed by Cand\`es and Donoho for this purpose \cite{candes-tight-frame}.

%%
\paragraph{Curvelets.}

The curvelet construction starts from a curvelet function $c$ that is oriented along the horizontal direction, and perform stretching
\eq{
	c_{2^j}(x_1,x_2) \approx 2^{-3j/4} c(2^{-j/2} x_1, 2^{-j} x_2),
} 
translation and rotation 
\eq{
	c_{2^j,u}^\th(x_1,x_2) = c_{2^j}(R_\th(x-u))
}
where $R_\th$ is the rotation of angle $\th$.

The atoms $c_{2^j,u}^\th$ is located near $u$, with an orientation $\th$, and has an aspect ratio ``width $\approx$ length$^2$'', which is the same aspect used to build an adaptive finite element approximation. This aspect ratio is essential to capture the anisotropic regularity near edges for images in the cartoon model \eqref{eq-cartoon-model} for $\al=2$. 

\myfigure{
\image{approximation}{.36}{curvelet-spacial}
\image{approximation}{.4}{curvelet-fourier}
}{%
	Left: a curvelet $c_m$, right: its Fourier transform localization.%	
}{fig-curvelet}

Figure \ref{fig-curvelet-discretization} shows the spacial and frequency localization of curvelets.

%%
\paragraph{Parameter discretization.}

To build an image representation, one need to sample the $u$ and $\th$ parameter.
To maintain a stable representation, the sub-sampling of the angles depends on the scale
\eq{
	\foralls 0 \leq k < 2^{-\lceil j/2 \rceil+2}, \quad
	\th_{k}^{(j)} = k \pi 2^{ \lceil j/2 \rceil -1 }  
}
and the spacial grid depends on the scale and on the angles
\eq{
	\foralls m=(m_1,m_2) \in \ZZ^2, \quad
	u_m^{(j,\th)} = R_\th( 2^{j/2} m_1, 2^j m_2 ).
}
Figure \ref{fig-curvelet-discretization} shows this sampling pattern.

\myfigure{
\image{approximation}{.8}{curvelet-discretization}
}{%
	Sampling pattern for the curvelet positions.%	
}{fig-curvelet-discretization}

%%
\paragraph{Curvelet tight frame.}

This sampling leads to a stable redundant family 
\eq{
	C_{j,m,k}(x) = c_{2^j,u}^{ \th }(x)
	\qwhereq \th = \th_{k}^{(j)}
	\qandq u=u_m^{(j,\th)},
}
that obeys a conservation of energy 
\eq{
	\norm{f}^2 = \sum_{j \in \ZZ} \sum_{ k=0 }^{ 2^{-\lceil j/2 \rceil+2} }
	\sum_{m \in \ZZ^2} |\dotp{f}{C_{j,m,k}}|^2
}
and a reconstruction formula
\eq{
	f = \sum_{j \in \ZZ} \sum_{ k=0 }^{ 2^{-\lceil j/2 \rceil+2} }
	\sum_{m \in \ZZ^2} \dotp{f}{C_{j,m,k}} C_{j,m,k}
}
that extends the properties of orthogonal basis (tight frame), although the representation is redundant (the atoms are not orthogonal).

A numerical implementation of this tight frame also defines a discrete tight frame for image of $N$ pixels, that is made of $\approx 5 N$ atoms \cite{candes-discrete-curvelet}. 

%%
\paragraph{Curvelet approximation.}

A non-linear $M$-term approximation in curvelets is defined as
\eq{
	f_M = \sum_{|\dotp{f}{C_{j,m,k}}| > T} \dotp{f}{C_{j,m,k}} C_{j,m,k}
}
where $T$ is a threshold that depends on $M$. One should note that $f_M$ is not necessarily the best $M$-term curvelet approximation since the curvelet frame is not orthogonal. 

For position $u_m^{(j,\th)}$ that are far away from an edges, the vanishing moments of the curvelets create a small coefficient $\dotp{f}{C_{j,m,k}}$. If  $u_m^{(j,\th)}$ is close to an edge curve whose tangent has direction $\tilde\th$, then the coefficient $\dotp{f}{C_{j,m,k}}$ decays very fast to zero when $|\th-\tilde \th|$ increases. Figure \ref{fig-curvelets-vs-wavelets} shows the principle of this curvelet approximation, and compares it with directional wavelets that have a square support.  


\myfigure{
\image{approximation}{.8}{curvelets-vs-wavelets}
}{%
	Comparison of the principle of wavelets (left) and curvelet (right) approximations of a cartoon image.%	
}{fig-curvelets-vs-wavelets}

Using these two properties together with the sparse sampling of the curvelet in space and orientation leads to the following approximation error decay
\eq{
	\norm{f-f_M}^2 = O(\log^3(M) M^{-2})
}
for image in the cartoon model \eqref{eq-cartoon-model} for $\al=2$.
This is close to the decay of adaptive triangulations \eqref{eq-error-triangulation}, but this time one computes $f_M$ with a fast $O(N \log(N))$ algorithm for an image of $N$ pixels.

In practice, the redundancy of the curvelet frame makes it not suitable for image compression. Its efficiency is however useful for denoising purpose, where it can improve over wavelet to denoise geometric images and textures, see Figure \ref{fig-curvelet-denoising}. The result is obtained by using a thresholding denoiser as detailed in Section \ref{subsec-denoise-hard}.


\myfigure{
\tabtrois{
\image{approximation}{.3}{curvelet-denoising-noisy}&
\image{approximation}{.3}{curvelet-denoising-wav}&
\image{approximation}{.3}{curvelet-denoising-curvelets}\\
Noisy $f$ & Wavelet $\tilde f$ & Curvelets $\tilde f$
}
}{%
	Comparison of wavelets (translation invariant) and curvelet denoising. %	
}{fig-curvelet-denoising}








