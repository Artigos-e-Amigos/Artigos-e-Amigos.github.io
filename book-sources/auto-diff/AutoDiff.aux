\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Finite Differences and Symbolic Calculus}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Computational Graphs}{1}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  A computational graph. }}{2}{figure.1}}
\newlabel{fig-compgraph}{{1}{2}{A computational graph}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Relation between the variable for the forward (left) and backward (right) modes. }}{2}{figure.2}}
\newlabel{fig-forward-backward}{{2}{2}{Relation between the variable for the forward (left) and backward (right) modes}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Forward Mode of Automatic Differentiation}{2}{section.3}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{3}{section*.1}}
\newlabel{eq-simple-func-autodiff}{{1}{3}{Simple example}{equation.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Dual numbers.}{3}{section*.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Example of a simple computational graph. }}{4}{figure.3}}
\newlabel{fig-dag-example-simple}{{3}{4}{Example of a simple computational graph}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Reverse Mode of Automatic Differentiation}{4}{section.4}}
\@writefile{toc}{\contentsline {paragraph}{Back-propagation.}{5}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{5}{section*.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Feed-forward Compositions}{5}{section.5}}
\newlabel{eq-simple-lin-dag}{{2}{5}{Feed-forward Compositions}{equation.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Complexity of forward (left) and backward (right) modes for composition of functions. }}{6}{figure.4}}
\newlabel{fig-matrix-mult}{{4}{6}{Complexity of forward (left) and backward (right) modes for composition of functions}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Computational graph for a feedforward architecture. }}{6}{figure.5}}
\newlabel{fig-mlp}{{5}{6}{Computational graph for a feedforward architecture}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Feed-forward Architecture}{6}{section.6}}
\newlabel{eq-feednets}{{3}{6}{Feed-forward Architecture}{equation.6.3}{}}
\newlabel{eq-loss-feedf}{{4}{6}{Feed-forward Architecture}{equation.6.4}{}}
\newlabel{eq-backprop-discr}{{5}{6}{Feed-forward Architecture}{equation.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Multi-layer perceptron parameterization. }}{7}{figure.6}}
\newlabel{fig-mlp-param}{{6}{7}{Multi-layer perceptron parameterization}{figure.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Multilayers perceptron.}{7}{section*.5}}
\newlabel{eq-mlp-func}{{6}{7}{Multilayers perceptron}{equation.6.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Link with adjoint state method.}{7}{section*.6}}
\newlabel{eq-flow-eq}{{7}{7}{Link with adjoint state method}{equation.6.7}{}}
\newlabel{eq-ode-structure}{{8}{7}{Link with adjoint state method}{equation.6.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Recurrent Architectures}{7}{section.7}}
\newlabel{eq-feednets-recur}{{9}{7}{Recurrent Architectures}{equation.7.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Computational graph for a recurrent architecture. }}{8}{figure.7}}
\newlabel{fig-recur}{{7}{8}{Computational graph for a recurrent architecture}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Recurrent residual perceptron parameterization. }}{8}{figure.8}}
\newlabel{fig-recurrent-param}{{8}{8}{Recurrent residual perceptron parameterization}{figure.8}{}}
\newlabel{eq-backprop-discr}{{10}{8}{Recurrent Architectures}{equation.7.10}{}}
\newlabel{eq-jacobian-mlp}{{11}{8}{Recurrent Architectures}{equation.7.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Residual recurrent networks. }{8}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Mitigating memory requirement. }{8}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Fixed point maps}{9}{section*.9}}
\newlabel{eq-impl-func-formula}{{12}{9}{Fixed point maps}{equation.7.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Argmin layers}{9}{section*.10}}
\newlabel{eq-argmin-layer}{{13}{9}{Argmin layers}{equation.7.13}{}}
\newlabel{eq-danskin}{{14}{9}{Argmin layers}{equation.7.14}{}}
\bibstyle{plain}
\bibdata{all}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn's algorithm}{10}{section*.11}}
