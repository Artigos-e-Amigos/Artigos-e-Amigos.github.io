\documentclass[10pt]{article}
%\documentclass[14pt]{extarticle}

% Be sure to use PDF Latex
\pdfoutput=1

% links
\usepackage[bookmarks,bookmarksdepth=2, colorlinks=true, linkcolor=blue,citecolor=red, urlcolor=blue]{hyperref}


\usepackage{fullpage}

\usepackage[latin1]{inputenc}
\usepackage{../mystyle}
\usepackage{wrapfig}



\newcommand{\dims}{d}


\graphicspath{{../figures/}}



\title{Recap on Probably Approximately Correct learning  theory} 

\author{%
\begin{tabular}{c}
	Gabriel Peyr{\'e} \\ CNRS \& DMA \\
	 \'Ecole Normale Sup\'erieure \\
	 \url{gabriel.peyre@ens.fr}\\
	 \url{https://mathematical-tours.github.io}\\
	 \url{www.numerical-tours.com}
\end{tabular}
}

\date{\today}

\renewcommand{\subsection}[1]{\section{#1}}
%\renewcommand{\paragraph}[1]{\subsection{#1}}

%%

\begin{document}




\maketitle

\begin{abstract}
		This document is a short presentation of some important results of Probably Approximately Correct (PAC) learning theory. Its goal is to asses the generalization performance of learning methods. The main reference (and in particular the proofs of the mentioned results) is the fantastic book ``Foundations of Machine Learning'' by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar \url{https://cs.nyu.edu/~mohri/mlbook/} and the cristal clear course notes ``Learning Theory from First Principles'' \url{https://www.di.ens.fr/~fbach/learning_theory_class/index.html} of Francis Bach.
\end{abstract}

\vspace{5mm}
 

\input{../chapters/machine-learning-sec-pac.tex}


% \nocite{*}

\bibliographystyle{plain}
\bibliography{all}

\end{document}
