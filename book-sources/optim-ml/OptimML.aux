\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation in Machine Learning}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Unconstraint optimization}{2}{subsection.1.1}}
\newlabel{eq-general-pbm}{{1}{2}{Unconstraint optimization}{equation.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Left: linear regression, middle: linear classifier, right: loss function for classification. }}{3}{figure.1}}
\newlabel{fig-ml-ex}{{1}{3}{Left: linear regression, middle: linear classifier, right: loss function for classification}{figure.1}{}}
\newlabel{eq-general-pbm-min}{{2}{3}{Unconstraint optimization}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regression}{3}{subsection.1.2}}
\newlabel{eq-least-square}{{3}{3}{Regression}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Classification}{3}{subsection.1.3}}
\newlabel{eq-classif}{{4}{3}{Classification}{equation.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Basics of Convex Analysis}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Existence of Solutions}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness. }}{4}{figure.2}}
\newlabel{fig-minimizer-exists}{{2}{4}{Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Coercivity condition for least squares. }}{4}{figure.3}}
\newlabel{fig-least-square}{{3}{4}{Coercivity condition for least squares}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convexity}{4}{subsection.2.2}}
\newlabel{eq-convexity-def}{{5}{4}{Convexity}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions. }}{5}{figure.4}}
\newlabel{fig-cvx-vs-noncvx}{{4}{5}{Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Comparison of convex functions $f : \mathbb  {R}^p \rightarrow \mathbb  {R}$ (for $p=1$) and convex sets $C \subset \mathbb  {R}^p$ (for $p=2$). }}{5}{figure.5}}
\newlabel{fig-cvx-set}{{5}{5}{Comparison of convex functions $f : \RR ^p \rightarrow \RR $ (for $p=1$) and convex sets $C \subset \RR ^p$ (for $p=2$)}{figure.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Strict convexity.}{5}{section*.2}}
\newlabel{eq-strict-convexity-def}{{6}{5}{Strict convexity}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Convex Sets}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Derivative and gradient}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient}{6}{subsection.3.1}}
\newlabel{eq-grad-dfn}{{7}{6}{Gradient}{equation.3.7}{}}
\newlabel{prop-above-tgt}{{1}{6}{}{prop.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Function with local maxima/minima (left), saddle point (middle) and global minimum (right). }}{7}{figure.6}}
\newlabel{fig-first-order}{{6}{7}{Function with local maxima/minima (left), saddle point (middle) and global minimum (right)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}First Order Conditions}{7}{subsection.3.2}}
\newlabel{prop-cs-min}{{2}{7}{}{prop.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Least Squares}{8}{subsection.3.3}}
\newlabel{eq-grad-ls}{{8}{8}{Least Squares}{equation.3.8}{}}
\newlabel{eq-sol-leastsquare}{{9}{8}{Least Squares}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Link with PCA}{8}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$. }}{9}{figure.7}}
\newlabel{fig-link-pca}{{7}{9}{Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$}{figure.7}{}}
\newlabel{eq-pca-decomp}{{10}{9}{Link with PCA}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Classification}{9}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Chain Rule}{10}{subsection.3.6}}
\newlabel{eq-grad-composition-linear}{{11}{10}{Chain Rule}{equation.3.11}{}}
\newlabel{eq-differential-defn}{{12}{10}{Chain Rule}{equation.3.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Gradient Descent Algorithm}{10}{section.4}}
\newlabel{sec-grad-desc-basic}{{4}{10}{Gradient Descent Algorithm}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Steepest Descent Direction}{10}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof. }}{11}{figure.8}}
\newlabel{fig-expansion-taylor}{{8}{11}{Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right). }}{12}{figure.9}}
\newlabel{fig-gradesc}{{9}{12}{Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right)}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Gradient Descent}{12}{subsection.4.2}}
\newlabel{eq-grad-desc}{{13}{12}{Gradient Descent}{equation.4.13}{}}
\newlabel{eq-armijo-rule}{{14}{12}{Armijo rule}{equation.4.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Convergence Analysis}{13}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Quadratic Case}{13}{subsection.5.1}}
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis for the quadratic case.}{13}{section*.3}}
\newlabel{prop-graddesc-quad}{{4}{13}{}{prop.4}{}}
\newlabel{eq-global-linrate-grad}{{15}{13}{}{equation.5.15}{}}
\newlabel{eq-best-rate-local}{{16}{13}{}{equation.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Contraction constant $h(\tau )$ for a quadratic function (right). }}{14}{figure.10}}
\newlabel{fig-grad-desc-contract}{{10}{14}{Contraction constant $h(\tau )$ for a quadratic function (right)}{figure.10}{}}
\newlabel{eq-rate-strong-quad}{{17}{14}{Convergence analysis for the quadratic case}{equation.5.17}{}}
\newlabel{prop-graddesc-quad-sublin}{{5}{14}{}{prop.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}General Case}{15}{subsection.5.2}}
\@writefile{toc}{\contentsline {paragraph}{Hessian.}{15}{section*.4}}
\newlabel{eq-taylor-hess}{{18}{15}{Hessian}{equation.5.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Smoothness and strong convexity.}{16}{section*.5}}
\newlabel{eq-lipsch-grad}{{{$\mathcal  {R}_L$}}{16}{Smoothness and strong convexity}{equation.5.19}{}}
\newlabel{eq-strong-conv}{{{$\mathcal  {S}_\mu $}}{16}{Smoothness and strong convexity}{equation.5.19}{}}
\newlabel{prop-smooth-strong}{{6}{17}{}{prop.6}{}}
\newlabel{eq-above-below-quad}{{19}{17}{}{equation.5.19}{}}
\newlabel{eq-upper-lower-bound-hess}{{20}{17}{}{equation.5.20}{}}
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis.}{17}{section*.6}}
\newlabel{thm-gradsec-non-strong-conv}{{1}{17}{}{thm.1}{}}
\newlabel{eq-sublin-rate-gd}{{21}{17}{}{equation.5.21}{}}
\newlabel{eq-proox-x'rad-nonstrong-1}{{22}{18}{Convergence analysis}{equation.5.22}{}}
\newlabel{eq-conv-rate-proof-1}{{25}{18}{Convergence analysis}{equation.5.25}{}}
\newlabel{eq-rate-strong}{{26}{18}{Convergence analysis}{equation.5.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Acceleration}{19}{subsection.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Mirror Descent and Implicit Bias}{19}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Bregman Divergences}{19}{subsection.6.1}}
\newlabel{eq-burg-entropy}{{27}{20}{Bregman Divergences}{equation.6.27}{}}
\newlabel{eq-power-entropies}{{28}{20}{Bregman Divergences}{equation.6.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Mirror descent}{21}{subsection.6.2}}
\newlabel{eq-mirror-descent}{{29}{21}{Mirror descent}{equation.6.29}{}}
\newlabel{eq-mirror-dual}{{30}{21}{Mirror descent}{equation.6.30}{}}
\@writefile{toc}{\contentsline {paragraph}{Mirror flow.}{21}{section*.7}}
\newlabel{eq-hessian-flow}{{31}{21}{Mirror flow}{equation.6.31}{}}
\@writefile{toc}{\contentsline {paragraph}{Convergence.}{21}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Re-parameterized flows}{22}{subsection.6.3}}
\@writefile{toc}{\contentsline {paragraph}{Dual parameterization}{22}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Example: power-type parameterization}{22}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Counter-example: SDP matrices}{22}{section*.11}}
\newlabel{eq-flow-repar-matrices}{{32}{22}{Counter-example: SDP matrices}{equation.6.32}{}}
\citation{gunasekar2018characterizing}
\citation{hoff2017lasso}
\citation{vavskevivcius2019implicit}
\citation{zhao2019implicit}
\citation{woodworth2020kernel}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Implicit Bias}{23}{subsection.6.4}}
\newlabel{prop-implicit-bias-l2}{{7}{23}{}{prop.7}{}}
\newlabel{prop-implicit-bias-mirror}{{8}{23}{}{prop.8}{}}
\newlabel{eq-implicit-bias-mirror}{{33}{23}{}{equation.6.33}{}}
\newlabel{eq-evol-dual}{{34}{23}{Implicit Bias}{equation.6.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Regularization}{24}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Penalized Least Squares}{24}{subsection.7.1}}
\newlabel{eq-regul-ls}{{35}{24}{Penalized Least Squares}{equation.7.35}{}}
\newlabel{eq-regul-constr}{{36}{24}{}{equation.7.36}{}}
\newlabel{eq-ineq-proof-regul}{{37}{24}{Penalized Least Squares}{equation.7.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Ridge Regression}{24}{subsection.7.2}}
\newlabel{eq-regul-ls-1}{{38}{24}{}{equation.7.38}{}}
\newlabel{eq-regul-ls-2}{{39}{24}{}{equation.7.39}{}}
\@writefile{toc}{\contentsline {paragraph}{Pseudo-inverse.}{25}{section*.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces $\ell ^q$ balls $ \left \{ x \tmspace  +\thickmuskip {.2777em};\tmspace  +\thickmuskip {.2777em} \DOTSB \sum@ \slimits@ _k |x_k|^q \leqslant 1 \right \} $ for varying $q$. }}{25}{figure.11}}
\newlabel{fig-sparsity-lq}{{11}{25}{$\ell ^q$ balls $\enscond {x}{\sum _k |x_k|^q \leq 1}$ for varying $q$}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Lasso}{25}{subsection.7.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Evolution with $\lambda $ of the function $F(x) \ensuremath  {\mathrel {\mathop {=}\limits ^{\unhbox \voidb@x \hbox {\upshape  \relax \fontsize  {5}{6}\selectfont  def.}}}}\frac  {1}{2}|\tmspace  -\thinmuskip {.1667em}| \cdot -y |\tmspace  -\thinmuskip {.1667em}|^2+\lambda |\cdot |$. }}{26}{figure.12}}
\newlabel{fig-varspars}{{12}{26}{Evolution with $\la $ of the function $F(x) \eqdef \frac {1}{2}\norm {\cdot -y}^2+\la |\cdot |$}{figure.12}{}}
\newlabel{prop-soft-tresdh}{{11}{26}{}{prop.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Iterative Soft Thresholding}{26}{subsection.7.4}}
\newlabel{sec-ista}{{7.4}{26}{Iterative Soft Thresholding}{subsection.7.4}{}}
\newlabel{eq-ista-surrog}{{40}{26}{Iterative Soft Thresholding}{equation.7.40}{}}
\newlabel{eq-ista}{{41}{27}{}{equation.7.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Stochastic Optimization}{27}{section.8}}
\newlabel{sec-stochastic-optim}{{8}{27}{Stochastic Optimization}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Minimizing Sums and Expectation}{27}{subsection.8.1}}
\newlabel{eq-min-sums}{{42}{27}{Minimizing Sums and Expectation}{equation.8.42}{}}
\newlabel{eq-min-int}{{43}{27}{Minimizing Sums and Expectation}{equation.8.43}{}}
\newlabel{eq-stochastic-erm}{{44}{27}{Minimizing Sums and Expectation}{equation.8.44}{}}
\newlabel{eq-stoch-logistic}{{45}{27}{Minimizing Sums and Expectation}{equation.8.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Evolution of the error of the BGD for logistic classification. }}{28}{figure.13}}
\newlabel{fig-bgd}{{13}{28}{Evolution of the error of the BGD for logistic classification}{figure.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Batch Gradient Descent (BGD)}{28}{subsection.8.2}}
\newlabel{eq-full-grad}{{46}{28}{Batch Gradient Descent (BGD)}{equation.8.46}{}}
\newlabel{eq-grad-formula}{{47}{28}{Batch Gradient Descent (BGD)}{equation.8.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Stochastic Gradient Descent (SGD)}{28}{subsection.8.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  Display of a large number of trajectories $k \DOTSB \mapstochar \rightarrow x_k \in \mathbb  {R}$ generated by several runs of SGD. On the top row, each curve is a trajectory, and the bottom row displays the corresponding density. }}{29}{figure.16}}
\newlabel{fig-sgd-traject}{{16}{29}{Display of a large number of trajectories $k \mapsto x_k \in \RR $ generated by several runs of SGD. On the top row, each curve is a trajectory, and the bottom row displays the corresponding density}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Unbiased gradient estimate}}{29}{figure.14}}
\newlabel{eq-unbiased-grad}{{48}{29}{Stochastic Gradient Descent (SGD)}{equation.8.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Schematic view of SGD iterates}}{29}{figure.15}}
\newlabel{eq-stepsize-sgd}{{49}{29}{Stochastic Gradient Descent (SGD)}{equation.8.49}{}}
\newlabel{thm-conv-sgd}{{2}{29}{}{thm.2}{}}
\newlabel{eq-rate-sgd}{{50}{29}{}{equation.8.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Evolution of the error of the SGD for logistic classification (dashed line shows BGD). }}{30}{figure.17}}
\newlabel{fig-sgd}{{17}{30}{Evolution of the error of the SGD for logistic classification (dashed line shows BGD)}{figure.17}{}}
\newlabel{eq-sgd-proof-1}{{51}{30}{Stochastic Gradient Descent (SGD)}{equation.8.51}{}}
\newlabel{eq-sgd-proof-2}{{52}{30}{Stochastic Gradient Descent (SGD)}{equation.8.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Stochastic Gradient Descent with Averaging (SGA)}{31}{subsection.8.4}}
\newlabel{sec-sga}{{8.4}{31}{Stochastic Gradient Descent with Averaging (SGA)}{subsection.8.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Stochastic Averaged Gradient Descent (SAG)}{31}{subsection.8.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  Evolution of $\qopname  \relax o{log}_{10}(f(x_k)-f(x^\star ))$ for SGD, SGA and SAG. }}{32}{figure.18}}
\newlabel{fig-compariso-sgd}{{18}{32}{Evolution of $\log _{10}(f(x_k)-f(x^\star ))$ for SGD, SGA and SAG}{figure.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Multi-Layers Perceptron}{32}{section.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}MLP and its derivative}{32}{subsection.9.1}}
\@writefile{toc}{\contentsline {paragraph}{Expressiveness. }{32}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}MLP and Gradient Computation}{33}{subsection.9.2}}
\@writefile{toc}{\contentsline {paragraph}{Optimizing with respect to $u$.}{33}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Optimizing with respect to $W$.}{33}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Universality}{33}{subsection.9.3}}
\newlabel{sec-universality}{{9.3}{33}{Universality}{subsection.9.3}{}}
\newlabel{eq-constraint-univ}{{53}{34}{Universality}{equation.9.53}{}}
\newlabel{thm-universality}{{3}{34}{Cybenko, 1989}{thm.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Proof in dimension $p=1$.}{34}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Proof in arbitrary dimension $p$.}{34}{section*.17}}
\newlabel{prop-proof-univ-1}{{13}{34}{}{prop.13}{}}
\newlabel{eq-prop-proof-univ-1}{{54}{34}{}{equation.9.54}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative rates.}{35}{section*.18}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Automatic Differentiation}{36}{section.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Finite Differences and Symbolic Calculus}{36}{subsection.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Computational Graphs}{36}{subsection.10.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  A computational graph. }}{37}{figure.19}}
\newlabel{fig-compgraph}{{19}{37}{A computational graph}{figure.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  Relation between the variable for the forward (left) and backward (right) modes. }}{37}{figure.20}}
\newlabel{fig-forward-backward}{{20}{37}{Relation between the variable for the forward (left) and backward (right) modes}{figure.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Forward Mode of Automatic Differentiation}{37}{subsection.10.3}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{37}{section*.19}}
\newlabel{eq-simple-func-autodiff}{{55}{37}{Simple example}{equation.10.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  Example of a simple computational graph. }}{38}{figure.21}}
\newlabel{fig-dag-example-simple}{{21}{38}{Example of a simple computational graph}{figure.21}{}}
\@writefile{toc}{\contentsline {paragraph}{Dual numbers.}{38}{section*.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Reverse Mode of Automatic Differentiation}{39}{subsection.10.4}}
\@writefile{toc}{\contentsline {paragraph}{Back-propagation.}{39}{section*.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  Complexity of forward (left) and backward (right) modes for composition of functions. }}{40}{figure.22}}
\newlabel{fig-matrix-mult}{{22}{40}{Complexity of forward (left) and backward (right) modes for composition of functions}{figure.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{40}{section*.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Feed-forward Compositions}{40}{subsection.10.5}}
\newlabel{eq-simple-lin-dag}{{56}{40}{Feed-forward Compositions}{equation.10.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  Computational graph for a feedforward architecture. }}{41}{figure.23}}
\newlabel{fig-mlp}{{23}{41}{Computational graph for a feedforward architecture}{figure.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Feed-forward Architecture}{41}{subsection.10.6}}
\newlabel{eq-feednets}{{57}{41}{Feed-forward Architecture}{equation.10.57}{}}
\newlabel{eq-loss-feedf}{{58}{41}{Feed-forward Architecture}{equation.10.58}{}}
\newlabel{eq-backprop-discr}{{59}{41}{Feed-forward Architecture}{equation.10.59}{}}
\@writefile{toc}{\contentsline {paragraph}{Multilayers perceptron.}{41}{section*.23}}
\newlabel{eq-mlp-func}{{60}{41}{Multilayers perceptron}{equation.10.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Multi-layer perceptron parameterization. }}{42}{figure.24}}
\newlabel{fig-mlp-param}{{24}{42}{Multi-layer perceptron parameterization}{figure.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  Computational graph for a recurrent architecture. }}{42}{figure.25}}
\newlabel{fig-recur}{{25}{42}{Computational graph for a recurrent architecture}{figure.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Link with adjoint state method.}{42}{section*.24}}
\newlabel{eq-flow-eq}{{61}{42}{Link with adjoint state method}{equation.10.61}{}}
\newlabel{eq-ode-structure}{{62}{42}{Link with adjoint state method}{equation.10.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7}Recurrent Architectures}{42}{subsection.10.7}}
\newlabel{eq-feednets-recur}{{63}{42}{Recurrent Architectures}{equation.10.63}{}}
\newlabel{eq-backprop-discr}{{64}{42}{Recurrent Architectures}{equation.10.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  Recurrent residual perceptron parameterization. }}{43}{figure.26}}
\newlabel{fig-recurrent-param}{{26}{43}{Recurrent residual perceptron parameterization}{figure.26}{}}
\newlabel{eq-jacobian-mlp}{{65}{43}{Recurrent Architectures}{equation.10.65}{}}
\@writefile{toc}{\contentsline {paragraph}{Residual recurrent networks. }{43}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Mitigating memory requirement. }{43}{section*.26}}
\@writefile{toc}{\contentsline {paragraph}{Fixed point maps}{44}{section*.27}}
\newlabel{eq-impl-func-formula}{{66}{44}{Fixed point maps}{equation.10.66}{}}
\@writefile{toc}{\contentsline {paragraph}{Argmin layers}{44}{section*.28}}
\newlabel{eq-argmin-layer}{{67}{44}{Argmin layers}{equation.10.67}{}}
\newlabel{eq-danskin}{{68}{44}{Argmin layers}{equation.10.68}{}}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn's algorithm}{44}{section*.29}}
\bibstyle{plain}
\bibdata{all}
