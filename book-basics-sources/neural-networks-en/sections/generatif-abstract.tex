In the previous article, we saw how to train neural networks in a supervised manner. This makes it possible to effectively solve classification problems, for example image recognition.
%
What is perhaps even more surprising is that these neural networks are also used in an unsupervised manner in order to automatically generate \guill{virtual} texts or images, which are often called \guill{deep fakes}.
%
In this second article, I will draw a link between the learning of generative neural networks and the theory of optimal transport. This problem was framed by Gaspard Monge in the 18$^{\text{th}}$ century, then it was reformulated by Leonid Kantorovitch in the middle of 20$^{\text{th}}$ century. It has now become a tool of choice to tackle important problems in data science.