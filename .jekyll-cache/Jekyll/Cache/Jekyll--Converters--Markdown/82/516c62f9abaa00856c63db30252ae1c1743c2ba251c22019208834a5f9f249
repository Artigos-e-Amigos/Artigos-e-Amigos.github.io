I"><ul>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330461755621978112">@mayfer The animation shows the evolution as epsilon increases (regularuzation increases, so more de</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330450275681918978">@julienmairal Thanks for the pointer! It’s not so surprising that sparsemax appears earlier than 201</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330079868160184320">@roger_mansuy Connais tu un moyen d’en faire un semi-automatiquement ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330072965581565955">@roger_mansuy https://t.co/I1OrEksEn1</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330071779893776388">@roger_mansuy La fonction x^2/y est ma fonction convexe préférée :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330069992575004672">@roger_mansuy Fonction perspective, one of my favorite :) https://t.co/V3MrUCQume</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329691790220414982">@tomabangalore For pairwise energy mesures of the form int_0^1 cost(t,f(t)) dt, the best (discretize</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329690719641100288">@tomabangalore Pick the one closest to identity for some cherry picked distance on the group of diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329674825560956929">@tomabangalore This does not look like a well posed mathematical problem. But maybe instead of takin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328992163766673408">@FfKnighty @_AlecJacobson This being said, I found that in many cases one can only use primal or dua</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328991858924658693">@FfKnighty @_AlecJacobson Not sure this is helpful, but this is a (Matlab – sorry) example of appli</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328659932204441600">@AvramLevitter @_Kcnarf You can cook up configuration where Lloyd alternate between local minimizers</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328424047751946246">@ST4Good Well anyone interested in following my course on Computational Optimal Transport (Monday, 1</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328363608858890240">@BrunoLevy01 @KMMoerman @jorge_pacheco It is thm 20 in my course notes https://t.co/p6HoaMMJhg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328363358886752260">@BrunoLevy01 @KMMoerman @jorge_pacheco Also I think you might prefer not parametrizing the problem u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328362824133996547">@BrunoLevy01 @KMMoerman @jorge_pacheco I am unsure this will reinsure you, but the fact that the PCA</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328351990200545282">@BrunoLevy01 @KMMoerman @jorge_pacheco The line which minimizes the sum of squares of the distances </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328333983982231553">@BrunoLevy01 @KMMoerman @jorge_pacheco Its PCA (minimizes the projection error) :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328316110194827265">@francoisfleuret I think it is pretty standard (at least in Japan I guess). My rule is 6 with lemon </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328313973570035714">@francoisfleuret Soy sauce</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328313806808903680">@amirvaxman_dgp @XiaohuiChen18 Yes I think the radius where the ball start loosing simple connective</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328310566721622017">@amirvaxman_dgp @XiaohuiChen18 I am unsure this is enough, the exp map could be injective without th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328247805534760960">@XiaohuiChen18 This is not true in general (bc geodesic balls are not geodesically convex) but this </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328072927439413251">I am very impressed by the breath and depth of Andrew Witkin contributions (scale space, snakes, str</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328071826958901249">@SoufianeKHIAT Zeros crossing of differential operator (eg Laplacian) define curves on images. So if</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326603243825336320">@FarisYazdi I meant filpping only if you reduce the overall transport cost, so that it terminates.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326603007614660608">@themarklstone The world expert on this topic is Roger  Nussbaum. He wrote a nice review https://t.c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326602193454198785">@themarklstone The cone needs to be a proper convex cone. The statement is that any linear map from </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326601902197510144">Apparently these operators are called “Krauss map” in quantum mechanics. They are the linear maps pr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326600812823457795">As an example, given generic matrices (A_i)_{i=1}^n, the map X in R^{n x n} -&gt; sum_i A_i<em>X</em>A_i^T map</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326598772512731139">What the Perron-Frobenius theorem really is about are convex cones. You can replace the cone of posi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326596069799628800">@FarisYazdi This is why it took 150 years between Monge formulating the problem and Kantorovitch act</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326595826534191105">@FarisYazdi It will terminate (bc there is a finite number of possibility) but it will not find the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326584251723165697">@FarisYazdi This property was mentioned by Gaspard Monge in his original paper. https://t.co/YumHgvi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326583350857854976">And the associated mathematical result is the Perron-Frobenius theorem, which is one of my favorite.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326582677248614400">@FarisYazdi It is because here it minimizes the sum of the <em>squares</em> of the distances. If you minimi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326053794837372928">@BEBischof @docmilanfar For a continuous function on R^d, it is the L^2 norm of the gradient, which </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326050378123718657">@BEBischof Exactly, this was also the comment of @docmilanfar, you encode patterns as local minima o</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326049902145789952">@docmilanfar Exactly! It’s like learning a smoothness prior to encode shapes.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325417606996779008">@graveolens So for the wave equation the traveling pair of Diracs would be moving modulo 1.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325417176090730496">@graveolens The theta function is the green function when using periodic boundary conditions (which </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325391264104189953">@graveolens Do you mean the green functions ? For the wave in 1D it is just a pair of traveling dira</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325390706177880067">@dzakwanfalihh « Partial » refers to the fact that there is 2 variables (space and time). If there i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325029161870692355">@berthier_eloise Definitely!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324689535729651712">@SoutrikTrinity1 Yes I just move around the point y (on a circle) and display how the ratio evolves </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324247487364956160">@Alleycatsphinx Well the display is cryptic :) The central part displays the matrix K while the blue</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324090577772830722">@j_bertolotti @LenaicChizat @DrRBailo This is indeed the way I understood your (very relevant) remar</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324089470304616449">@docmilanfar @j_bertolotti @LenaicChizat @DrRBailo I could not agree more. I find it surprising that</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324071362835681281">@j_bertolotti @LenaicChizat @DrRBailo For pairs of spikes there is a very nice paper of @docmilanfar</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324071001236381696">@j_bertolotti @LenaicChizat @DrRBailo It corresponds to analyzing the super-resolution capability, a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324069938018082816">@j_bertolotti @LenaicChizat @DrRBailo I guess it depends on the meaning of « works » … regarding t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324067462804025350">@t_vayer Avec Filippo Santambrogio comme MC ça va être quelque chose :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323687899624296449">@tomrzah @I_m_a_teapot Seul un MC français pourra se soucier de la convergence des schémas numérique</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323685707706150915">@tomrzah @I_m_a_teapot Peut etre que tu seras retweet par McHammer aussi — ou bien McSolaar :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323363333840490501">@eigenhector The problem then reads   Min_f Max_phi int phi(f(x)) dmu(x) - int f(y) dnu(y)  (where n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323361803368366080">@eigenhector I guess using the language of GANs, f would be the generator and phi the discriminator/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323217494115581952">@eigensteve Exactly! The pushforward is somehow a « degenerated » Markov operator to which one wants</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323182203422023686">@JuanPiCarbajal @vaiter I tried but it modified even more the labels … and the english was not as </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322999492673765383">@delonju @dcoeurjo @CNRS @INS2I_CNRS C’est le barycentre de Wasserstein entre batman et spiderman ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322948997003333635">@vaiter But google translate messing up with the \label and \ref in an inconsistent manner was …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322886151800324096">@pgroisma If you do not threshold the coupling matrix, then for epsilon&gt;0 it is fully connected (eac</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322879170616168450">@pgroisma The permutation obtained for eps=0 is not random (it minimizes the sum of traveled distanc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322545661494153222">@amirvaxman_dgp Just explicit Euler + projection.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321936460447174661">@risi_kondor @kejace @phc27x @dan_rockmore I remember discussing this with you in 2005 in café Reggi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321920390168498177">@kejace @phc27x The difficult questions is wether there are equivalent of the FFT for these non comm</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321919305613758465">@laurentduval @phc27x I will release an english translation as free PDF soon!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321919047190065157">@vaiter Thanks to google translate it should be quite fast … it is just that I actually wrote my b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321749552286863361">@phc27x It is indeed a very good book!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321129194638741504">@galdust My picture is somehow a visual proof of this result :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321108008974372865">@achambertloir It is this paper: https://t.co/Dt9U2FdfMf</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321058396288749568">The asymptotic value of min(S_n)/n being strictly smaller than 1/2 is due to Vladimir Drinfeld. It i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320752471480913920">The webpage of Ken Perlin with details on the oscar and source code. https://t.co/tGFrhupGZU</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320693691552538625">@lisyarus @R4_Unit Here since it is a Gaussian process, stationarity means that the covariance C(x,y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320692809104564225">@t_vayer Yes it is true for any mincost flow problem. It is not true anymore for multimarginal probl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320684232231059456">@t_vayer The OT problem is totally unimodular, is it related to your question ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320427185040220161">@tonysilveti Exactly !!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320427078676787201">@fdecomite @robinhouston Indeed!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320020329884995587">@El_Gauchiste Here for the sake of visualization the lines are chosen in the least efficient way. Ch</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320019947653914624">@El_Gauchiste At each iteration one chooses a line on which the iterate is projected (the line being</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1319684013062295552">The formula for the W2 distance extends nicely to the unbalanced (scaled Gaussians) and entropic reg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1318139002101354496">@HanCao9 That the function lambda -&gt; x(lambda) is affine by part.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317508301584814081">@physics303 (i) compares distribution in a strong sense (if you view probability as density vectors </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317507994272387078">@physics303 The take home message is that for probability distribution, you can either (i) use a phi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317507569498411008">@physics303 Well, you can always bound L2&lt;=sqrt(L1), but using the L2 norm for probability distribut</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317429513769410560">@tomrzah Bc my papers are random permutations of these three inequalities…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317424617598947328">@tomrzah Combining the triangular inequality, Cauchy-Schwartz and Jensen’s inequality …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317371628456677378">@Atrix256 @scottlee It’s going to be a sub-graph of the hypercube graph … https://t.co/2vlD5KglMo </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317347850641170432">@scottlee @Atrix256 The general idea of error correcting codes is to add bits so that the resulting </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317123454030827521">@espadrine @ProbFact I consider x=(x1,x2,x3) that are probability vectors, so that x1+x2+x3=1. Each </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317047748223524864">@vnfrombucharest Bounding the TV is always controversial :) I once get a review complaining that the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317029344758333441">@jitinkapila Well, it is the canonical norm on probability distributions, so it is used everywhere t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317028345633247232">@tbmurphy Indeed. Here is a display of .5*TV/sqrt(1-exp(-KL)). https://t.co/blmT7PszLW</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317023678970908672">@rgrig My understanding is that most probabilists would do this, and most analysts wouldn’t (maybe b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316980724336046082">@ccanonne_ @docmilanfar Thanks for kindly supporting my miserable failures :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316362136931762179">@Mirobertson709 @SciPyTip Yes!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316296982114242560">@betabayesian Yes when tau=0 this is Gauss-Newton.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316288196116914176">The Hessian is the sum of two terms, and L-M only makes use the first term. It ensures that it is a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316281805977333760">@sergecell @srchvrs Indeed, on contrast to Newton it only makes use of the first derivative of x.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1315606580411469824">@f4grx @hbou Yes, it was intended to show that not every shape is admissible.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314830415736320000">@GhoshAvrajit Yes, the set of minimizer is in general a non convex set, think about the function f(x</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314180981079891969">@KyleCranmer « Exact » stability (centroid do not move) is only for p=1 (bc the function is not diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314168336708784128">@qberthet When I met a bunch of people I usually run a l1 minimization in my head to quickly figure </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314139769820061697">@nrui_tweet This is correct, although in dimension larger than 1 most of the time the solution is un</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313752883830173701">@MAKSBoralessa You should consider implementing a wavelet transform instead https://t.co/ZsyCNM93b4</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313565742076506113">@PierreAblin I am the other kind of guy (screen capture from my course of Friday) https://t.co/gPYf8</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313563195869671426">@PierreAblin So you are the kind of guy who considers under-determined problems are the only one wor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313449232267964416">@Al_levity Indeed! The non smooth points of the l1 ball are sparse vectors.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313445442835935233">And also: https://t.co/912fYuzBmj</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313444605971648512">Seems appropriate!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1312066608626630657">@xvrtzn Yes in some sense, it corresponds to the blue approximation error which is a straight line.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311408615262167040">@fakbill @honualx The trick to make amazingly good looking textures for graphics is to pass it throu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311408185908043777">@fakbill @honualx It is just a stationary Gaussian process (filtering white noise with some kernel) </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311378726710542336">@sylefeb @honualx The STAR of Sylvain is more than warmly recommended. And his many contributions on</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311374846685859850">@fakbill @honualx Indeed! https://t.co/fSCS32kTDc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311206125661020160">This relation is often called “Poisson summation formula” https://t.co/NPS4HiYlHj https://t.co/JVA7k</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309917194168434688">@XiaohuiChen18 Yes the wording intrinsic was not well chosen, I meant « geometric ». It shrinks towa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309793755831173121">@JFattaccioli @tomabangalore More on the « shape » section of the NT https://t.co/0ys5Cxqa1j</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309793550436052993">@JFattaccioli @tomabangalore You can find levelset implementation of mean curvature motions and geod</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309782407600574464">@JFattaccioli Not sure this answers your question but adding length penalty is a usual regularizer f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309771629564694528">@Davood_Norouzi At each step you can project on the constraint by adding s<em>curvature</em>normal where s </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309763801433112578">@tomabangalore The affine invariant flow is probably curvature^1/3, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309756019195744257">@ThisEpoch Indeed, maybe a better wording would be « geometric flows ».</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309746071996895233">@tomabangalore Well a circle is an ellipse :) #NeverAdmitYouAreWrong</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309214924414087168">@marc_lelarge @ENS_ULM Que dire de plus … j’ai fait de la pub auprès de mes étudiants. Mais n’oubl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308471090541465608">@n_keriven @alexpghayes Yes, randomized svd!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308459449615618048">@alexpghayes Like using the nuclear norm of a random low dimensional projection ? Poke @n_keriven ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308427244348551168">@MichaelAupetit @AustinRousan This is what I did :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308376622614155264">@LaLetraZeta @MalkymLesdrae @nrui_tweet Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308329781205893126">@sergecell Yes, indeed, tangent planes are the set of speeds of curves traced on the surface. So it </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308324246565924864">@MalkymLesdrae @nrui_tweet Depending on one’s maths background it is probably either trivial or cryp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307700696125448195">The figures are from this very nice review issue of J. Physio. Paris “Neurogeometry and visual perce</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307425647275528194">@tomrzah @MonniauxD @achambertloir Je connais un très bon bouquin sur le sujet :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307409077019054082">@MonniauxD @achambertloir Tu veux dire pour des vecteur à valeur dans ce corps (a la place de C), c’</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306292712581353474">@mraginsky Amazing, thx! As a reward, you’ll get a 5% discount on all the amazing products my future</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306192464932372481">@jm_alexia One can somehow interpret Wassertsein discriminators as (signed) distance functions to th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306191773086220288">@jm_alexia Yes Indeed distances functions are 1-lipschitz. People should replace discriminators by d</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306149225630240768">@lisyarus Adding viscosity and doing an exponential change of variable leads to a heat diffusion (Ho</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306110654894534658">@lisyarus Sorry epsilon*f’’</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306109676132630528">@lisyarus This is bc historicaly the solution was defined by adding epsilon*f’, and letting epsilon </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1305858404599107585">@dushoda Good question, precision impacts the constant in the O(n log(n)), probably on a not very go</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304894307686584320">@markkitti It is a mistake. On the top graph the vertical axis should be y, on the bottom it should </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304798922624376833">@OPirson @roger_mansuy Oui, Shannon et Turing.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304798717619376129">@roger_mansuy Claude Shannon, le père de la théorie de l’information.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304743255154864128">@Dirque_L @madsjw Exactly, of course this was not a typo!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304713063657025536">@sebastien_janas @DamienERNST1 People as mostly relying on classical spaces from analysis of PDEs (e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304712549473103877">@sebastien_janas @DamienERNST1 This does not means that increasing depth is better, just that changi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304712096496615425">@sebastien_janas @DamienERNST1 Good question. When you impose constraints on the speed of convergenc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710990832033792">@DKlemitz This and and its many related implications could/should occupy mathematicians for the next</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710692713496576">@DKlemitz And would also require understanding the properties (eg implicit bias) of the algorithm yo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710215020032000">@DKlemitz This is currently mostly out of reach for deep convolutive architectures and would also re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710103476600832">@DKlemitz To achieve this one needs to undersand the convergence speed of the approximation (bias) a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304709504798466049">@DKlemitz Because the important question in learning is not approximation having access to Infinite </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304005931991207936">Section 2 features a “conic unbalanced optimal transport for dummies” which might be useful for peop</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303957376161255425">@skysurf3000 @antiselfdual This is probably not what you are asking for, but you can link these two </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303428823246135296">@LauretteSTucker @nhigham Exactly! https://t.co/Ku2aHVO3vv</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303410022236516352">@LauretteSTucker @nhigham You could phrase it as the fact that the DFT matrix can be written as the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303399930053169153">@mixlamalice Chaque tutelle (ecole / mairie / etc) a sa propre logique et ses propres protocoles …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303392847966674947">@_ardeej Yes exactly you have to take separately connected components. This is explained on the Wiki</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303356890617909248">@_ardeej Yes exactly they are the so-called upper level sets.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303218678964641792">@burakericok I think there is actualy a third saddle D’’ which is hard to see bc the background leve</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302562779279360000">@El_Gauchiste The isosurface is extracted using marching cubes.</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@lisyarus @_tbng If X is not symmetric you can use</td>
          <td>X</td>
          <td>_p=</td>
          <td>XX^T</td>
          <td>_{p/2}^{1/2}](https://twitter.com/gabrielpeyre/status/1302511800974553090)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302281017038954497">@DFinsterwalder @Alleycatsphinx This is clearly beyond my league, but with only DC and not the full </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302210195112759296">@DFinsterwalder @Alleycatsphinx I believe most mathematicians (including myself) only make use of th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1301903615699845123">@LucaAmb Yes exactly</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1301394618420989952">@cliff_watkins In this paper, Robert McCann introduces the notion of displacement convexity (convexi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1300690007259328512">@k1monfared It is supposed to be more cache-friendly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1300689960194973696">@PincoPallinoQ It is supposed to be more cache-friendly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299756178805792768">@JeremyMMyers @cortogantese @paolagorigiorgi Yes all these distances are only for positive measures.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299711990915620869">@cortogantese @paolagorigiorgi Yes exactly! Singular covariances corresponds to infinitely narrow su</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299711705593925633">@misovalko La Rance, not far away! https://t.co/JYeDo3gLFg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299707404628750336">@cortogantese @paolagorigiorgi It depends wether you want to penalize singular (rank deficient) cova</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299656980911587328">@68kirk For Wassertein the distance is simply the Euclidean distance in 2D</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299593648665919495">@katchwreck @leland_mcinnes There is no closed form for the TV. But it behaves similarly to Hellinge</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299593194020184064">@Mathippaan It shows the « distance » btw 2 Gaussians as you move one of the two. The first Gaussian</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1298158501366239232">@BEBischof This was indeed cryptic. I meant that intuitvely f(x/y) compares how much x/y is close to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1298158158221844480">@DrAndreDavid @BEBischof If they are not normalized (ie x and y are only positive) you need to add t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1297631840849866758">@k1monfared @ProbFact This does not contradict the fact that in the limit the distribution is unifor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1297631005239631872">@k1monfared @ProbFact The number of real eigenvalues of a (n,n) random matrix is ~ sqrt(2/pi)*sqrt(n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295806431426478087">@therealoak111 If Amir Beck says it is true …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295805686652252160">@therealoak111 The paper was probably published quite a long time after.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295805515973500931">@gariguetteman @ValRobert974 Elle est top!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295799220058939394">@therealoak111 I found the information in this beautiful paper of Beck and Sabach , highly recommend</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295765481794809856">Weiszfeld was only 16 when he invented his method.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295420804981719047">@kejace @lisyarus One can also try curvature^1/3 which is affine invariant, and this wonderful geome</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295279928817090561">@lisyarus But if the intend is to smooth a « continuous » curve it is not really appropriate since i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295279869023199232">@lisyarus It is indeed progressively smoothes the polygon.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295085630418976768">@ilarrosac I took this solution from this page https://t.co/5JDtPRxCFM https://t.co/1RR5POGcNy</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295085336654159876">@ilarrosac Yes it is, although it is NP-hard, on small instances one can compute the exact solution.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294998799383965708">For 3 input points the solution is the Fermat point https://t.co/L6bUlMbpb5 and more generally edges</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294561023488339969">Highly recommended https://t.co/QNug8qRN5f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294199967318507520">@Bouh___ @tom_forsyth You guessed correctly, this is the first half of two posts, the second one bei</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294022261889019904">@qberthet A4 considered harmful</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293988111999995907">@GustavoGoretkin This was the initial post https://t.co/4AJ69yTY9T https://t.co/IoPeWEIa4O</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293935505940320259">@GustavoGoretkin This is correct, I forgot about this, but the initial purpose of the figure was to </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293467124892143623">@mathlinux @CMSE_at_MSU Indeed, it is NP hard to compute!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292552493122101259">@ValRobert974 @jmcourty This one is the best of course 😊</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292432574229422085">@JeffDean @HuguesHoppe This paper and those of @wimsweldens and Peter Schroder were the first constr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292208841745145858">@ZanotelliVRT @raymondh But using fractional derivatives would have been better/smarter :) For a fix</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292142117628911619">@QuantumAephraim For instance https://t.co/XNLdewmZfk https://t.co/z6x68KII7I</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291994342467477504">@InertialObservr https://t.co/RERbk7wilY https://t.co/mAH3Fqkeel</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291994056722198531">@InertialObservr I just used linear interpolation … you are out of my league!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291397266926972934">@ddcampayo Exactly, if you need to have access to the velocity and position at the same time, you ne</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291033158570344448">@PetersenGraph Yes but it only works for 3-connected graphs (so somehow the graph of a polyhedron)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290998913638567936">@docmilanfar In sharp contrast, I find the related paper of Andrew Witkin to be crystal clear, in pa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290979472951382019">Progression of the computed embedding as the linear system is being solved by an iterative method (c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290560008183336960">@achambertloir Sorry B should be D but the statement should be clarified. It is that indices i belon</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290028573579976710">@Atrix256 @shachaf Yes exactly. For irrational it is unique and for rational there are 2. I think ..</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290028369233317888">@Atrix256 Rational numbers considered harmful :) But even in this case there are only 2 cf, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1289893056687124481">@LucaAmb Indeed the heat kernel is the standard way to design covariance on manifold, for instance t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1289879666707185665">@LucaAmb The heat kernel is positive (bc the Laplacian is negative) but in general the geodesic kern</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1288852671236116481">@j_bertolotti https://t.co/xBV3CCsmfz</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1288852224345546752">@j_bertolotti Using an environment map https://t.co/p408VHEys8 produces nice mercury-looking results</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@ywang197 @yablak For the Laplacian,</td>
          <td>omega</td>
          <td>^is would be like cos(s*log(</td>
          <td>omega</td>
          <td>)) which does not see](https://twitter.com/gabrielpeyre/status/1287672583991721987)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287336629355581440">@JeKalifa @roger_mansuy 100% d’accord !! J’aurais adoré avoir Roger comme prof de prepa …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287104455369019392">@l__ds Yes exactly it is bc the reference measure is Lebesgue (and also I forgot a normalizing const</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287077201742106627">@ThosVarley Yes it is the same idea as https://t.co/jTbUz7HDe2 It is simply that you can estimate a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286981136233697281">@AtreyeeBanerj10 No you just pick the nearest. You could also use a fixed radius and count # neirghb</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286970976195018753">@LucaAmb I found this one https://t.co/YqCgnknNVD but I think there are others.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286969030042148864">@LucaAmb Yes there are indeed extensions using ratios of nn distances between the clouds.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286967403004526592">@ivrik It is the best reference on the topic imho!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286967262138728449">@crude2refined The normalized formula https://t.co/LCrdmC6rdu https://t.co/R4ZlWHcdfG</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286930752660283392">@ywang197 I think there is a normalization lacking, probably a log(n). Also the entropy with respect</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286930665808830465">@crude2refined I think there is a normalization lacking, probably a log(n). Also the entropy with re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286294574776934401">@TheNrBr LIC is the extreme limit of anisotropic diffusion, which can be understood as the heat diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285848489629540354">@jjvie Bonne idée, avec un premier numéro bien austère sur l’ensemble des contributions de Nesterov </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285625725068939264">@AnzaFabio There is no explicit solution (excepted on simple surface like a sphere). It converges to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285531052786712577">@fayolle @norpadon @swiffydk Exactly ! The Laplacian is the laplace beltrami of the surface, so that</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284195977662955520">@orlitany @HaggaiMaron @GalChechik @EthanFetaya Indeed a great paper!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284112329572048896">@necoleman @littmath Eigen-analysis of nonlinear operators considered harmful!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284110535999594498">@littmath The Laplacian trace(Hessian) should be replaced by the Monge-Ampere operator log(det(Hessi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284091133677764609">@loukasa_tweet My course notes are here https://t.co/iiVDwIeRQW I did the proof with Shannon code wo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284086283548209158">@loukasa_tweet Small remark: the optimal code length in general is not H(p), it can be as large as H</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284086241760288768">@loukasa_tweet The worse length for an optimal scheme on average will be necessarily of the order -l</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284074956058435584">@loukasa_tweet Optimal codes length on average are necessarily of length -log(pi) so I guess they ca</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1283433713112621056">@vatoinblue One can only approximate numerically the solution.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282801564219060230">@arthurmensch I was thinking of arbitrary change of variables, not only 1d. Seems a hard problem (to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282780389162049536">@AlexShtf It is actually true for any p&gt;0 …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282716703219167234">@yiyuezhuo Good question… As you can see I did not have much inspiration on this one…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282693781125574657">@siraferradans Nope, but @nicolas_courty @t_vayer @RFlamary are much sharper than me on these topics</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1280445637897715713">@roger_mansuy Non!! https://t.co/Bt6k5xjNhw</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1280399923440046080">@srimukhsai No, but I should have done this!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1279384928606617606">@ClauselMarianne @MonniauxD @sociobd Nan mais chez nous ca sera au top :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1279032581091201026">@kebabroyal_ Yes and yes!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278961806724280320">@kebabroyal_ I think it is also the simplest way to understand the properties of the heat equations.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278961589249552385">@kebabroyal_ The standard laplacian (1,-2,1) does, the discrete diffusion corresponding to local ave</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278254169716121601">@nerd9723 @roger_mansuy Yes n is time and I linearly interpolate to get a smooth animation. The colo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278253905399463938">@evertedsphere @ValFadeev I used linear interpolation.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277556774346919936">@bodonoghue85 I think it is the cheapest if you need to have access to x and y simultaneously at a g</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277512407426924545">@liwenliang @ankurhandos Indeed, the energy is increasing.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277503140712120326">@devanand_t They are the same, just leapfrog splits the update of y in two parts if one needs to hav</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277500699862011911">@ankurhandos Note sure this helps, but on a circle E=x^2+y^2 with a vector field (y,-x), the energy </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277235337941614597">@jm_alexia This could potentially go faster, but would also be more unstable and oscilate more. I gu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276546948573601793">@rtavenar Ping @mblondel_ml</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276515147708858368">@vadimkantorov I meant among all possible eigenvalues, the one which has maximal modulus. This modul</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276468316467535872">@KarimMakki4 Almost, it is a scaled rotation.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276431377437806593">@Laurent_Daudet Indeed! Nice catch!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276043347883425793">@FranckIutzeler Perron-Frobenius is one of my favorite theorem!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1275503550794268673">@madsjw Indeed! https://t.co/J9XunGn0Nm</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274657786249523203">@DevilleSy @BrKloeckner @fxcoudert Then running a kmeans on the average color should be a good start</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274647845669396480">@DevilleSy @BrKloeckner @fxcoudert By “sorting” you mean ordering them along a 1D axis ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274643988746813441">@achambertloir Wouldn’t ∧ \wedge U+2227 be better ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274643463653527554">@drherryandmrone That was Indeed what I was refering to.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274631007476596736">@SnowFake3 A part of a sphere</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274378318041821185">@sohail__b Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274359412627574786">@sofia75975685 No because here it is the argmin and not the min.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274359243882233856">@radiowhistler Every mathematician is the Bourbakist of another one.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274355209280552962">@sohail__b The construction operates by considering couples (x,r) in X times R_+, but does not actua</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274321264128282626">@GSavare @LenaicCsl @sohail__b The power of the perspective transform! For the interested reader, a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274286026794635272">@LenaicCsl @sohail__b I would not even dare going beyond pi/2! Bc taking the log is so tempting :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274243643935731714">@sohail__b Actually no it works for arbitrary metric space. The only catch is that you need to consi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273918974934253569">@drherryandmrone This would rather be the min and not the argmin.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273703527291015171">@LenaicCsl The preprint is out: https://t.co/Sdn6AaD7r0</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273170227447152642">@FrnkNlsn You figure exactly shows the delicate situation @lisyarus was wondering about, where one l</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273156403025383426">@ian__manchester @lisyarus I think for a quadratic tangency POCS converges at speed 1/sqrt(n) and sl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1272771020521340930">@Alleycatsphinx QR décomposition has countless applications, in particular to solve least squares an</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271831501559271424">@BEBischof I meant that the indexing of the points do not matter, if you relabel the points it defin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271831007461851136">@FunVisualMath Actually it is a mistake, it should be the square norm of the Hessian and not of the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271717525869465608">@LenaicCsl https://t.co/OCwfdiQJrh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271710923581988864">@BEBischof One usually measures the amplitude of the displacements of points (defining the Wasserste</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271710030090338307">@BEBischof So the space of distributions has two natural geometries: 1) the one where you move the p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271709319483056129">@BEBischof I borrowed the naming from https://t.co/lHYMjNfyZt You can view a discrete probability di</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271708396283219968">@bobbythebrain44 @arsatiki Yes exactly I borrowed the name from the similar concept in fluid dynamic</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271547130851835906">@jonathanalis1 @CompSciFact Computer Aided Geometric Design</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271533491440750592">@FunVisualMath Yes x is indeed 2D.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271142427894657026">@t_vayer I think iff conditions are not usefull bc you cannot check them directly from the dual solu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271138442559307781">@josemig91861392 Exactly. You need the regularizarion to blow if you want to impose the use of affin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271098319327105024">@josemig91861392 No it is the other way around, lambda=0 you do not regularize so you impose f(xi)=y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271076043466604545">@AlexShtf It re-enters the picture in the linear system you need to solve to find the parameters a, </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271073944540524547">@CSMLab Oh yes indeed this is big typo ….</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269713731208065025">@cristi_vicas I think forward mode autodiff is not implemented in pytorch, but I can imagine it is n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269661033590857730">@liuyao12 Indeed!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269580172065476610">@omaclaren It seems indeed to be exactly the same concept.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269558482249883648">As noted by several people one can view dual numbers as 2x2 matrices. https://t.co/omLK5oh6K3</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269557551470768129">@egostrum @ylecun Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269556631957917697">@egostrum @ylecun It has the same complexity as finite differences but it gives exact results so it </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269553333494263809">@idhamananta It is a bit like complex numbers. But instead of writing i^2=-1 you write epsilon^2=0. </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269552892006092800">@egostrum @ylecun Dual number is a way to compute the Taylor expansion recursively by applying the c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269210361695293440">@qberthet @PicaudV Indeed …  but for my defense the continuous formula I gave are such that if you</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269164589993902080">@0xhui @BrunoLevy01 @keenanisalive You mean like solving a travelling salesman problem but with some</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269164121272070145">@FranckIutzeler Indeed, the magic constant :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269163870171664384">@PicaudV Yes, although it is not totally obvious that you can transfer convergence speed of the cont</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269163244213678085">@EmilyBendsSpace I interpolated btw the coefficients (and not the functions) so that these functions</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268960950541070339">@tomrzah L^1 vs L^inf</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268960347106525186">@neu_rips @DimitrisPapail @qberthet This is a smart move, thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268806685629517825">@LucasVB - for p=3 it is 1.08 Hölder so C^1, for large p it is ~0.2p Hölder - it is not symmetric fo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268806525495189505">@LucasVB Not sure this helps to get intuition but: - It is parameterized by p the number of vanishin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268543829424189445">@n_keriven That is not true! https://t.co/p2Yf9xeQLM</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268533332599545856">@aayushbansal @ylecun The initial figure was already great. I added colors :) https://t.co/A203MvrDY</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268532763084435457">@n_keriven I guess I should not take it personnaly when former students collaborate together and wri</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268183987790495745">@j824h @Westoncb But you could change this ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268183861432901634">@j824h @Westoncb Oh indeed sorry !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268181041065443329">@j824h @Westoncb They do interact, there is a sum among all particles.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268158371221377024">@sam_power_825 Sorry, I guess this was the meaning of your mathcal{F}</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268158063518851076">@sam_power_825 Exactly, and after you can write this as a Wasserstein gradient flow over probability</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268145601927053312">@amirvaxman_dgp Yes exactly, in the low bandwidth limit particle density solves a non linear heat eq</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268104457881112577">@TehRaio Yes it somehow converges to local modes, so centroids for mixtures of gaussian-like cluster</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268090124853747714">@Westoncb The trajectories do not follow a fixed vector field (although it might look like it is the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267524286165721088">@Pet_Roleum several examples are given in the comments</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267493589057187841">@Submersion13 (not displayed here, but my simulation is Indeed on a 2d torus)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267493285377040388">@Submersion13 On a torus the harmonic part is just a constant, which is the mean of the vector field</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267492662933848076">@ClauselMarianne @chriswolfvision overlearf c’est l’enfer…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267471804043493384">@BenHouston3D @aashay_menace Indeed, best possible illustration of this decomposition.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267347899316346881">@aashay_menace Or in Poisson image editing to project the vector field on gradient fields (so irrota</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267347550140542976">@aashay_menace For instance to impose incompressibility in fluid simulation by projecting the veloci</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267093766625734660">@TheUneuro The only closed forms I know are for Gaussian distribution. @HichamJanati4 found it. Even</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267077362472058885">The left part shows decaying the temperature for an (almost) infinite number of samples. The right p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1266658230790426626">@NdeRancourt Indeed, this is one of the most astonishing things about this map. It corresponds to th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1266029880069808134">@JustinMSolomon Research is cyclic so you are probably ahead of everyone.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265597982830977024">@Alleycatsphinx You meant basis pursuit? Bc matching pursuit is rather sequential.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265362042514477057">@GuillaumeG_ I doubt it is…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265360849843126276">@GuillaumeG_ Like the hessian being strictly positive almost everywhere ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265197435061755904">@tomabangalore Numerically it is the same! One has to re-distance the functon from time to time, to </a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@tomabangalore To be exactly equivalent I think there is an additional</td>
          <td>nabla f</td>
          <td>in front of the div](https://twitter.com/gabrielpeyre/status/1265188025610485760)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265187561389031426">@tomabangalore https://t.co/oTPqbijGY4 https://t.co/gd3VA7VnWw</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264499314111512577">@seismatica Keynote …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264475556940320770">@g_borjan The alpha parameter which controls the smoothness (number of derivatives) varies. The Sobo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264214201637232645">@FrnkNlsn Non symmetric matrices considered harmful :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263784651577065473">@amirvaxman_dgp @Laurett07429292 Why? I think all the fun would be lost in considering diagonalizabl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263599069403865090">@asmeurer All the matrices need to be invertible in this statement.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263578905111191552">@sohail__b The FLT is still linear in any dimension but the domain needs to be discretized on a squa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263572468289191953">@sohail__b It is not as fast as fast legendre transform or using convex hull algorithms, but it is s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263570439848316929">@sohail__b No it works in any dimension.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263567725798076417">@YassineAlouini It is ex 66 in Denis Serre´s list https://t.co/YMNd9GUhr4 https://t.co/ZjnjRz9oJn</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263564965555961856">@GaelVaroquaux The difficulty is to deal with non symmetric matrices.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263556430185074688">@SteffenStatsML @Bord_n Yes the diffculty is to show that a square matrix is an exponential (for the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263555269566631941">@Radegund @AlgebraFact Not really actually… one implication is obvious (exp(x) is the square of ex</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263554787670360064">@CsabaSzepesvari This only shows one direction of the implication. The other one is more involved, i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263554491342872577">@phc27x Exactly !  But checking wether a matrix is the square of another one is not so simple … so</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263553937220743175">@BoyInDaBox89 @AlgebraFact The application I have in mind is to determine which space transformation</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263552912292265985">@SteffenStatsML The issue is the restriction to real matrices (plus possibly dealing with non diagon</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263526178058579968">@Laurett07429292 Exactly :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263519793421959168">@achambertloir Yes exactly, now that I think about it it is indeed cristal clear … I guess once on</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263519123792891919">@pddixit This was exactly my motivation. A transformation is generated by a linear dynamical system </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263518818132992001">@RobJLow Oops, sorry I forgot to say that these matrices need to be invertible ….</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263518182834348032">@achambertloir I was not expecting such a simple description, I was not even sure it was semi algebr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263494226047832064">@dcoeurjo @kebabroyal_ Most TCS people would be perfectly fine with n^7 not being considered as hard</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263376391581642755">@kebabroyal_ Indeed, the kernel is the polar of the convex hull of the polar! This is from a paper b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263172472180027394">The kernel of a polygon can be computed in linear time: D. T. Lee and F. Preparata. An optimal algor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263004692273127424">@kebabroyal_ Not that I am aware of … good question.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262495863936606213">@skoularidou May you be optimally transported back to your home as soon as possible! Best wishes.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262474777236836352">The associated slides are here https://t.co/IwrCmdGy8f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262357431268585472">@neu_rips Here is the proof I wrote for my course (I hope it is correct …). https://t.co/txUxSEVJK</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262355266252783616">@neu_rips If span(F) is dense in continuous function then it metrizes the convergence in law. If F i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262354388066197506">@neu_rips It is often called « flat norm » for people doing geometric measure theory … it is the n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262333024420626433">@ValRobert974 J’espère que c’etait une matrice circulante au moins.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261759121751236608">@TamasGorbe I did a similar animation! Nice. https://t.co/FKcjVOMakM https://t.co/WOWs2iqq5y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647939052675072">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Egalement il faut convenir d’une definition </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647453624905729">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Ma legende est particulièrement naze. Ce qui</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647183251755009">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Oui ca montre les derivees de la Gaussiennes</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261637267745112064">@tomrzah @GuillaumeG_ @Pianocktailiste Fractional derivative vs fractional Laplacian. https://t.co/x</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261626120514011137">@tomrzah @GuillaumeG_ @Pianocktailiste A bas les conditions aux bords et les domaines non compacts.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261611221352144896">@tomrzah @GuillaumeG_ @Pianocktailiste Calculer des racines de i c’est trop dangereux. Vaut mieux se</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260977411426062337">@j_bertolotti You can quantize the output at each step of the integrator, but I guess then it will e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260594552622256129">@Jess_Riedel Well it works well for conservative systems. For many other problems I think Runge-Kutt</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260236604465131520">@brewingsense @JustinMSolomon @sam_power_825 So it is not super useful to get some insight about Gam</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260235745819852812">@brewingsense @JustinMSolomon @sam_power_825 Well, the catch is to properly define which notion of c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260218953625931776">@JustinMSolomon @sam_power_825 The naming « epi-convergence » (convergence of the epigraph) makes mu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259396263709155328">@ValRobert974 \exists x, \forall t&lt;T, …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259395546730684424">@qberthet @PierreAblin Initially written for a Master 2 course, but I am supposed to give a tutorial</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259127700666662913">@ChengSoonOng @GiorgioPatrini Maybe you can try this link ? https://t.co/9b6wyQzION</a></li>
</ul>
:ET