I"r<ul>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330461755621978112">@mayfer The animation shows the evolution as epsilon increases (regularuzation increases, so more de</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330450275681918978">@julienmairal Thanks for the pointer! It’s not so surprising that sparsemax appears earlier than 201</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330079868160184320">@roger_mansuy Connais tu un moyen d’en faire un semi-automatiquement ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330072965581565955">@roger_mansuy https://t.co/I1OrEksEn1</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330071779893776388">@roger_mansuy La fonction x^2/y est ma fonction convexe préférée :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330069992575004672">@roger_mansuy Fonction perspective, one of my favorite :) https://t.co/V3MrUCQume</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329691790220414982">@tomabangalore For pairwise energy mesures of the form int_0^1 cost(t,f(t)) dt, the best (discretize</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329690719641100288">@tomabangalore Pick the one closest to identity for some cherry picked distance on the group of diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329674825560956929">@tomabangalore This does not look like a well posed mathematical problem. But maybe instead of takin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328992163766673408">@FfKnighty @_AlecJacobson This being said, I found that in many cases one can only use primal or dua</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328991858924658693">@FfKnighty @_AlecJacobson Not sure this is helpful, but this is a (Matlab – sorry) example of appli</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328659932204441600">@AvramLevitter @_Kcnarf You can cook up configuration where Lloyd alternate between local minimizers</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328424047751946246">@ST4Good Well anyone interested in following my course on Computational Optimal Transport (Monday, 1</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328363608858890240">@BrunoLevy01 @KMMoerman @jorge_pacheco It is thm 20 in my course notes https://t.co/p6HoaMMJhg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328363358886752260">@BrunoLevy01 @KMMoerman @jorge_pacheco Also I think you might prefer not parametrizing the problem u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328362824133996547">@BrunoLevy01 @KMMoerman @jorge_pacheco I am unsure this will reinsure you, but the fact that the PCA</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328351990200545282">@BrunoLevy01 @KMMoerman @jorge_pacheco The line which minimizes the sum of squares of the distances </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328333983982231553">@BrunoLevy01 @KMMoerman @jorge_pacheco Its PCA (minimizes the projection error) :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328316110194827265">@francoisfleuret I think it is pretty standard (at least in Japan I guess). My rule is 6 with lemon </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328313973570035714">@francoisfleuret Soy sauce</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328313806808903680">@amirvaxman_dgp @XiaohuiChen18 Yes I think the radius where the ball start loosing simple connective</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328310566721622017">@amirvaxman_dgp @XiaohuiChen18 I am unsure this is enough, the exp map could be injective without th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328247805534760960">@XiaohuiChen18 This is not true in general (bc geodesic balls are not geodesically convex) but this </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328072927439413251">I am very impressed by the breath and depth of Andrew Witkin contributions (scale space, snakes, str</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328071826958901249">@SoufianeKHIAT Zeros crossing of differential operator (eg Laplacian) define curves on images. So if</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326603243825336320">@FarisYazdi I meant filpping only if you reduce the overall transport cost, so that it terminates.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326603007614660608">@themarklstone The world expert on this topic is Roger  Nussbaum. He wrote a nice review https://t.c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326602193454198785">@themarklstone The cone needs to be a proper convex cone. The statement is that any linear map from </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326601902197510144">Apparently these operators are called “Krauss map” in quantum mechanics. They are the linear maps pr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326600812823457795">As an example, given generic matrices (A_i)_{i=1}^n, the map X in R^{n x n} -&gt; sum_i A_i<em>X</em>A_i^T map</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326598772512731139">What the Perron-Frobenius theorem really is about are convex cones. You can replace the cone of posi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326596069799628800">@FarisYazdi This is why it took 150 years between Monge formulating the problem and Kantorovitch act</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326595826534191105">@FarisYazdi It will terminate (bc there is a finite number of possibility) but it will not find the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326584251723165697">@FarisYazdi This property was mentioned by Gaspard Monge in his original paper. https://t.co/YumHgvi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326583350857854976">And the associated mathematical result is the Perron-Frobenius theorem, which is one of my favorite.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326582677248614400">@FarisYazdi It is because here it minimizes the sum of the <em>squares</em> of the distances. If you minimi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326053794837372928">@BEBischof @docmilanfar For a continuous function on R^d, it is the L^2 norm of the gradient, which </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326050378123718657">@BEBischof Exactly, this was also the comment of @docmilanfar, you encode patterns as local minima o</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326049902145789952">@docmilanfar Exactly! It’s like learning a smoothness prior to encode shapes.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325417606996779008">@graveolens So for the wave equation the traveling pair of Diracs would be moving modulo 1.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325417176090730496">@graveolens The theta function is the green function when using periodic boundary conditions (which </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325391264104189953">@graveolens Do you mean the green functions ? For the wave in 1D it is just a pair of traveling dira</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325390706177880067">@dzakwanfalihh « Partial » refers to the fact that there is 2 variables (space and time). If there i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325029161870692355">@berthier_eloise Definitely!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324689535729651712">@SoutrikTrinity1 Yes I just move around the point y (on a circle) and display how the ratio evolves </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324247487364956160">@Alleycatsphinx Well the display is cryptic :) The central part displays the matrix K while the blue</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324090577772830722">@j_bertolotti @LenaicChizat @DrRBailo This is indeed the way I understood your (very relevant) remar</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324089470304616449">@docmilanfar @j_bertolotti @LenaicChizat @DrRBailo I could not agree more. I find it surprising that</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324071362835681281">@j_bertolotti @LenaicChizat @DrRBailo For pairs of spikes there is a very nice paper of @docmilanfar</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324071001236381696">@j_bertolotti @LenaicChizat @DrRBailo It corresponds to analyzing the super-resolution capability, a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324069938018082816">@j_bertolotti @LenaicChizat @DrRBailo I guess it depends on the meaning of « works » … regarding t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324067462804025350">@t_vayer Avec Filippo Santambrogio comme MC ça va être quelque chose :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323687899624296449">@tomrzah @I_m_a_teapot Seul un MC français pourra se soucier de la convergence des schémas numérique</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323685707706150915">@tomrzah @I_m_a_teapot Peut etre que tu seras retweet par McHammer aussi — ou bien McSolaar :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323363333840490501">@eigenhector The problem then reads   Min_f Max_phi int phi(f(x)) dmu(x) - int f(y) dnu(y)  (where n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323361803368366080">@eigenhector I guess using the language of GANs, f would be the generator and phi the discriminator/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323217494115581952">@eigensteve Exactly! The pushforward is somehow a « degenerated » Markov operator to which one wants</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323182203422023686">@JuanPiCarbajal @vaiter I tried but it modified even more the labels … and the english was not as </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322999492673765383">@delonju @dcoeurjo @CNRS @INS2I_CNRS C’est le barycentre de Wasserstein entre batman et spiderman ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322948997003333635">@vaiter But google translate messing up with the \label and \ref in an inconsistent manner was …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322886151800324096">@pgroisma If you do not threshold the coupling matrix, then for epsilon&gt;0 it is fully connected (eac</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322879170616168450">@pgroisma The permutation obtained for eps=0 is not random (it minimizes the sum of traveled distanc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322545661494153222">@amirvaxman_dgp Just explicit Euler + projection.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321936460447174661">@risi_kondor @kejace @phc27x @dan_rockmore I remember discussing this with you in 2005 in café Reggi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321920390168498177">@kejace @phc27x The difficult questions is wether there are equivalent of the FFT for these non comm</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321919305613758465">@laurentduval @phc27x I will release an english translation as free PDF soon!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321919047190065157">@vaiter Thanks to google translate it should be quite fast … it is just that I actually wrote my b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321749552286863361">@phc27x It is indeed a very good book!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321129194638741504">@galdust My picture is somehow a visual proof of this result :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321108008974372865">@achambertloir It is this paper: https://t.co/Dt9U2FdfMf</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321058396288749568">The asymptotic value of min(S_n)/n being strictly smaller than 1/2 is due to Vladimir Drinfeld. It i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320752471480913920">The webpage of Ken Perlin with details on the oscar and source code. https://t.co/tGFrhupGZU</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320693691552538625">@lisyarus @R4_Unit Here since it is a Gaussian process, stationarity means that the covariance C(x,y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320692809104564225">@t_vayer Yes it is true for any mincost flow problem. It is not true anymore for multimarginal probl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320684232231059456">@t_vayer The OT problem is totally unimodular, is it related to your question ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320427185040220161">@tonysilveti Exactly !!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320427078676787201">@fdecomite @robinhouston Indeed!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320020329884995587">@El_Gauchiste Here for the sake of visualization the lines are chosen in the least efficient way. Ch</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320019947653914624">@El_Gauchiste At each iteration one chooses a line on which the iterate is projected (the line being</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1319684013062295552">The formula for the W2 distance extends nicely to the unbalanced (scaled Gaussians) and entropic reg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1318139002101354496">@HanCao9 That the function lambda -&gt; x(lambda) is affine by part.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317508301584814081">@physics303 (i) compares distribution in a strong sense (if you view probability as density vectors </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317507994272387078">@physics303 The take home message is that for probability distribution, you can either (i) use a phi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317507569498411008">@physics303 Well, you can always bound L2&lt;=sqrt(L1), but using the L2 norm for probability distribut</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317429513769410560">@tomrzah Bc my papers are random permutations of these three inequalities…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317424617598947328">@tomrzah Combining the triangular inequality, Cauchy-Schwartz and Jensen’s inequality …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317371628456677378">@Atrix256 @scottlee It’s going to be a sub-graph of the hypercube graph … https://t.co/2vlD5KglMo </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317347850641170432">@scottlee @Atrix256 The general idea of error correcting codes is to add bits so that the resulting </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317123454030827521">@espadrine @ProbFact I consider x=(x1,x2,x3) that are probability vectors, so that x1+x2+x3=1. Each </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317047748223524864">@vnfrombucharest Bounding the TV is always controversial :) I once get a review complaining that the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317029344758333441">@jitinkapila Well, it is the canonical norm on probability distributions, so it is used everywhere t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317028345633247232">@tbmurphy Indeed. Here is a display of .5*TV/sqrt(1-exp(-KL)). https://t.co/blmT7PszLW</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317023678970908672">@rgrig My understanding is that most probabilists would do this, and most analysts wouldn’t (maybe b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316980724336046082">@ccanonne_ @docmilanfar Thanks for kindly supporting my miserable failures :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316362136931762179">@Mirobertson709 @SciPyTip Yes!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316296982114242560">@betabayesian Yes when tau=0 this is Gauss-Newton.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316288196116914176">The Hessian is the sum of two terms, and L-M only makes use the first term. It ensures that it is a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316281805977333760">@sergecell @srchvrs Indeed, on contrast to Newton it only makes use of the first derivative of x.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1315606580411469824">@f4grx @hbou Yes, it was intended to show that not every shape is admissible.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314830415736320000">@GhoshAvrajit Yes, the set of minimizer is in general a non convex set, think about the function f(x</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314180981079891969">@KyleCranmer « Exact » stability (centroid do not move) is only for p=1 (bc the function is not diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314168336708784128">@qberthet When I met a bunch of people I usually run a l1 minimization in my head to quickly figure </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314139769820061697">@nrui_tweet This is correct, although in dimension larger than 1 most of the time the solution is un</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313752883830173701">@MAKSBoralessa You should consider implementing a wavelet transform instead https://t.co/ZsyCNM93b4</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313565742076506113">@PierreAblin I am the other kind of guy (screen capture from my course of Friday) https://t.co/gPYf8</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313563195869671426">@PierreAblin So you are the kind of guy who considers under-determined problems are the only one wor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313449232267964416">@Al_levity Indeed! The non smooth points of the l1 ball are sparse vectors.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313445442835935233">And also: https://t.co/912fYuzBmj</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313444605971648512">Seems appropriate!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1312066608626630657">@xvrtzn Yes in some sense, it corresponds to the blue approximation error which is a straight line.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311408615262167040">@fakbill @honualx The trick to make amazingly good looking textures for graphics is to pass it throu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311408185908043777">@fakbill @honualx It is just a stationary Gaussian process (filtering white noise with some kernel) </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311378726710542336">@sylefeb @honualx The STAR of Sylvain is more than warmly recommended. And his many contributions on</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311374846685859850">@fakbill @honualx Indeed! https://t.co/fSCS32kTDc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311206125661020160">This relation is often called “Poisson summation formula” https://t.co/NPS4HiYlHj https://t.co/JVA7k</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309917194168434688">@XiaohuiChen18 Yes the wording intrinsic was not well chosen, I meant « geometric ». It shrinks towa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309793755831173121">@JFattaccioli @tomabangalore More on the « shape » section of the NT https://t.co/0ys5Cxqa1j</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309793550436052993">@JFattaccioli @tomabangalore You can find levelset implementation of mean curvature motions and geod</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309782407600574464">@JFattaccioli Not sure this answers your question but adding length penalty is a usual regularizer f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309771629564694528">@Davood_Norouzi At each step you can project on the constraint by adding s<em>curvature</em>normal where s </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309763801433112578">@tomabangalore The affine invariant flow is probably curvature^1/3, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309756019195744257">@ThisEpoch Indeed, maybe a better wording would be « geometric flows ».</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309746071996895233">@tomabangalore Well a circle is an ellipse :) #NeverAdmitYouAreWrong</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309214924414087168">@marc_lelarge @ENS_ULM Que dire de plus … j’ai fait de la pub auprès de mes étudiants. Mais n’oubl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308471090541465608">@n_keriven @alexpghayes Yes, randomized svd!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308459449615618048">@alexpghayes Like using the nuclear norm of a random low dimensional projection ? Poke @n_keriven ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308427244348551168">@MichaelAupetit @AustinRousan This is what I did :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308376622614155264">@LaLetraZeta @MalkymLesdrae @nrui_tweet Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308329781205893126">@sergecell Yes, indeed, tangent planes are the set of speeds of curves traced on the surface. So it </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308324246565924864">@MalkymLesdrae @nrui_tweet Depending on one’s maths background it is probably either trivial or cryp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307700696125448195">The figures are from this very nice review issue of J. Physio. Paris “Neurogeometry and visual perce</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307425647275528194">@tomrzah @MonniauxD @achambertloir Je connais un très bon bouquin sur le sujet :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307409077019054082">@MonniauxD @achambertloir Tu veux dire pour des vecteur à valeur dans ce corps (a la place de C), c’</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306292712581353474">@mraginsky Amazing, thx! As a reward, you’ll get a 5% discount on all the amazing products my future</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306192464932372481">@jm_alexia One can somehow interpret Wassertsein discriminators as (signed) distance functions to th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306191773086220288">@jm_alexia Yes Indeed distances functions are 1-lipschitz. People should replace discriminators by d</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306149225630240768">@lisyarus Adding viscosity and doing an exponential change of variable leads to a heat diffusion (Ho</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306110654894534658">@lisyarus Sorry epsilon*f’’</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306109676132630528">@lisyarus This is bc historicaly the solution was defined by adding epsilon*f’, and letting epsilon </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1305858404599107585">@dushoda Good question, precision impacts the constant in the O(n log(n)), probably on a not very go</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304894307686584320">@markkitti It is a mistake. On the top graph the vertical axis should be y, on the bottom it should </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304798922624376833">@OPirson @roger_mansuy Oui, Shannon et Turing.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304798717619376129">@roger_mansuy Claude Shannon, le père de la théorie de l’information.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304743255154864128">@Dirque_L @madsjw Exactly, of course this was not a typo!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304713063657025536">@sebastien_janas @DamienERNST1 People as mostly relying on classical spaces from analysis of PDEs (e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304712549473103877">@sebastien_janas @DamienERNST1 This does not means that increasing depth is better, just that changi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304712096496615425">@sebastien_janas @DamienERNST1 Good question. When you impose constraints on the speed of convergenc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710990832033792">@DKlemitz This and and its many related implications could/should occupy mathematicians for the next</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710692713496576">@DKlemitz And would also require understanding the properties (eg implicit bias) of the algorithm yo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710215020032000">@DKlemitz This is currently mostly out of reach for deep convolutive architectures and would also re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710103476600832">@DKlemitz To achieve this one needs to undersand the convergence speed of the approximation (bias) a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304709504798466049">@DKlemitz Because the important question in learning is not approximation having access to Infinite </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304005931991207936">Section 2 features a “conic unbalanced optimal transport for dummies” which might be useful for peop</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303957376161255425">@skysurf3000 @antiselfdual This is probably not what you are asking for, but you can link these two </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303428823246135296">@LauretteSTucker @nhigham Exactly! https://t.co/Ku2aHVO3vv</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303410022236516352">@LauretteSTucker @nhigham You could phrase it as the fact that the DFT matrix can be written as the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303399930053169153">@mixlamalice Chaque tutelle (ecole / mairie / etc) a sa propre logique et ses propres protocoles …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303392847966674947">@_ardeej Yes exactly you have to take separately connected components. This is explained on the Wiki</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303356890617909248">@_ardeej Yes exactly they are the so-called upper level sets.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303218678964641792">@burakericok I think there is actualy a third saddle D’’ which is hard to see bc the background leve</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302562779279360000">@El_Gauchiste The isosurface is extracted using marching cubes.</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@lisyarus @_tbng If X is not symmetric you can use</td>
          <td>X</td>
          <td>_p=</td>
          <td>XX^T</td>
          <td>_{p/2}^{1/2}](https://twitter.com/gabrielpeyre/status/1302511800974553090)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302281017038954497">@DFinsterwalder @Alleycatsphinx This is clearly beyond my league, but with only DC and not the full </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302210195112759296">@DFinsterwalder @Alleycatsphinx I believe most mathematicians (including myself) only make use of th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1301903615699845123">@LucaAmb Yes exactly</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1301394618420989952">@cliff_watkins In this paper, Robert McCann introduces the notion of displacement convexity (convexi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1300690007259328512">@k1monfared It is supposed to be more cache-friendly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1300689960194973696">@PincoPallinoQ It is supposed to be more cache-friendly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299756178805792768">@JeremyMMyers @cortogantese @paolagorigiorgi Yes all these distances are only for positive measures.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299711990915620869">@cortogantese @paolagorigiorgi Yes exactly! Singular covariances corresponds to infinitely narrow su</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299711705593925633">@misovalko La Rance, not far away! https://t.co/JYeDo3gLFg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299707404628750336">@cortogantese @paolagorigiorgi It depends wether you want to penalize singular (rank deficient) cova</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299656980911587328">@68kirk For Wassertein the distance is simply the Euclidean distance in 2D</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299593648665919495">@katchwreck @leland_mcinnes There is no closed form for the TV. But it behaves similarly to Hellinge</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299593194020184064">@Mathippaan It shows the « distance » btw 2 Gaussians as you move one of the two. The first Gaussian</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1298158501366239232">@BEBischof This was indeed cryptic. I meant that intuitvely f(x/y) compares how much x/y is close to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1298158158221844480">@DrAndreDavid @BEBischof If they are not normalized (ie x and y are only positive) you need to add t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1297631840849866758">@k1monfared @ProbFact This does not contradict the fact that in the limit the distribution is unifor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1297631005239631872">@k1monfared @ProbFact The number of real eigenvalues of a (n,n) random matrix is ~ sqrt(2/pi)*sqrt(n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295806431426478087">@therealoak111 If Amir Beck says it is true …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295805686652252160">@therealoak111 The paper was probably published quite a long time after.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295805515973500931">@gariguetteman @ValRobert974 Elle est top!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295799220058939394">@therealoak111 I found the information in this beautiful paper of Beck and Sabach , highly recommend</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295765481794809856">Weiszfeld was only 16 when he invented his method.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295420804981719047">@kejace @lisyarus One can also try curvature^1/3 which is affine invariant, and this wonderful geome</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295279928817090561">@lisyarus But if the intend is to smooth a « continuous » curve it is not really appropriate since i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295279869023199232">@lisyarus It is indeed progressively smoothes the polygon.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295085630418976768">@ilarrosac I took this solution from this page https://t.co/5JDtPRxCFM https://t.co/1RR5POGcNy</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295085336654159876">@ilarrosac Yes it is, although it is NP-hard, on small instances one can compute the exact solution.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294998799383965708">For 3 input points the solution is the Fermat point https://t.co/L6bUlMbpb5 and more generally edges</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294561023488339969">Highly recommended https://t.co/QNug8qRN5f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294199967318507520">@Bouh___ @tom_forsyth You guessed correctly, this is the first half of two posts, the second one bei</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294022261889019904">@qberthet A4 considered harmful</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293988111999995907">@GustavoGoretkin This was the initial post https://t.co/4AJ69yTY9T https://t.co/IoPeWEIa4O</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293935505940320259">@GustavoGoretkin This is correct, I forgot about this, but the initial purpose of the figure was to </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293467124892143623">@mathlinux @CMSE_at_MSU Indeed, it is NP hard to compute!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292552493122101259">@ValRobert974 @jmcourty This one is the best of course 😊</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292432574229422085">@JeffDean @HuguesHoppe This paper and those of @wimsweldens and Peter Schroder were the first constr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292208841745145858">@ZanotelliVRT @raymondh But using fractional derivatives would have been better/smarter :) For a fix</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292142117628911619">@QuantumAephraim For instance https://t.co/XNLdewmZfk https://t.co/z6x68KII7I</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291994342467477504">@InertialObservr https://t.co/RERbk7wilY https://t.co/mAH3Fqkeel</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291994056722198531">@InertialObservr I just used linear interpolation … you are out of my league!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291397266926972934">@ddcampayo Exactly, if you need to have access to the velocity and position at the same time, you ne</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291033158570344448">@PetersenGraph Yes but it only works for 3-connected graphs (so somehow the graph of a polyhedron)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290998913638567936">@docmilanfar In sharp contrast, I find the related paper of Andrew Witkin to be crystal clear, in pa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290979472951382019">Progression of the computed embedding as the linear system is being solved by an iterative method (c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290560008183336960">@achambertloir Sorry B should be D but the statement should be clarified. It is that indices i belon</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290028573579976710">@Atrix256 @shachaf Yes exactly. For irrational it is unique and for rational there are 2. I think ..</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290028369233317888">@Atrix256 Rational numbers considered harmful :) But even in this case there are only 2 cf, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1289893056687124481">@LucaAmb Indeed the heat kernel is the standard way to design covariance on manifold, for instance t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1289879666707185665">@LucaAmb The heat kernel is positive (bc the Laplacian is negative) but in general the geodesic kern</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1288852671236116481">@j_bertolotti https://t.co/xBV3CCsmfz</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1288852224345546752">@j_bertolotti Using an environment map https://t.co/p408VHEys8 produces nice mercury-looking results</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@ywang197 @yablak For the Laplacian,</td>
          <td>omega</td>
          <td>^is would be like cos(s*log(</td>
          <td>omega</td>
          <td>)) which does not see](https://twitter.com/gabrielpeyre/status/1287672583991721987)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287336629355581440">@JeKalifa @roger_mansuy 100% d’accord !! J’aurais adoré avoir Roger comme prof de prepa …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287104455369019392">@l__ds Yes exactly it is bc the reference measure is Lebesgue (and also I forgot a normalizing const</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287077201742106627">@ThosVarley Yes it is the same idea as https://t.co/jTbUz7HDe2 It is simply that you can estimate a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286981136233697281">@AtreyeeBanerj10 No you just pick the nearest. You could also use a fixed radius and count # neirghb</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286970976195018753">@LucaAmb I found this one https://t.co/YqCgnknNVD but I think there are others.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286969030042148864">@LucaAmb Yes there are indeed extensions using ratios of nn distances between the clouds.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286967403004526592">@ivrik It is the best reference on the topic imho!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286967262138728449">@crude2refined The normalized formula https://t.co/LCrdmC6rdu https://t.co/R4ZlWHcdfG</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286930752660283392">@ywang197 I think there is a normalization lacking, probably a log(n). Also the entropy with respect</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286930665808830465">@crude2refined I think there is a normalization lacking, probably a log(n). Also the entropy with re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286294574776934401">@TheNrBr LIC is the extreme limit of anisotropic diffusion, which can be understood as the heat diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285848489629540354">@jjvie Bonne idée, avec un premier numéro bien austère sur l’ensemble des contributions de Nesterov </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285625725068939264">@AnzaFabio There is no explicit solution (excepted on simple surface like a sphere). It converges to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285531052786712577">@fayolle @norpadon @swiffydk Exactly ! The Laplacian is the laplace beltrami of the surface, so that</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284195977662955520">@orlitany @HaggaiMaron @GalChechik @EthanFetaya Indeed a great paper!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284112329572048896">@necoleman @littmath Eigen-analysis of nonlinear operators considered harmful!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284110535999594498">@littmath The Laplacian trace(Hessian) should be replaced by the Monge-Ampere operator log(det(Hessi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284091133677764609">@loukasa_tweet My course notes are here https://t.co/iiVDwIeRQW I did the proof with Shannon code wo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284086283548209158">@loukasa_tweet Small remark: the optimal code length in general is not H(p), it can be as large as H</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284086241760288768">@loukasa_tweet The worse length for an optimal scheme on average will be necessarily of the order -l</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284074956058435584">@loukasa_tweet Optimal codes length on average are necessarily of length -log(pi) so I guess they ca</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1283433713112621056">@vatoinblue One can only approximate numerically the solution.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282801564219060230">@arthurmensch I was thinking of arbitrary change of variables, not only 1d. Seems a hard problem (to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282780389162049536">@AlexShtf It is actually true for any p&gt;0 …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282716703219167234">@yiyuezhuo Good question… As you can see I did not have much inspiration on this one…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282693781125574657">@siraferradans Nope, but @nicolas_courty @t_vayer @RFlamary are much sharper than me on these topics</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1280445637897715713">@roger_mansuy Non!! https://t.co/Bt6k5xjNhw</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1280399923440046080">@srimukhsai No, but I should have done this!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1279384928606617606">@ClauselMarianne @MonniauxD @sociobd Nan mais chez nous ca sera au top :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1279032581091201026">@kebabroyal_ Yes and yes!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278961806724280320">@kebabroyal_ I think it is also the simplest way to understand the properties of the heat equations.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278961589249552385">@kebabroyal_ The standard laplacian (1,-2,1) does, the discrete diffusion corresponding to local ave</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278254169716121601">@nerd9723 @roger_mansuy Yes n is time and I linearly interpolate to get a smooth animation. The colo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278253905399463938">@evertedsphere @ValFadeev I used linear interpolation.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277556774346919936">@bodonoghue85 I think it is the cheapest if you need to have access to x and y simultaneously at a g</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277512407426924545">@liwenliang @ankurhandos Indeed, the energy is increasing.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277503140712120326">@devanand_t They are the same, just leapfrog splits the update of y in two parts if one needs to hav</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277500699862011911">@ankurhandos Note sure this helps, but on a circle E=x^2+y^2 with a vector field (y,-x), the energy </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277235337941614597">@jm_alexia This could potentially go faster, but would also be more unstable and oscilate more. I gu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276546948573601793">@rtavenar Ping @mblondel_ml</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276515147708858368">@vadimkantorov I meant among all possible eigenvalues, the one which has maximal modulus. This modul</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276468316467535872">@KarimMakki4 Almost, it is a scaled rotation.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276431377437806593">@Laurent_Daudet Indeed! Nice catch!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276043347883425793">@FranckIutzeler Perron-Frobenius is one of my favorite theorem!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1275503550794268673">@madsjw Indeed! https://t.co/J9XunGn0Nm</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274657786249523203">@DevilleSy @BrKloeckner @fxcoudert Then running a kmeans on the average color should be a good start</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274647845669396480">@DevilleSy @BrKloeckner @fxcoudert By “sorting” you mean ordering them along a 1D axis ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274643988746813441">@achambertloir Wouldn’t ∧ \wedge U+2227 be better ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274643463653527554">@drherryandmrone That was Indeed what I was refering to.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274631007476596736">@SnowFake3 A part of a sphere</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274378318041821185">@sohail__b Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274359412627574786">@sofia75975685 No because here it is the argmin and not the min.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274359243882233856">@radiowhistler Every mathematician is the Bourbakist of another one.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274355209280552962">@sohail__b The construction operates by considering couples (x,r) in X times R_+, but does not actua</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274321264128282626">@GSavare @LenaicCsl @sohail__b The power of the perspective transform! For the interested reader, a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274286026794635272">@LenaicCsl @sohail__b I would not even dare going beyond pi/2! Bc taking the log is so tempting :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274243643935731714">@sohail__b Actually no it works for arbitrary metric space. The only catch is that you need to consi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273918974934253569">@drherryandmrone This would rather be the min and not the argmin.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273703527291015171">@LenaicCsl The preprint is out: https://t.co/Sdn6AaD7r0</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273170227447152642">@FrnkNlsn You figure exactly shows the delicate situation @lisyarus was wondering about, where one l</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273156403025383426">@ian__manchester @lisyarus I think for a quadratic tangency POCS converges at speed 1/sqrt(n) and sl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1272771020521340930">@Alleycatsphinx QR décomposition has countless applications, in particular to solve least squares an</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271831501559271424">@BEBischof I meant that the indexing of the points do not matter, if you relabel the points it defin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271831007461851136">@FunVisualMath Actually it is a mistake, it should be the square norm of the Hessian and not of the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271717525869465608">@LenaicCsl https://t.co/OCwfdiQJrh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271710923581988864">@BEBischof One usually measures the amplitude of the displacements of points (defining the Wasserste</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271710030090338307">@BEBischof So the space of distributions has two natural geometries: 1) the one where you move the p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271709319483056129">@BEBischof I borrowed the naming from https://t.co/lHYMjNfyZt You can view a discrete probability di</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271708396283219968">@bobbythebrain44 @arsatiki Yes exactly I borrowed the name from the similar concept in fluid dynamic</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271547130851835906">@jonathanalis1 @CompSciFact Computer Aided Geometric Design</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271533491440750592">@FunVisualMath Yes x is indeed 2D.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271142427894657026">@t_vayer I think iff conditions are not usefull bc you cannot check them directly from the dual solu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271138442559307781">@josemig91861392 Exactly. You need the regularizarion to blow if you want to impose the use of affin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271098319327105024">@josemig91861392 No it is the other way around, lambda=0 you do not regularize so you impose f(xi)=y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271076043466604545">@AlexShtf It re-enters the picture in the linear system you need to solve to find the parameters a, </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271073944540524547">@CSMLab Oh yes indeed this is big typo ….</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269713731208065025">@cristi_vicas I think forward mode autodiff is not implemented in pytorch, but I can imagine it is n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269661033590857730">@liuyao12 Indeed!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269580172065476610">@omaclaren It seems indeed to be exactly the same concept.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269558482249883648">As noted by several people one can view dual numbers as 2x2 matrices. https://t.co/omLK5oh6K3</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269557551470768129">@egostrum @ylecun Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269556631957917697">@egostrum @ylecun It has the same complexity as finite differences but it gives exact results so it </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269553333494263809">@idhamananta It is a bit like complex numbers. But instead of writing i^2=-1 you write epsilon^2=0. </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269552892006092800">@egostrum @ylecun Dual number is a way to compute the Taylor expansion recursively by applying the c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269210361695293440">@qberthet @PicaudV Indeed …  but for my defense the continuous formula I gave are such that if you</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269164589993902080">@0xhui @BrunoLevy01 @keenanisalive You mean like solving a travelling salesman problem but with some</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269164121272070145">@FranckIutzeler Indeed, the magic constant :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269163870171664384">@PicaudV Yes, although it is not totally obvious that you can transfer convergence speed of the cont</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269163244213678085">@EmilyBendsSpace I interpolated btw the coefficients (and not the functions) so that these functions</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268960950541070339">@tomrzah L^1 vs L^inf</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268960347106525186">@neu_rips @DimitrisPapail @qberthet This is a smart move, thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268806685629517825">@LucasVB - for p=3 it is 1.08 Hölder so C^1, for large p it is ~0.2p Hölder - it is not symmetric fo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268806525495189505">@LucasVB Not sure this helps to get intuition but: - It is parameterized by p the number of vanishin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268543829424189445">@n_keriven That is not true! https://t.co/p2Yf9xeQLM</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268533332599545856">@aayushbansal @ylecun The initial figure was already great. I added colors :) https://t.co/A203MvrDY</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268532763084435457">@n_keriven I guess I should not take it personnaly when former students collaborate together and wri</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268183987790495745">@j824h @Westoncb But you could change this ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268183861432901634">@j824h @Westoncb Oh indeed sorry !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268181041065443329">@j824h @Westoncb They do interact, there is a sum among all particles.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268158371221377024">@sam_power_825 Sorry, I guess this was the meaning of your mathcal{F}</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268158063518851076">@sam_power_825 Exactly, and after you can write this as a Wasserstein gradient flow over probability</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268145601927053312">@amirvaxman_dgp Yes exactly, in the low bandwidth limit particle density solves a non linear heat eq</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268104457881112577">@TehRaio Yes it somehow converges to local modes, so centroids for mixtures of gaussian-like cluster</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268090124853747714">@Westoncb The trajectories do not follow a fixed vector field (although it might look like it is the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267524286165721088">@Pet_Roleum several examples are given in the comments</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267493589057187841">@Submersion13 (not displayed here, but my simulation is Indeed on a 2d torus)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267493285377040388">@Submersion13 On a torus the harmonic part is just a constant, which is the mean of the vector field</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267492662933848076">@ClauselMarianne @chriswolfvision overlearf c’est l’enfer…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267471804043493384">@BenHouston3D @aashay_menace Indeed, best possible illustration of this decomposition.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267347899316346881">@aashay_menace Or in Poisson image editing to project the vector field on gradient fields (so irrota</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267347550140542976">@aashay_menace For instance to impose incompressibility in fluid simulation by projecting the veloci</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267093766625734660">@TheUneuro The only closed forms I know are for Gaussian distribution. @HichamJanati4 found it. Even</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267077362472058885">The left part shows decaying the temperature for an (almost) infinite number of samples. The right p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1266658230790426626">@NdeRancourt Indeed, this is one of the most astonishing things about this map. It corresponds to th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1266029880069808134">@JustinMSolomon Research is cyclic so you are probably ahead of everyone.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265597982830977024">@Alleycatsphinx You meant basis pursuit? Bc matching pursuit is rather sequential.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265362042514477057">@GuillaumeG_ I doubt it is…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265360849843126276">@GuillaumeG_ Like the hessian being strictly positive almost everywhere ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265197435061755904">@tomabangalore Numerically it is the same! One has to re-distance the functon from time to time, to </a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@tomabangalore To be exactly equivalent I think there is an additional</td>
          <td>nabla f</td>
          <td>in front of the div](https://twitter.com/gabrielpeyre/status/1265188025610485760)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265187561389031426">@tomabangalore https://t.co/oTPqbijGY4 https://t.co/gd3VA7VnWw</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264499314111512577">@seismatica Keynote …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264475556940320770">@g_borjan The alpha parameter which controls the smoothness (number of derivatives) varies. The Sobo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264214201637232645">@FrnkNlsn Non symmetric matrices considered harmful :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263784651577065473">@amirvaxman_dgp @Laurett07429292 Why? I think all the fun would be lost in considering diagonalizabl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263599069403865090">@asmeurer All the matrices need to be invertible in this statement.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263578905111191552">@sohail__b The FLT is still linear in any dimension but the domain needs to be discretized on a squa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263572468289191953">@sohail__b It is not as fast as fast legendre transform or using convex hull algorithms, but it is s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263570439848316929">@sohail__b No it works in any dimension.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263567725798076417">@YassineAlouini It is ex 66 in Denis Serre´s list https://t.co/YMNd9GUhr4 https://t.co/ZjnjRz9oJn</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263564965555961856">@GaelVaroquaux The difficulty is to deal with non symmetric matrices.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263556430185074688">@SteffenStatsML @Bord_n Yes the diffculty is to show that a square matrix is an exponential (for the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263555269566631941">@Radegund @AlgebraFact Not really actually… one implication is obvious (exp(x) is the square of ex</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263554787670360064">@CsabaSzepesvari This only shows one direction of the implication. The other one is more involved, i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263554491342872577">@phc27x Exactly !  But checking wether a matrix is the square of another one is not so simple … so</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263553937220743175">@BoyInDaBox89 @AlgebraFact The application I have in mind is to determine which space transformation</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263552912292265985">@SteffenStatsML The issue is the restriction to real matrices (plus possibly dealing with non diagon</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263526178058579968">@Laurett07429292 Exactly :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263519793421959168">@achambertloir Yes exactly, now that I think about it it is indeed cristal clear … I guess once on</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263519123792891919">@pddixit This was exactly my motivation. A transformation is generated by a linear dynamical system </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263518818132992001">@RobJLow Oops, sorry I forgot to say that these matrices need to be invertible ….</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263518182834348032">@achambertloir I was not expecting such a simple description, I was not even sure it was semi algebr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263494226047832064">@dcoeurjo @kebabroyal_ Most TCS people would be perfectly fine with n^7 not being considered as hard</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263376391581642755">@kebabroyal_ Indeed, the kernel is the polar of the convex hull of the polar! This is from a paper b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263172472180027394">The kernel of a polygon can be computed in linear time: D. T. Lee and F. Preparata. An optimal algor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263004692273127424">@kebabroyal_ Not that I am aware of … good question.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262495863936606213">@skoularidou May you be optimally transported back to your home as soon as possible! Best wishes.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262474777236836352">The associated slides are here https://t.co/IwrCmdGy8f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262357431268585472">@neu_rips Here is the proof I wrote for my course (I hope it is correct …). https://t.co/txUxSEVJK</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262355266252783616">@neu_rips If span(F) is dense in continuous function then it metrizes the convergence in law. If F i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262354388066197506">@neu_rips It is often called « flat norm » for people doing geometric measure theory … it is the n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262333024420626433">@ValRobert974 J’espère que c’etait une matrice circulante au moins.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261759121751236608">@TamasGorbe I did a similar animation! Nice. https://t.co/FKcjVOMakM https://t.co/WOWs2iqq5y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647939052675072">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Egalement il faut convenir d’une definition </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647453624905729">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Ma legende est particulièrement naze. Ce qui</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647183251755009">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Oui ca montre les derivees de la Gaussiennes</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261637267745112064">@tomrzah @GuillaumeG_ @Pianocktailiste Fractional derivative vs fractional Laplacian. https://t.co/x</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261626120514011137">@tomrzah @GuillaumeG_ @Pianocktailiste A bas les conditions aux bords et les domaines non compacts.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261611221352144896">@tomrzah @GuillaumeG_ @Pianocktailiste Calculer des racines de i c’est trop dangereux. Vaut mieux se</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260977411426062337">@j_bertolotti You can quantize the output at each step of the integrator, but I guess then it will e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260594552622256129">@Jess_Riedel Well it works well for conservative systems. For many other problems I think Runge-Kutt</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260236604465131520">@brewingsense @JustinMSolomon @sam_power_825 So it is not super useful to get some insight about Gam</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260235745819852812">@brewingsense @JustinMSolomon @sam_power_825 Well, the catch is to properly define which notion of c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260218953625931776">@JustinMSolomon @sam_power_825 The naming « epi-convergence » (convergence of the epigraph) makes mu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259396263709155328">@ValRobert974 \exists x, \forall t&lt;T, …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259395546730684424">@qberthet @PierreAblin Initially written for a Master 2 course, but I am supposed to give a tutorial</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259127700666662913">@ChengSoonOng @GiorgioPatrini Maybe you can try this link ? https://t.co/9b6wyQzION</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259026619928399872">@miketranchina Good point, I will add them.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258790438565302273">@mariotelfig Initially high school students and their maths teachers.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258757560624197632">@LucasVB Check also their app https://t.co/uEx8MeeWYk</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258757304280702978">@LucasVB Did you saw this recent siggraph paper? https://t.co/i9zu797Qah</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258732372561977345">The animated version. https://t.co/0BlhIYIIxK</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258703573313032192">@lewischewis Exactly !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258513058613190657">@qberthet Agreed. And Hellinger is the geodesic distance associated to KL :) And the Wasserstein-Fis</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258508701444390914">@undefdev Indeed for probability distributions it plays not role. But since in some case in the book</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258506427775062017">@qberthet Well KL is not a distance ;) For Wasserstein, I guess it is gaining momentum !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258345171646681088">@brewingsense @LucasVB https://t.co/36vb7LRFEK https://t.co/B1bPOyTb56</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258307417852477440">@LucasVB Similarely to KL and TV it is a f-divergence, so it has similar behavior. Wasserstein is of</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258058640826695680">@leonardblier @marc_mezard Yes it will be online very soon.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1257636826975744000">@abletterer @roiporanne Indeed I was wrong from the start, thx for the clarification.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1257581893916205056">@abletterer I think it needs to be non self intersecting.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1257569507553562624">@abletterer No, they are positive for arbitrary polygon!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1257002335127113730">@grisaitis @hardmaru Each frame is a different k. Each is a level set of the function, ie number of </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256970700029599746">@leonid_fedorov @LukaszKaczmarcz It follows from the empty circumcircle property of Delaune simplexe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256947340327477250">The animated version … https://t.co/yCer1TBx5p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256590542688919552">@pgadey Indeed! Convex Hull algorithms rule!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256590311695974406">@LukaszKaczmarcz Good question but I guess this is correct.</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@LukaszKaczmarcz It has to be</td>
          <td> </td>
          <td>x</td>
          <td> </td>
          <td>^2 ie isotropic parabola (unless you want the Delaunay for anothe](https://twitter.com/gabrielpeyre/status/1256590097887174656)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256569541376839682">@pgadey I projected the edges on the surface for visualisation but it should be understood as being </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256568916823982086">@pgadey For the euclidean metric in R^{d+1}</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256185511137476608">@trisidorou “In the case of convex functions we show that a positive definite ‘Hessian’ of f implies</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256185027999793152">@trisidorou Lewis and Sendov. https://t.co/8BUwN43pSD https://t.co/wwXtYeFiKC</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256183944271351809">@trisidorou While for first derivatives the eigenvectors basically play no role, for the second deri</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256183593354870786">@trisidorou Maybe strict/strong convexity does not lift from vectors to matrices? At least second de</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256177535064342528">@lisyarus At least using the second eigenvector is the basic method for spectral clustering.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255872697684410376">@Ayoubsaab7 Ca depend si le probleme est multiclass (1 seul objet par image parmis plusieures classe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255830633131147276">@delonju J’ai bcp hésité et j’hésite encore a ecrire comme ca. Tu penses que c’est mieux ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255823880519704576">@rtavenar Faire un 3e chapitre sur backprop/optim serait génial, mais niveau lycée c’est un gros cha</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255822658706591751">@Alrahil17 @roger_mansuy J’ai mis à jour avec comme notation s_1 a la place de sigma(1), et ajouté u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255783490978754561">@nibot Optimal transport is a mathematical and computational tool that transforms a notion of distan</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255490568521691136">@asemic_horizon In this case this is the classical Laplacian, the method is particularly simple (so </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255149275996028941">@Patapom2 Of course if you are not in 1D you cannot use primitive, and this is why you have to rely </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255148764123156480">@Patapom2 I think what you are asking is the L1 norm of the difference (which corresponds to the tot</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255102548542992387">@imleslahdin @ProbFact Indeed that’s a good metaphor!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255085226528837632">@LowDimensional We have explained this a bit (not much though) in our book (check my pinned tweet) a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255059520407166976">@LowDimensional Indeed if they are normalized you need equal creation and destruction (to maintain u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255046536368721920">@LowDimensional Not sure this is what you mean, but f-div track creation/destruction of mass, dual n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254400826484891650">@e_d_andersen I fully agree!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254317775276949505">@adamnemecek1 @evgeniyzhe @ylecun For real valued functions I believe Risch algorithm is indeed auto</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254316154073624577">@nvmxoxo2 You mean applying Gerschgorin disk theorem to the compagnon matrix? I think it would on pr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254118294212300809">@MIT_CSAIL Our factorizations are adjoints one from each other :) https://t.co/cS0wnD2s3H</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254117145073659905">@MIT_CSAIL A course by Gilbert Strang on the FFT https://t.co/XqVPJv6yaC</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254116939670175745">@p_gwack @MIT_CSAIL Because Gauss actually used the FFT to cary over some computations, “Gauss and t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254059547846729731">@topher_batty @steverockan Could you elaborate? This seems interesting.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254059121395011585">@steverockan For 2D curves I do not think it makes a significant difference, right? But in 3D indeed</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254050940342206464">@theohonohan @hamish_todd Thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254048648343818241">This IPOL paper+code paper of Pascal Monasse pushes the idea of coutouring bilinearly interpolated i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254040944984416257">@hamish_todd @theohonohan (Which is also what I do in my animations when I refine the grid).</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254040649952899073">@hamish_todd @theohonohan I have the impression he still relies on marching cube on a finer grid to </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254040046174420994">@theohonohan @hamish_todd I mean going directly from a pixel array to a collection of piecewise poly</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254039332425515013">@hamish_todd @theohonohan But this not not seem to adres the question of contouring (transforming a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254037017165512706">@theohonohan @hamish_todd Seems hard to efficiently do contouring of eg a cubic spline interpolation</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253738465814106112">@ValRobert974 Le transport optimal est un outil mathématique et informatique qui permet de transform</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253628109334687744">@fakbill I was refering to the usual type of proof where on do a bias/variance decomposition and the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253614817732186114">@fakbill Exactly. Doing the proof without noise is the first step to do the proof with noise. If one</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253598463759134720">@fakbill Did not found analysis of early stopping. This will require understanding its convergence. </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253583420141309953">@fakbill R-L is the most widely used algorithm for deconvolution under positivity constraint. I want</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253437566818320385">@fakbill If there is noise you need to add a regularizer or do early stopping (mirror descent induce</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253371628894982145">@Dirque_L @qberthet @KrzakalaF that escalated quickly…</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253369091382968325">@neu_rips Ah ok great, thx for the clarification !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253367613331320835">@Dirque_L @qberthet @KrzakalaF You meant Laplace, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253367463057686530">@neu_rips Is it a Bregman divergence? To which entropy function is it associated to?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253366280972795905">@sohail__b @neu_rips Hum, how do you remove the log sandwitched in the H and H^T ? I had to do a Tay</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253360013453688837">@neu_rips Yes exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253354692626784256">@KrzakalaF You can derive the adjoint state method by introducing lagrange multipliers evolving back</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253351987845922820">@mariotelfig @neu_rips They only show convergence for gaussian convolution apparently. In this case </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253350263185969153">@qberthet @KrzakalaF I was going to answer the same. My rule of thumb is you should always credit Li</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253349903222349827">@KrzakalaF Maybe not that old, but I guess one could credit the french school of variational calculu</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@neu_rips If you minimize KL(A*x</td>
          <td>y) using mirror descent with a step size =1 for the KL divergence i](https://twitter.com/gabrielpeyre/status/1253339247802675299)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253257731156389888">@stammertescu @2prime_PKU @mraginsky Exactly !! The  equation I wrote is just an iteration of Sinkho</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253256437012336641">@Swaarx Yes exactly. It is the same trick.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253235001782665216">@2prime_PKU Indeed it is the same idea! The Hopf transform turns a Hamilton-Jacobi equation into a h</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253221862022303745">I should add that the complexity of the Legendre transform is O(n). The value of this tweet is that </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253221132649533442">@Dirque_L Exactly! This is just Sinkhorn iterarion!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252904743996973057">@AmitSinhababu @kebabroyal_ No they are roots of polynomials being linear in t. But roots do not dep</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252869878148501506">@mblondel_ml With you ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252516340642066434">@mariotelfig And now it is under creative common license :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252516070268731392">@david_picard I just add a random point in the center ar each frame.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252515153125531655">@gregeganSF In your case you can do it by hand since x and y depend linearly on the outer sqrt, you </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252510151887724544">@gregeganSF I feel you could add extra variables for each sqrt and then use Groebner bases to ellimi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252331195771375617">@themarklstone Should be online very soon.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251936998031536128">@achambertloir @altiroweey @FerroRodolfo Also it is useful to fix by hand - aspect ratio of the fram</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251932461317095424">@achambertloir @altiroweey @FerroRodolfo I mostly use Matlab bc I am used to it, but using Matplotli</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251928186696405002">@achambertloir @altiroweey @FerroRodolfo Well it is brute force: - I save each frame in a .png file </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251920288914329602">@altiroweey @FerroRodolfo The roots of P are varying so indeed the coefficients evolve accordingly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251883814915387394">@nicoguaro @legendarybsn @tomrzah For complex polynomials? The issue is manipulatings polynomials wi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251845069772148736">@CaseyKneale Done! I have released the video on wiki commons https://t.co/CXbOaCn2Ub</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251839633706692608">@fdilke Apparently it extends to entire functions under extra hypotheses https://t.co/o72OOFt9xW</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251825964658688000">@legendarybsn @tomrzah Great question. As # zeros increase the functions degenerate and the zero of </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251501106674499585">@gregeganSF Carl de Boor even has a Matlab code to generate the basis! https://t.co/tHN6pKmxwg  http</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251499152548265985">@gregeganSF I do not think this is what you are looking for, but given n points, the de Boor basis g</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251149281550929926">@LeechLattice Actually if you wait long enough (really long) the ears finally got resolved and the m</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@tomabangalore @tomrzah It is just minimizing int</td>
          <td>nabla f(x)</td>
          <td>^2 + (</td>
          <td>x</td>
          <td>-1)^2 dx … quite dumb but s](https://twitter.com/gabrielpeyre/status/1251059606190723073)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@omaclaren For instance, you can replace min_x</td>
          <td>Ax-y</td>
          <td><em>1 by min</em>{u,x}</td>
          <td>u-x</td>
          <td><em>1 + indicator</em>{Ax=u}, the](https://twitter.com/gabrielpeyre/status/1250759590108676097)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1250758002044555266">@omaclaren There are many convex optimization methods (eg Douglas-Rachford) where you repeatly apply</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1250050910488875010">@t_vayer I think by computing the derivative along all the singular vectors you obtain that all the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249787181226475520">@roydanroy Kempe theorem is that one can actually draw subsets of an algebraic curve (eg a line segm</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249786103370002432">@roydanroy I think this description is a bit misleading. The configuration space of all the points i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249780076830429184">@roydanroy https://t.co/AFpSBjMUhJ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249779550659239946">@roydanroy It seems that you can draw arbitrary algebraic curves. A. B. Kempe. “On a General Method </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249394882084569088">@theohonohan @j_bertolotti @rolandVM And regarding the mean value, it is the same as Kernel ridge re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249381366296346625">@j_bertolotti @rolandVM I think the method is often called “kriging” and was invented by Danie Krige</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249032553912295424">@BrunoLevy01 @sylefeb I gave a talk in the same room as George Lucas gave his keynote the day before</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249031669102923781">@BrunoLevy01 @sylefeb I remember metting him for the first time at my first siggraph in 2005 and I w</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1248919902892818432">@BrunoLevy01 @sylefeb @BrunoLevy01 is the most amazing research centre director I know!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1248271499804454912">@keenanisalive @JustinMSolomon I think you nailed it :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247950748115185664">@FfKnighty @hhaammmaadd Yes Indeed the leading eigenvalue needs to be unique and you should randomiz</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247915946007760900">@jacquesdurden But f_a is a linear function (inner product between x and the vectorial product of th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247473204513267712">@PUNEETJ27008791 Yes exactly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247469547805966338">@david_picard Example with eigenvalues (1,1.05e^{i<em>pi/50},1.05e^{-i</em>pi/50}) so that it approaches an</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247439071032610818">@david_picard I will show later what’s happening when there two eigenvalues with same module, in whi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1246712965673504768">@LucasVB @InertialObservr Yes exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1246419939206008835">@skoularidou @brandondamos Yes, excepted W1 on the left which is not a f-divergence (which is why th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1246173091543334912">@BenSchweitzer It is when you drop the pendulum from the top position with speed 0: it is an instabl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245991283400851456">@docmilanfar Thanks, indeed, I did a mistake … Cannot find a picture of David Lowe, who was at tha</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245819507370844161">@GuillaumeG_ @EleoBellot @roger_mansuy Purée t’es en forme :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245819269084045322">@GuillaumeG_ @EleoBellot @roger_mansuy « Offset curve »/« parallel curve » j’aurai dit https://t.co/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245795274750132225">@Theo_Lacombe_ Good point!! Thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245788342836494336">@Theo_Lacombe_ Yes the degree is fixed (otherwise you are correct this would be a problem). So wlog </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245779027165696003">@tonysilveti I think continuity is simple to prove by plugging one root in the other polynomial and </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245778154268110862">@achambertloir To start, I just want local 1/p holder regularity (so only if I do small enough modif</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245774879380692994">@tomabangalore That is exactly my bet. Seems obvious. But I can’t find a formal proof.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245774617224056834">@scheidegger Indeed. Maybe I am not using the correct wordings but I cannot find any reference for t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245774214482792448">@achambertloir Yes I think so … seems obvious … any idea of the proof ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244699496732397569">@Atrix256 But did he checked before it was a subset of a topological vector space?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244699216804556800">@Atrix256 I guess he was right about cylinders having an infinite number of faces :) https://t.co/EV</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244611272676724736">@leonid_fedorov I think for any functional, when you minimize it, the triangulation will exhibit bru</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244549604424724480">@leonid_fedorov I think discontinuities in the triangulations appear when there are fusion and split</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244318965175853057">@JustinMSolomon @_AlecJacobson @keenanisalive I was not able to find a ref for this, excepted wikipe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244317976158973952">@JustinMSolomon @_AlecJacobson @keenanisalive Yes I think Delta^k needs k&gt;d/2 to have bounded Green </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244287500157431813">@thomas_laverne @keenanisalive You are correct, there was a bug, here is the updated version … htt</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244285062272647168">@pointprocesses I create each frame using matlab/python and then merge them using imagemagik.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244284872061001732">@liuyao12 @keenanisalive I think for images/surfaces biharmonic (thin plate splines) is really the b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244279236317380608">@keenanisalive Good catch thanks (should have stick to constraint on the boundary…)! the green fun</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1243972388301287425">And the reference is a link to the excellent blog post by @Dirque_L</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1243597952385454081">@ValRobert974 @GuillaumeG_ <em>Gabriel</em> Cramer https://t.co/DtT7at9H43</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242545612920365058">@MasonJLegere Probably for the v2</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242506938233114631">@F_Vaggi They should give mini-Fields-medals to maths twitter accounts. Preferably made in chocolate</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242501962958061570">Also I have updated this numerical tour (on Ridge/Lasso/etc) to stick to the notations of the course</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242428302717063168">@Dirque_L @AbinavRavi I concur.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242427921157013511">@GuillaumeG_ @ParisMLgroup It is a good exercise on my 2D example to compute the « worse » starting </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242402849012813824">@bahstgwamt @68kirk My impression is that 3 or 4 steps of golden section is enough in practice for a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242397717109841920">@bahstgwamt @68kirk Beside training GANs on gigantic datasets (or maybe not) I only minimize (2D) qu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242397436129271810">@bahstgwamt @68kirk Indeed, for a generic function you would have to resort to some line search proc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242396913758068737">@bahstgwamt @68kirk It is exo3 from  https://t.co/s5aPHNLSDy https://t.co/KOy4HFm7Ng</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242395674311884805">@bahstgwamt @68kirk On such a simple example (quadratic function) there is a (actually quite nice) c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242389643527626753">@68kirk The function changes during the animation (it becomes more and mire anisotropic) and the bla</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241988660968468483">@AbinavRavi Indeed CG would converge in 2 steps for quadratic functions in 2D (which is not what I s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241815259305979904">@brewingsense Sorry, it is just a random function …</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241651617906245632">@thewarpedspace https://t.co/fDdSq7aRpa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241651445608452096">@thewarpedspace Wow, it was not done on purpose!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241488249857638401">@JustinMSolomon So maybe they are not that useless after all!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241415689153454080">@SamCMaths @octonion To clarify, what I meant is that W upper bounds TV only for discrete spaces. As</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241415285317533696">@SamCMaths @octonion Probably we are not discussing about the same thing, I do not get the issue you</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241371910476304391">@SamCMaths @octonion This is why I have put the term with alpha^bot and the recession function of ph</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241282452632584192">The original figure is from @mathyawp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1240584120780152833">@a_picciau Not to be confused with Givens rotations :) !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1240234344230330368">@johnhearnbcn @JorisMeys @mariotelfig Sorry – see my last tweet about the fact that even for this s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1240234097236205569">@mariotelfig I think my initial statement was misleading, should not have used the wording “closed f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1240225890635743232">@recifs See my last tweet, I meant x as a function of y (phase space portrait), so you are correct t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1239809026063548416">@InertialObservr Indeed, it is exactly since the solution is a sum of Gaussian radial basis function</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1239692799701848066">@BryceBesler @j_bertolotti Chap. 5 of the PhD thesis of my student Quentin Denoyelle includes a revi</a></li>
</ul>
:ET