I"∞¬<ul>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330461755621978112">@mayfer The animation shows the evolution as epsilon increases (regularuzation increases, so more de</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330450275681918978">@julienmairal Thanks for the pointer! It‚Äôs not so surprising that sparsemax appears earlier than 201</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330079868160184320">@roger_mansuy Connais tu un moyen d‚Äôen faire un semi-automatiquement ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330072965581565955">@roger_mansuy https://t.co/I1OrEksEn1</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330071779893776388">@roger_mansuy La fonction x^2/y est ma fonction convexe pr√©f√©r√©e :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1330069992575004672">@roger_mansuy Fonction perspective, one of my favorite :) https://t.co/V3MrUCQume</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329691790220414982">@tomabangalore For pairwise energy mesures of the form int_0^1 cost(t,f(t)) dt, the best (discretize</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329690719641100288">@tomabangalore Pick the one closest to identity for some cherry picked distance on the group of diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1329674825560956929">@tomabangalore This does not look like a well posed mathematical problem. But maybe instead of takin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328992163766673408">@FfKnighty @_AlecJacobson This being said, I found that in many cases one can only use primal or dua</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328991858924658693">@FfKnighty @_AlecJacobson Not sure this is helpful, but this is a (Matlab ‚Äì sorry) example of appli</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328659932204441600">@AvramLevitter @_Kcnarf You can cook up configuration where Lloyd alternate between local minimizers</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328424047751946246">@ST4Good Well anyone interested in following my course on Computational Optimal Transport (Monday, 1</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328363608858890240">@BrunoLevy01 @KMMoerman @jorge_pacheco It is thm 20 in my course notes https://t.co/p6HoaMMJhg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328363358886752260">@BrunoLevy01 @KMMoerman @jorge_pacheco Also I think you might prefer not parametrizing the problem u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328362824133996547">@BrunoLevy01 @KMMoerman @jorge_pacheco I am unsure this will reinsure you, but the fact that the PCA</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328351990200545282">@BrunoLevy01 @KMMoerman @jorge_pacheco The line which minimizes the sum of squares of the distances </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328333983982231553">@BrunoLevy01 @KMMoerman @jorge_pacheco Its PCA (minimizes the projection error) :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328316110194827265">@francoisfleuret I think it is pretty standard (at least in Japan I guess). My rule is 6 with lemon </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328313973570035714">@francoisfleuret Soy sauce</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328313806808903680">@amirvaxman_dgp @XiaohuiChen18 Yes I think the radius where the ball start loosing simple connective</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328310566721622017">@amirvaxman_dgp @XiaohuiChen18 I am unsure this is enough, the exp map could be injective without th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328247805534760960">@XiaohuiChen18 This is not true in general (bc geodesic balls are not geodesically convex) but this </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328072927439413251">I am very impressed by the breath and depth of Andrew Witkin contributions (scale space, snakes, str</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1328071826958901249">@SoufianeKHIAT Zeros crossing of differential operator (eg Laplacian) define curves on images. So if</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326603243825336320">@FarisYazdi I meant filpping only if you reduce the overall transport cost, so that it terminates.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326603007614660608">@themarklstone The world expert on this topic is Roger  Nussbaum. He wrote a nice review https://t.c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326602193454198785">@themarklstone The cone needs to be a proper convex cone. The statement is that any linear map from </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326601902197510144">Apparently these operators are called ‚ÄúKrauss map‚Äù in quantum mechanics. They are the linear maps pr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326600812823457795">As an example, given generic matrices (A_i)_{i=1}^n, the map X in R^{n x n} -&gt; sum_i A_i<em>X</em>A_i^T map</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326598772512731139">What the Perron-Frobenius theorem really is about are convex cones. You can replace the cone of posi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326596069799628800">@FarisYazdi This is why it took 150 years between Monge formulating the problem and Kantorovitch act</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326595826534191105">@FarisYazdi It will terminate (bc there is a finite number of possibility) but it will not find the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326584251723165697">@FarisYazdi This property was mentioned by Gaspard Monge in his original paper. https://t.co/YumHgvi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326583350857854976">And the associated mathematical result is the Perron-Frobenius theorem, which is one of my favorite.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326582677248614400">@FarisYazdi It is because here it minimizes the sum of the <em>squares</em> of the distances. If you minimi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326053794837372928">@BEBischof @docmilanfar For a continuous function on R^d, it is the L^2 norm of the gradient, which </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326050378123718657">@BEBischof Exactly, this was also the comment of @docmilanfar, you encode patterns as local minima o</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1326049902145789952">@docmilanfar Exactly! It‚Äôs like learning a smoothness prior to encode shapes.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325417606996779008">@graveolens So for the wave equation the traveling pair of Diracs would be moving modulo 1.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325417176090730496">@graveolens The theta function is the green function when using periodic boundary conditions (which </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325391264104189953">@graveolens Do you mean the green functions ? For the wave in 1D it is just a pair of traveling dira</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325390706177880067">@dzakwanfalihh ¬´¬†Partial¬†¬ª refers to the fact that there is 2 variables (space and time). If there i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1325029161870692355">@berthier_eloise Definitely!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324689535729651712">@SoutrikTrinity1 Yes I just move around the point y (on a circle) and display how the ratio evolves </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324247487364956160">@Alleycatsphinx Well the display is cryptic :) The central part displays the matrix K while the blue</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324090577772830722">@j_bertolotti @LenaicChizat @DrRBailo This is indeed the way I understood your (very relevant) remar</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324089470304616449">@docmilanfar @j_bertolotti @LenaicChizat @DrRBailo I could not agree more. I find it surprising that</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324071362835681281">@j_bertolotti @LenaicChizat @DrRBailo For pairs of spikes there is a very nice paper of @docmilanfar</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324071001236381696">@j_bertolotti @LenaicChizat @DrRBailo It corresponds to analyzing the super-resolution capability, a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324069938018082816">@j_bertolotti @LenaicChizat @DrRBailo I guess it depends on the meaning of ¬´¬†works¬†¬ª ‚Ä¶ regarding t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1324067462804025350">@t_vayer Avec Filippo Santambrogio comme MC √ßa va √™tre quelque chose :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323687899624296449">@tomrzah @I_m_a_teapot Seul un MC fran√ßais pourra se soucier de la convergence des sch√©mas num√©rique</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323685707706150915">@tomrzah @I_m_a_teapot Peut etre que tu seras retweet par McHammer aussi ‚Äî ou bien McSolaar :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323363333840490501">@eigenhector The problem then reads   Min_f Max_phi int phi(f(x)) dmu(x) - int f(y) dnu(y)  (where n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323361803368366080">@eigenhector I guess using the language of GANs, f would be the generator and phi the discriminator/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323217494115581952">@eigensteve Exactly! The pushforward is somehow a ¬´¬†degenerated¬†¬ª Markov operator to which one wants</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1323182203422023686">@JuanPiCarbajal @vaiter I tried but it modified even more the labels ‚Ä¶ and the english was not as </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322999492673765383">@delonju @dcoeurjo @CNRS @INS2I_CNRS C‚Äôest le barycentre de Wasserstein entre batman et spiderman ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322948997003333635">@vaiter But google translate messing up with the \label and \ref in an inconsistent manner was ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322886151800324096">@pgroisma If you do not threshold the coupling matrix, then for epsilon&gt;0 it is fully connected (eac</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322879170616168450">@pgroisma The permutation obtained for eps=0 is not random (it minimizes the sum of traveled distanc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1322545661494153222">@amirvaxman_dgp Just explicit Euler + projection.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321936460447174661">@risi_kondor @kejace @phc27x @dan_rockmore I remember discussing this with you in 2005 in caf√© Reggi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321920390168498177">@kejace @phc27x The difficult questions is wether there are equivalent of the FFT for these non comm</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321919305613758465">@laurentduval @phc27x I will release an english translation as free PDF soon!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321919047190065157">@vaiter Thanks to google translate it should be quite fast ‚Ä¶ it is just that I actually wrote my b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321749552286863361">@phc27x It is indeed a very good book!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321129194638741504">@galdust My picture is somehow a visual proof of this result :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321108008974372865">@achambertloir It is this paper: https://t.co/Dt9U2FdfMf</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1321058396288749568">The asymptotic value of min(S_n)/n being strictly smaller than 1/2 is due to Vladimir Drinfeld. It i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320752471480913920">The webpage of Ken Perlin with details on the oscar and source code. https://t.co/tGFrhupGZU</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320693691552538625">@lisyarus @R4_Unit Here since it is a Gaussian process, stationarity means that the covariance C(x,y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320692809104564225">@t_vayer Yes it is true for any mincost flow problem. It is not true anymore for multimarginal probl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320684232231059456">@t_vayer The OT problem is totally unimodular, is it related to your question ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320427185040220161">@tonysilveti Exactly !!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320427078676787201">@fdecomite @robinhouston Indeed!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320020329884995587">@El_Gauchiste Here for the sake of visualization the lines are chosen in the least efficient way. Ch</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1320019947653914624">@El_Gauchiste At each iteration one chooses a line on which the iterate is projected (the line being</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1319684013062295552">The formula for the W2 distance extends nicely to the unbalanced (scaled Gaussians) and entropic reg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1318139002101354496">@HanCao9 That the function lambda -&gt; x(lambda) is affine by part.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317508301584814081">@physics303 (i) compares distribution in a strong sense (if you view probability as density vectors </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317507994272387078">@physics303 The take home message is that for probability distribution, you can either (i) use a phi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317507569498411008">@physics303 Well, you can always bound L2&lt;=sqrt(L1), but using the L2 norm for probability distribut</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317429513769410560">@tomrzah Bc my papers are random permutations of these three inequalities‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317424617598947328">@tomrzah Combining the triangular inequality, Cauchy-Schwartz and Jensen‚Äôs inequality ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317371628456677378">@Atrix256 @scottlee It‚Äôs going to be a sub-graph of the hypercube graph ‚Ä¶ https://t.co/2vlD5KglMo </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317347850641170432">@scottlee @Atrix256 The general idea of error correcting codes is to add bits so that the resulting </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317123454030827521">@espadrine @ProbFact I consider x=(x1,x2,x3) that are probability vectors, so that x1+x2+x3=1. Each </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317047748223524864">@vnfrombucharest Bounding the TV is always controversial :) I once get a review complaining that the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317029344758333441">@jitinkapila Well, it is the canonical norm on probability distributions, so it is used everywhere t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317028345633247232">@tbmurphy Indeed. Here is a display of .5*TV/sqrt(1-exp(-KL)). https://t.co/blmT7PszLW</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1317023678970908672">@rgrig My understanding is that most probabilists would do this, and most analysts wouldn‚Äôt (maybe b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316980724336046082">@ccanonne_ @docmilanfar Thanks for kindly supporting my miserable failures :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316362136931762179">@Mirobertson709 @SciPyTip Yes!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316296982114242560">@betabayesian Yes when tau=0 this is Gauss-Newton.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316288196116914176">The Hessian is the sum of two terms, and L-M only makes use the first term. It ensures that it is a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1316281805977333760">@sergecell @srchvrs Indeed, on contrast to Newton it only makes use of the first derivative of x.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1315606580411469824">@f4grx @hbou Yes, it was intended to show that not every shape is admissible.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314830415736320000">@GhoshAvrajit Yes, the set of minimizer is in general a non convex set, think about the function f(x</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314180981079891969">@KyleCranmer ¬´¬†Exact¬†¬ª stability (centroid do not move) is only for p=1 (bc the function is not diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314168336708784128">@qberthet When I met a bunch of people I usually run a l1 minimization in my head to quickly figure </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1314139769820061697">@nrui_tweet This is correct, although in dimension larger than 1 most of the time the solution is un</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313752883830173701">@MAKSBoralessa You should consider implementing a wavelet transform instead https://t.co/ZsyCNM93b4</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313565742076506113">@PierreAblin I am the other kind of guy (screen capture from my course of Friday) https://t.co/gPYf8</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313563195869671426">@PierreAblin So you are the kind of guy who considers under-determined problems are the only one wor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313449232267964416">@Al_levity Indeed! The non smooth points of the l1 ball are sparse vectors.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313445442835935233">And also: https://t.co/912fYuzBmj</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1313444605971648512">Seems appropriate!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1312066608626630657">@xvrtzn Yes in some sense, it corresponds to the blue approximation error which is a straight line.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311408615262167040">@fakbill @honualx The trick to make amazingly good looking textures for graphics is to pass it throu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311408185908043777">@fakbill @honualx It is just a stationary Gaussian process (filtering white noise with some kernel) </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311378726710542336">@sylefeb @honualx The STAR of Sylvain is more than warmly recommended. And his many contributions on</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311374846685859850">@fakbill @honualx Indeed! https://t.co/fSCS32kTDc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1311206125661020160">This relation is often called ‚ÄúPoisson summation formula‚Äù https://t.co/NPS4HiYlHj https://t.co/JVA7k</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309917194168434688">@XiaohuiChen18 Yes the wording intrinsic was not well chosen, I meant ¬´¬†geometric¬†¬ª. It shrinks towa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309793755831173121">@JFattaccioli @tomabangalore More on the ¬´¬†shape¬†¬ª section of the NT https://t.co/0ys5Cxqa1j</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309793550436052993">@JFattaccioli @tomabangalore You can find levelset implementation of mean curvature motions and geod</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309782407600574464">@JFattaccioli Not sure this answers your question but adding length penalty is a usual regularizer f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309771629564694528">@Davood_Norouzi At each step you can project on the constraint by adding s<em>curvature</em>normal where s </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309763801433112578">@tomabangalore The affine invariant flow is probably curvature^1/3, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309756019195744257">@ThisEpoch Indeed, maybe a better wording would be ¬´¬†geometric flows¬†¬ª.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309746071996895233">@tomabangalore Well a circle is an ellipse :) #NeverAdmitYouAreWrong</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1309214924414087168">@marc_lelarge @ENS_ULM Que dire de plus ‚Ä¶ j‚Äôai fait de la pub aupr√®s de mes √©tudiants. Mais n‚Äôoubl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308471090541465608">@n_keriven @alexpghayes Yes, randomized svd!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308459449615618048">@alexpghayes Like using the nuclear norm of a random low dimensional projection ? Poke @n_keriven ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308427244348551168">@MichaelAupetit @AustinRousan This is what I did :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308376622614155264">@LaLetraZeta @MalkymLesdrae @nrui_tweet Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308329781205893126">@sergecell Yes, indeed, tangent planes are the set of speeds of curves traced on the surface. So it </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1308324246565924864">@MalkymLesdrae @nrui_tweet Depending on one‚Äôs maths background it is probably either trivial or cryp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307700696125448195">The figures are from this very nice review issue of J. Physio. Paris ‚ÄúNeurogeometry and visual perce</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307425647275528194">@tomrzah @MonniauxD @achambertloir Je connais un tr√®s bon bouquin sur le sujet :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1307409077019054082">@MonniauxD @achambertloir Tu veux dire pour des vecteur √† valeur dans ce corps (a la place de C), c‚Äô</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306292712581353474">@mraginsky Amazing, thx! As a reward, you‚Äôll get a 5% discount on all the amazing products my future</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306192464932372481">@jm_alexia One can somehow interpret Wassertsein discriminators as (signed) distance functions to th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306191773086220288">@jm_alexia Yes Indeed distances functions are 1-lipschitz. People should replace discriminators by d</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306149225630240768">@lisyarus Adding viscosity and doing an exponential change of variable leads to a heat diffusion (Ho</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306110654894534658">@lisyarus Sorry epsilon*f‚Äô‚Äô</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1306109676132630528">@lisyarus This is bc historicaly the solution was defined by adding epsilon*f‚Äô, and letting epsilon </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1305858404599107585">@dushoda Good question, precision impacts the constant in the O(n log(n)), probably on a not very go</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304894307686584320">@markkitti It is a mistake. On the top graph the vertical axis should be y, on the bottom it should </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304798922624376833">@OPirson @roger_mansuy Oui, Shannon et Turing.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304798717619376129">@roger_mansuy Claude Shannon, le p√®re de la th√©orie de l‚Äôinformation.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304743255154864128">@Dirque_L @madsjw Exactly, of course this was not a typo!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304713063657025536">@sebastien_janas @DamienERNST1 People as mostly relying on classical spaces from analysis of PDEs (e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304712549473103877">@sebastien_janas @DamienERNST1 This does not means that increasing depth is better, just that changi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304712096496615425">@sebastien_janas @DamienERNST1 Good question. When you impose constraints on the speed of convergenc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710990832033792">@DKlemitz This and and its many related implications could/should occupy mathematicians for the next</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710692713496576">@DKlemitz And would also require understanding the properties (eg implicit bias) of the algorithm yo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710215020032000">@DKlemitz This is currently mostly out of reach for deep convolutive architectures and would also re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304710103476600832">@DKlemitz To achieve this one needs to undersand the convergence speed of the approximation (bias) a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304709504798466049">@DKlemitz Because the important question in learning is not approximation having access to Infinite </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1304005931991207936">Section 2 features a ‚Äúconic unbalanced optimal transport for dummies‚Äù which might be useful for peop</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303957376161255425">@skysurf3000 @antiselfdual This is probably not what you are asking for, but you can link these two </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303428823246135296">@LauretteSTucker @nhigham Exactly! https://t.co/Ku2aHVO3vv</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303410022236516352">@LauretteSTucker @nhigham You could phrase it as the fact that the DFT matrix can be written as the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303399930053169153">@mixlamalice Chaque tutelle (ecole / mairie / etc) a sa propre logique et ses propres protocoles ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303392847966674947">@_ardeej Yes exactly you have to take separately connected components. This is explained on the Wiki</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303356890617909248">@_ardeej Yes exactly they are the so-called upper level sets.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1303218678964641792">@burakericok I think there is actualy a third saddle D‚Äô‚Äô which is hard to see bc the background leve</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302562779279360000">@El_Gauchiste The isosurface is extracted using marching cubes.</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@lisyarus @_tbng If X is not symmetric you can use</td>
          <td>X</td>
          <td>_p=</td>
          <td>XX^T</td>
          <td>_{p/2}^{1/2}](https://twitter.com/gabrielpeyre/status/1302511800974553090)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302281017038954497">@DFinsterwalder @Alleycatsphinx This is clearly beyond my league, but with only DC and not the full </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1302210195112759296">@DFinsterwalder @Alleycatsphinx I believe most mathematicians (including myself) only make use of th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1301903615699845123">@LucaAmb Yes exactly</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1301394618420989952">@cliff_watkins In this paper, Robert McCann introduces the notion of displacement convexity (convexi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1300690007259328512">@k1monfared It is supposed to be more cache-friendly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1300689960194973696">@PincoPallinoQ It is supposed to be more cache-friendly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299756178805792768">@JeremyMMyers @cortogantese @paolagorigiorgi Yes all these distances are only for positive measures.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299711990915620869">@cortogantese @paolagorigiorgi Yes exactly! Singular covariances corresponds to infinitely narrow su</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299711705593925633">@misovalko La Rance, not far away! https://t.co/JYeDo3gLFg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299707404628750336">@cortogantese @paolagorigiorgi It depends wether you want to penalize singular (rank deficient) cova</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299656980911587328">@68kirk For Wassertein the distance is simply the Euclidean distance in 2D</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299593648665919495">@katchwreck @leland_mcinnes There is no closed form for the TV. But it behaves similarly to Hellinge</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1299593194020184064">@Mathippaan It shows the ¬´¬†distance¬†¬ª btw 2 Gaussians as you move one of the two. The first Gaussian</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1298158501366239232">@BEBischof This was indeed cryptic. I meant that intuitvely f(x/y) compares how much x/y is close to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1298158158221844480">@DrAndreDavid @BEBischof If they are not normalized (ie x and y are only positive) you need to add t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1297631840849866758">@k1monfared @ProbFact This does not contradict the fact that in the limit the distribution is unifor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1297631005239631872">@k1monfared @ProbFact The number of real eigenvalues of a (n,n) random matrix is ~¬†sqrt(2/pi)*sqrt(n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295806431426478087">@therealoak111 If Amir Beck says it is true ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295805686652252160">@therealoak111 The paper was probably published quite a long time after.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295805515973500931">@gariguetteman @ValRobert974 Elle est top!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295799220058939394">@therealoak111 I found the information in this beautiful paper of Beck and Sabach , highly recommend</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295765481794809856">Weiszfeld was only 16 when he invented his method.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295420804981719047">@kejace @lisyarus One can also try curvature^1/3 which is affine invariant, and this wonderful geome</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295279928817090561">@lisyarus But if the intend is to smooth a ¬´¬†continuous¬†¬ª curve it is not really appropriate since i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295279869023199232">@lisyarus It is indeed progressively smoothes the polygon.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295085630418976768">@ilarrosac I took this solution from this page https://t.co/5JDtPRxCFM https://t.co/1RR5POGcNy</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1295085336654159876">@ilarrosac Yes it is, although it is NP-hard, on small instances one can compute the exact solution.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294998799383965708">For 3 input points the solution is the Fermat point https://t.co/L6bUlMbpb5 and more generally edges</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294561023488339969">Highly recommended https://t.co/QNug8qRN5f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294199967318507520">@Bouh___ @tom_forsyth You guessed correctly, this is the first half of two posts, the second one bei</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1294022261889019904">@qberthet A4 considered harmful</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293988111999995907">@GustavoGoretkin This was the initial post https://t.co/4AJ69yTY9T https://t.co/IoPeWEIa4O</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293935505940320259">@GustavoGoretkin This is correct, I forgot about this, but the initial purpose of the figure was to </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1293467124892143623">@mathlinux @CMSE_at_MSU Indeed, it is NP hard to compute!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292552493122101259">@ValRobert974 @jmcourty This one is the best of course üòä</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292432574229422085">@JeffDean @HuguesHoppe This paper and those of @wimsweldens and Peter Schroder were the first constr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292208841745145858">@ZanotelliVRT @raymondh But using fractional derivatives would have been better/smarter :) For a fix</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1292142117628911619">@QuantumAephraim For instance https://t.co/XNLdewmZfk https://t.co/z6x68KII7I</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291994342467477504">@InertialObservr https://t.co/RERbk7wilY https://t.co/mAH3Fqkeel</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291994056722198531">@InertialObservr I just used linear interpolation ‚Ä¶ you are out of my league!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291397266926972934">@ddcampayo Exactly, if you need to have access to the velocity and position at the same time, you ne</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1291033158570344448">@PetersenGraph Yes but it only works for 3-connected graphs (so somehow the graph of a polyhedron)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290998913638567936">@docmilanfar In sharp contrast, I find the related paper of Andrew Witkin to be crystal clear, in pa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290979472951382019">Progression of the computed embedding as the linear system is being solved by an iterative method (c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290560008183336960">@achambertloir Sorry B should be D but the statement should be clarified. It is that indices i belon</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290028573579976710">@Atrix256 @shachaf Yes exactly. For irrational it is unique and for rational there are 2. I think ..</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1290028369233317888">@Atrix256 Rational numbers considered harmful :) But even in this case there are only 2 cf, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1289893056687124481">@LucaAmb Indeed the heat kernel is the standard way to design covariance on manifold, for instance t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1289879666707185665">@LucaAmb The heat kernel is positive (bc the Laplacian is negative) but in general the geodesic kern</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1288852671236116481">@j_bertolotti https://t.co/xBV3CCsmfz</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1288852224345546752">@j_bertolotti Using an environment map https://t.co/p408VHEys8 produces nice mercury-looking results</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@ywang197 @yablak For the Laplacian,</td>
          <td>omega</td>
          <td>^is would be like cos(s*log(</td>
          <td>omega</td>
          <td>)) which does not see](https://twitter.com/gabrielpeyre/status/1287672583991721987)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287336629355581440">@JeKalifa @roger_mansuy 100% d‚Äôaccord !! J‚Äôaurais ador√© avoir Roger comme prof de prepa ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287104455369019392">@l__ds Yes exactly it is bc the reference measure is Lebesgue (and also I forgot a normalizing const</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1287077201742106627">@ThosVarley Yes it is the same idea as https://t.co/jTbUz7HDe2 It is simply that you can estimate a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286981136233697281">@AtreyeeBanerj10 No you just pick the nearest. You could also use a fixed radius and count # neirghb</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286970976195018753">@LucaAmb I found this one https://t.co/YqCgnknNVD but I think there are others.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286969030042148864">@LucaAmb Yes there are indeed extensions using ratios of nn distances between the clouds.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286967403004526592">@ivrik It is the best reference on the topic imho!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286967262138728449">@crude2refined The normalized formula https://t.co/LCrdmC6rdu https://t.co/R4ZlWHcdfG</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286930752660283392">@ywang197 I think there is a normalization lacking, probably a log(n). Also the entropy with respect</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286930665808830465">@crude2refined I think there is a normalization lacking, probably a log(n). Also the entropy with re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1286294574776934401">@TheNrBr LIC is the extreme limit of anisotropic diffusion, which can be understood as the heat diff</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285848489629540354">@jjvie Bonne id√©e, avec un premier num√©ro bien aust√®re sur l‚Äôensemble des contributions de Nesterov </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285625725068939264">@AnzaFabio There is no explicit solution (excepted on simple surface like a sphere). It converges to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1285531052786712577">@fayolle @norpadon @swiffydk Exactly ! The Laplacian is the laplace beltrami of the surface, so that</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284195977662955520">@orlitany @HaggaiMaron @GalChechik @EthanFetaya Indeed a great paper!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284112329572048896">@necoleman @littmath Eigen-analysis of nonlinear operators considered harmful!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284110535999594498">@littmath The Laplacian trace(Hessian) should be replaced by the Monge-Ampere operator log(det(Hessi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284091133677764609">@loukasa_tweet My course notes are here https://t.co/iiVDwIeRQW I did the proof with Shannon code wo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284086283548209158">@loukasa_tweet Small remark: the optimal code length in general is not H(p), it can be as large as H</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284086241760288768">@loukasa_tweet The worse length for an optimal scheme on average will be necessarily of the order -l</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1284074956058435584">@loukasa_tweet Optimal codes length on average are necessarily of length -log(pi) so I guess they ca</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1283433713112621056">@vatoinblue One can only approximate numerically the solution.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282801564219060230">@arthurmensch I was thinking of arbitrary change of variables, not only 1d. Seems a hard problem (to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282780389162049536">@AlexShtf It is actually true for any p&gt;0 ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282716703219167234">@yiyuezhuo Good question‚Ä¶ As you can see I did not have much inspiration on this one‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1282693781125574657">@siraferradans Nope, but @nicolas_courty @t_vayer @RFlamary are much sharper than me on these topics</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1280445637897715713">@roger_mansuy Non!! https://t.co/Bt6k5xjNhw</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1280399923440046080">@srimukhsai No, but I should have done this!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1279384928606617606">@ClauselMarianne @MonniauxD @sociobd Nan mais chez nous ca sera au top :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1279032581091201026">@kebabroyal_ Yes and yes!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278961806724280320">@kebabroyal_ I think it is also the simplest way to understand the properties of the heat equations.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278961589249552385">@kebabroyal_ The standard laplacian (1,-2,1) does, the discrete diffusion corresponding to local ave</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278254169716121601">@nerd9723 @roger_mansuy Yes n is time and I linearly interpolate to get a smooth animation. The colo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1278253905399463938">@evertedsphere @ValFadeev I used linear interpolation.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277556774346919936">@bodonoghue85 I think it is the cheapest if you need to have access to x and y simultaneously at a g</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277512407426924545">@liwenliang @ankurhandos Indeed, the energy is increasing.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277503140712120326">@devanand_t They are the same, just leapfrog splits the update of y in two parts if one needs to hav</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277500699862011911">@ankurhandos Note sure this helps, but on a circle E=x^2+y^2 with a vector field (y,-x), the energy </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1277235337941614597">@jm_alexia This could potentially go faster, but would also be more unstable and oscilate more. I gu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276546948573601793">@rtavenar Ping @mblondel_ml</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276515147708858368">@vadimkantorov I meant among all possible eigenvalues, the one which has maximal modulus. This modul</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276468316467535872">@KarimMakki4 Almost, it is a scaled rotation.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276431377437806593">@Laurent_Daudet Indeed! Nice catch!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1276043347883425793">@FranckIutzeler Perron-Frobenius is one of my favorite theorem!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1275503550794268673">@madsjw Indeed! https://t.co/J9XunGn0Nm</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274657786249523203">@DevilleSy @BrKloeckner @fxcoudert Then running a kmeans on the average color should be a good start</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274647845669396480">@DevilleSy @BrKloeckner @fxcoudert By ‚Äúsorting‚Äù you mean ordering them along a 1D axis ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274643988746813441">@achambertloir Wouldn‚Äôt ‚àß \wedge U+2227 be better ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274643463653527554">@drherryandmrone That was Indeed what I was refering to.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274631007476596736">@SnowFake3 A part of a sphere</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274378318041821185">@sohail__b Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274359412627574786">@sofia75975685 No because here it is the argmin and not the min.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274359243882233856">@radiowhistler Every mathematician is the Bourbakist of another one.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274355209280552962">@sohail__b The construction operates by considering couples (x,r) in X times R_+, but does not actua</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274321264128282626">@GSavare @LenaicCsl @sohail__b The power of the perspective transform! For the interested reader, a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274286026794635272">@LenaicCsl @sohail__b I would not even dare going beyond pi/2! Bc taking the log is so tempting :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1274243643935731714">@sohail__b Actually no it works for arbitrary metric space. The only catch is that you need to consi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273918974934253569">@drherryandmrone This would rather be the min and not the argmin.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273703527291015171">@LenaicCsl The preprint is out: https://t.co/Sdn6AaD7r0</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273170227447152642">@FrnkNlsn You figure exactly shows the delicate situation @lisyarus was wondering about, where one l</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1273156403025383426">@ian__manchester @lisyarus I think for a quadratic tangency POCS converges at speed 1/sqrt(n) and sl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1272771020521340930">@Alleycatsphinx QR d√©composition has countless applications, in particular to solve least squares an</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271831501559271424">@BEBischof I meant that the indexing of the points do not matter, if you relabel the points it defin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271831007461851136">@FunVisualMath Actually it is a mistake, it should be the square norm of the Hessian and not of the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271717525869465608">@LenaicCsl https://t.co/OCwfdiQJrh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271710923581988864">@BEBischof One usually measures the amplitude of the displacements of points (defining the Wasserste</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271710030090338307">@BEBischof So the space of distributions has two natural geometries: 1) the one where you move the p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271709319483056129">@BEBischof I borrowed the naming from https://t.co/lHYMjNfyZt You can view a discrete probability di</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271708396283219968">@bobbythebrain44 @arsatiki Yes exactly I borrowed the name from the similar concept in fluid dynamic</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271547130851835906">@jonathanalis1 @CompSciFact Computer Aided Geometric Design</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271533491440750592">@FunVisualMath Yes x is indeed 2D.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271142427894657026">@t_vayer I think iff conditions are not usefull bc you cannot check them directly from the dual solu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271138442559307781">@josemig91861392 Exactly. You need the regularizarion to blow if you want to impose the use of affin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271098319327105024">@josemig91861392 No it is the other way around, lambda=0 you do not regularize so you impose f(xi)=y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271076043466604545">@AlexShtf It re-enters the picture in the linear system you need to solve to find the parameters a, </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1271073944540524547">@CSMLab Oh yes indeed this is big typo ‚Ä¶.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269713731208065025">@cristi_vicas I think forward mode autodiff is not implemented in pytorch, but I can imagine it is n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269661033590857730">@liuyao12 Indeed!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269580172065476610">@omaclaren It seems indeed to be exactly the same concept.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269558482249883648">As noted by several people one can view dual numbers as 2x2 matrices. https://t.co/omLK5oh6K3</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269557551470768129">@egostrum @ylecun Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269556631957917697">@egostrum @ylecun It has the same complexity as finite differences but it gives exact results so it </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269553333494263809">@idhamananta It is a bit like complex numbers. But instead of writing i^2=-1 you write epsilon^2=0. </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269552892006092800">@egostrum @ylecun Dual number is a way to compute the Taylor expansion recursively by applying the c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269210361695293440">@qberthet @PicaudV Indeed ‚Ä¶¬† but for my defense the continuous formula I gave are such that if you</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269164589993902080">@0xhui @BrunoLevy01 @keenanisalive You mean like solving a travelling salesman problem but with some</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269164121272070145">@FranckIutzeler Indeed, the magic constant :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269163870171664384">@PicaudV Yes, although it is not totally obvious that you can transfer convergence speed of the cont</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1269163244213678085">@EmilyBendsSpace I interpolated btw the coefficients (and not the functions) so that these functions</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268960950541070339">@tomrzah L^1 vs L^inf</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268960347106525186">@neu_rips @DimitrisPapail @qberthet This is a smart move, thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268806685629517825">@LucasVB - for p=3 it is 1.08 H√∂lder so C^1, for large p it is ~0.2p H√∂lder - it is not symmetric fo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268806525495189505">@LucasVB Not sure this helps to get intuition but: - It is parameterized by p the number of vanishin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268543829424189445">@n_keriven That is not true! https://t.co/p2Yf9xeQLM</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268533332599545856">@aayushbansal @ylecun The initial figure was already great. I added colors :) https://t.co/A203MvrDY</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268532763084435457">@n_keriven I guess I should not take it personnaly when former students collaborate together and wri</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268183987790495745">@j824h @Westoncb But you could change this ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268183861432901634">@j824h @Westoncb Oh indeed sorry !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268181041065443329">@j824h @Westoncb They do interact, there is a sum among all particles.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268158371221377024">@sam_power_825 Sorry, I guess this was the meaning of your mathcal{F}</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268158063518851076">@sam_power_825 Exactly, and after you can write this as a Wasserstein gradient flow over probability</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268145601927053312">@amirvaxman_dgp Yes exactly, in the low bandwidth limit particle density solves a non linear heat eq</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268104457881112577">@TehRaio Yes it somehow converges to local modes, so centroids for mixtures of gaussian-like cluster</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1268090124853747714">@Westoncb The trajectories do not follow a fixed vector field (although it might look like it is the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267524286165721088">@Pet_Roleum several examples are given in the comments</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267493589057187841">@Submersion13 (not displayed here, but my simulation is Indeed on a 2d torus)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267493285377040388">@Submersion13 On a torus the harmonic part is just a constant, which is the mean of the vector field</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267492662933848076">@ClauselMarianne @chriswolfvision overlearf c‚Äôest l‚Äôenfer‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267471804043493384">@BenHouston3D @aashay_menace Indeed, best possible illustration of this decomposition.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267347899316346881">@aashay_menace Or in Poisson image editing to project the vector field on gradient fields (so irrota</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267347550140542976">@aashay_menace For instance to impose incompressibility in fluid simulation by projecting the veloci</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267093766625734660">@TheUneuro The only closed forms I know are for Gaussian distribution. @HichamJanati4 found it. Even</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1267077362472058885">The left part shows decaying the temperature for an (almost) infinite number of samples. The right p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1266658230790426626">@NdeRancourt Indeed, this is one of the most astonishing things about this map. It corresponds to th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1266029880069808134">@JustinMSolomon Research is cyclic so you are probably ahead of everyone.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265597982830977024">@Alleycatsphinx You meant basis pursuit? Bc matching pursuit is rather sequential.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265362042514477057">@GuillaumeG_ I doubt it is‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265360849843126276">@GuillaumeG_ Like the hessian being strictly positive almost everywhere ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265197435061755904">@tomabangalore Numerically it is the same! One has to re-distance the functon from time to time, to </a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@tomabangalore To be exactly equivalent I think there is an additional</td>
          <td>nabla f</td>
          <td>in front of the div](https://twitter.com/gabrielpeyre/status/1265188025610485760)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1265187561389031426">@tomabangalore https://t.co/oTPqbijGY4 https://t.co/gd3VA7VnWw</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264499314111512577">@seismatica Keynote ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264475556940320770">@g_borjan The alpha parameter which controls the smoothness (number of derivatives) varies. The Sobo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1264214201637232645">@FrnkNlsn Non symmetric matrices considered harmful :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263784651577065473">@amirvaxman_dgp @Laurett07429292 Why? I think all the fun would be lost in considering diagonalizabl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263599069403865090">@asmeurer All the matrices need to be invertible in this statement.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263578905111191552">@sohail__b The FLT is still linear in any dimension but the domain needs to be discretized on a squa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263572468289191953">@sohail__b It is not as fast as fast legendre transform or using convex hull algorithms, but it is s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263570439848316929">@sohail__b No it works in any dimension.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263567725798076417">@YassineAlouini It is ex 66 in Denis Serre¬¥s list https://t.co/YMNd9GUhr4 https://t.co/ZjnjRz9oJn</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263564965555961856">@GaelVaroquaux The difficulty is to deal with non symmetric matrices.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263556430185074688">@SteffenStatsML @Bord_n Yes the diffculty is to show that a square matrix is an exponential (for the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263555269566631941">@Radegund @AlgebraFact Not really actually‚Ä¶ one implication is obvious (exp(x) is the square of ex</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263554787670360064">@CsabaSzepesvari This only shows one direction of the implication. The other one is more involved, i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263554491342872577">@phc27x Exactly !  But checking wether a matrix is the square of another one is not so simple ‚Ä¶ so</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263553937220743175">@BoyInDaBox89 @AlgebraFact The application I have in mind is to determine which space transformation</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263552912292265985">@SteffenStatsML The issue is the restriction to real matrices (plus possibly dealing with non diagon</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263526178058579968">@Laurett07429292 Exactly :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263519793421959168">@achambertloir Yes exactly, now that I think about it it is indeed cristal clear ‚Ä¶ I guess once on</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263519123792891919">@pddixit This was exactly my motivation. A transformation is generated by a linear dynamical system </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263518818132992001">@RobJLow Oops, sorry I forgot to say that these matrices need to be invertible ‚Ä¶.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263518182834348032">@achambertloir I was not expecting such a simple description, I was not even sure it was semi algebr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263494226047832064">@dcoeurjo @kebabroyal_ Most TCS people would be perfectly fine with n^7 not being considered as hard</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263376391581642755">@kebabroyal_ Indeed, the kernel is the polar of the convex hull of the polar! This is from a paper b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263172472180027394">The kernel of a polygon can be computed in linear time: D. T. Lee and F. Preparata. An optimal algor</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1263004692273127424">@kebabroyal_ Not that I am aware of ‚Ä¶ good question.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262495863936606213">@skoularidou May you be optimally transported back to your home as soon as possible! Best wishes.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262474777236836352">The associated slides are here https://t.co/IwrCmdGy8f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262357431268585472">@neu_rips Here is the proof I wrote for my course (I hope it is correct ‚Ä¶). https://t.co/txUxSEVJK</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262355266252783616">@neu_rips If span(F) is dense in continuous function then it metrizes the convergence in law. If F i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262354388066197506">@neu_rips It is often called ¬´¬†flat norm¬†¬ª for people doing geometric measure theory ‚Ä¶ it is the n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1262333024420626433">@ValRobert974 J‚Äôesp√®re que c‚Äôetait une matrice circulante au moins.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261759121751236608">@TamasGorbe I did a similar animation! Nice. https://t.co/FKcjVOMakM https://t.co/WOWs2iqq5y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647939052675072">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Egalement il faut convenir d‚Äôune definition </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647453624905729">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Ma legende est particuli√®rement naze. Ce qui</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261647183251755009">@LaurentDietric2 @tomrzah @GuillaumeG_ @Pianocktailiste Oui ca montre les derivees de la Gaussiennes</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261637267745112064">@tomrzah @GuillaumeG_ @Pianocktailiste Fractional derivative vs fractional Laplacian. https://t.co/x</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261626120514011137">@tomrzah @GuillaumeG_ @Pianocktailiste A bas les conditions aux bords et les domaines non compacts.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1261611221352144896">@tomrzah @GuillaumeG_ @Pianocktailiste Calculer des racines de i c‚Äôest trop dangereux. Vaut mieux se</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260977411426062337">@j_bertolotti You can quantize the output at each step of the integrator, but I guess then it will e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260594552622256129">@Jess_Riedel Well it works well for conservative systems. For many other problems I think Runge-Kutt</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260236604465131520">@brewingsense @JustinMSolomon @sam_power_825 So it is not super useful to get some insight about Gam</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260235745819852812">@brewingsense @JustinMSolomon @sam_power_825 Well, the catch is to properly define which notion of c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1260218953625931776">@JustinMSolomon @sam_power_825 The naming ¬´¬†epi-convergence¬†¬ª (convergence of the epigraph) makes mu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259396263709155328">@ValRobert974 \exists x, \forall t&lt;T, ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259395546730684424">@qberthet @PierreAblin Initially written for a Master 2 course, but I am supposed to give a tutorial</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259127700666662913">@ChengSoonOng @GiorgioPatrini Maybe you can try this link ? https://t.co/9b6wyQzION</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1259026619928399872">@miketranchina Good point, I will add them.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258790438565302273">@mariotelfig Initially high school students and their maths teachers.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258757560624197632">@LucasVB Check also their app https://t.co/uEx8MeeWYk</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258757304280702978">@LucasVB Did you saw this recent siggraph paper? https://t.co/i9zu797Qah</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258732372561977345">The animated version. https://t.co/0BlhIYIIxK</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258703573313032192">@lewischewis Exactly !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258513058613190657">@qberthet Agreed. And Hellinger is the geodesic distance associated to KL :) And the Wasserstein-Fis</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258508701444390914">@undefdev Indeed for probability distributions it plays not role. But since in some case in the book</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258506427775062017">@qberthet Well KL is not a distance ;) For Wasserstein, I guess it is gaining momentum !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258345171646681088">@brewingsense @LucasVB https://t.co/36vb7LRFEK https://t.co/B1bPOyTb56</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258307417852477440">@LucasVB Similarely to KL and TV it is a f-divergence, so it has similar behavior. Wasserstein is of</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1258058640826695680">@leonardblier @marc_mezard Yes it will be online very soon.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1257636826975744000">@abletterer @roiporanne Indeed I was wrong from the start, thx for the clarification.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1257581893916205056">@abletterer I think it needs to be non self intersecting.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1257569507553562624">@abletterer No, they are positive for arbitrary polygon!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1257002335127113730">@grisaitis @hardmaru Each frame is a different k. Each is a level set of the function, ie number of </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256970700029599746">@leonid_fedorov @LukaszKaczmarcz It follows from the empty circumcircle property of Delaune simplexe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256947340327477250">The animated version ‚Ä¶ https://t.co/yCer1TBx5p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256590542688919552">@pgadey Indeed! Convex Hull algorithms rule!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256590311695974406">@LukaszKaczmarcz Good question but I guess this is correct.</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@LukaszKaczmarcz It has to be</td>
          <td>¬†</td>
          <td>x</td>
          <td>¬†</td>
          <td>^2 ie isotropic parabola (unless you want the Delaunay for anothe](https://twitter.com/gabrielpeyre/status/1256590097887174656)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256569541376839682">@pgadey I projected the edges on the surface for visualisation but it should be understood as being </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256568916823982086">@pgadey For the euclidean metric in R^{d+1}</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256185511137476608">@trisidorou ‚ÄúIn the case of convex functions we show that a positive definite ‚ÄòHessian‚Äô of f implies</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256185027999793152">@trisidorou Lewis and Sendov. https://t.co/8BUwN43pSD https://t.co/wwXtYeFiKC</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256183944271351809">@trisidorou While for first derivatives the eigenvectors basically play no role, for the second deri</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256183593354870786">@trisidorou Maybe strict/strong convexity does not lift from vectors to matrices? At least second de</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1256177535064342528">@lisyarus At least using the second eigenvector is the basic method for spectral clustering.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255872697684410376">@Ayoubsaab7 Ca depend si le probleme est multiclass (1 seul objet par image parmis plusieures classe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255830633131147276">@delonju J‚Äôai bcp h√©sit√© et j‚Äôh√©site encore a ecrire comme ca. Tu penses que c‚Äôest mieux ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255823880519704576">@rtavenar Faire un 3e chapitre sur backprop/optim serait g√©nial, mais niveau lyc√©e c‚Äôest un gros cha</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255822658706591751">@Alrahil17 @roger_mansuy J‚Äôai mis √† jour avec comme notation s_1 a la place de sigma(1), et ajout√© u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255783490978754561">@nibot Optimal transport is a mathematical and computational tool that transforms a notion of distan</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255490568521691136">@asemic_horizon In this case this is the classical Laplacian, the method is particularly simple (so </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255149275996028941">@Patapom2 Of course if you are not in 1D you cannot use primitive, and this is why you have to rely </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255148764123156480">@Patapom2 I think what you are asking is the L1 norm of the difference (which corresponds to the tot</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255102548542992387">@imleslahdin @ProbFact Indeed that‚Äôs a good metaphor!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255085226528837632">@LowDimensional We have explained this a bit (not much though) in our book (check my pinned tweet) a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255059520407166976">@LowDimensional Indeed if they are normalized you need equal creation and destruction (to maintain u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1255046536368721920">@LowDimensional Not sure this is what you mean, but f-div track creation/destruction of mass, dual n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254400826484891650">@e_d_andersen I fully agree!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254317775276949505">@adamnemecek1 @evgeniyzhe @ylecun For real valued functions I believe Risch algorithm is indeed auto</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254316154073624577">@nvmxoxo2 You mean applying Gerschgorin disk theorem to the compagnon matrix? I think it would on pr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254118294212300809">@MIT_CSAIL Our factorizations are adjoints one from each other :) https://t.co/cS0wnD2s3H</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254117145073659905">@MIT_CSAIL A course by Gilbert Strang on the FFT https://t.co/XqVPJv6yaC</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254116939670175745">@p_gwack @MIT_CSAIL Because Gauss actually used the FFT to cary over some computations, ‚ÄúGauss and t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254059547846729731">@topher_batty @steverockan Could you elaborate? This seems interesting.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254059121395011585">@steverockan For 2D curves I do not think it makes a significant difference, right? But in 3D indeed</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254050940342206464">@theohonohan @hamish_todd Thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254048648343818241">This IPOL paper+code paper of Pascal Monasse pushes the idea of coutouring bilinearly interpolated i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254040944984416257">@hamish_todd @theohonohan (Which is also what I do in my animations when I refine the grid).</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254040649952899073">@hamish_todd @theohonohan I have the impression he still relies on marching cube on a finer grid to </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254040046174420994">@theohonohan @hamish_todd I mean going directly from a pixel array to a collection of piecewise poly</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254039332425515013">@hamish_todd @theohonohan But this not not seem to adres the question of contouring (transforming a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1254037017165512706">@theohonohan @hamish_todd Seems hard to efficiently do contouring of eg a cubic spline interpolation</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253738465814106112">@ValRobert974 Le transport optimal est un outil math√©matique et informatique qui permet de transform</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253628109334687744">@fakbill I was refering to the usual type of proof where on do a bias/variance decomposition and the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253614817732186114">@fakbill Exactly. Doing the proof without noise is the first step to do the proof with noise. If one</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253598463759134720">@fakbill Did not found analysis of early stopping. This will require understanding its convergence. </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253583420141309953">@fakbill R-L is the most widely used algorithm for deconvolution under positivity constraint. I want</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253437566818320385">@fakbill If there is noise you need to add a regularizer or do early stopping (mirror descent induce</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253371628894982145">@Dirque_L @qberthet @KrzakalaF that escalated quickly‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253369091382968325">@neu_rips Ah ok great, thx for the clarification !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253367613331320835">@Dirque_L @qberthet @KrzakalaF You meant Laplace, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253367463057686530">@neu_rips Is it a Bregman divergence? To which entropy function is it associated to?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253366280972795905">@sohail__b @neu_rips Hum, how do you remove the log sandwitched in the H and H^T ? I had to do a Tay</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253360013453688837">@neu_rips Yes exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253354692626784256">@KrzakalaF You can derive the adjoint state method by introducing lagrange multipliers evolving back</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253351987845922820">@mariotelfig @neu_rips They only show convergence for gaussian convolution apparently. In this case </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253350263185969153">@qberthet @KrzakalaF I was going to answer the same. My rule of thumb is you should always credit Li</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253349903222349827">@KrzakalaF Maybe not that old, but I guess one could credit the french school of variational calculu</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@neu_rips If you minimize KL(A*x</td>
          <td>y) using mirror descent with a step size =1 for the KL divergence i](https://twitter.com/gabrielpeyre/status/1253339247802675299)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253257731156389888">@stammertescu @2prime_PKU @mraginsky Exactly !! The  equation I wrote is just an iteration of Sinkho</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253256437012336641">@Swaarx Yes exactly. It is the same trick.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253235001782665216">@2prime_PKU Indeed it is the same idea! The Hopf transform turns a Hamilton-Jacobi equation into a h</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253221862022303745">I should add that the complexity of the Legendre transform is O(n). The value of this tweet is that </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1253221132649533442">@Dirque_L Exactly! This is just Sinkhorn iterarion!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252904743996973057">@AmitSinhababu @kebabroyal_ No they are roots of polynomials being linear in t. But roots do not dep</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252869878148501506">@mblondel_ml With you ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252516340642066434">@mariotelfig And now it is under creative common license :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252516070268731392">@david_picard I just add a random point in the center ar each frame.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252515153125531655">@gregeganSF In your case you can do it by hand since x and y depend linearly on the outer sqrt, you </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252510151887724544">@gregeganSF I feel you could add extra variables for each sqrt and then use Groebner bases to ellimi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1252331195771375617">@themarklstone Should be online very soon.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251936998031536128">@achambertloir @altiroweey @FerroRodolfo Also it is useful to fix by hand - aspect ratio of the fram</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251932461317095424">@achambertloir @altiroweey @FerroRodolfo I mostly use Matlab bc I am used to it, but using Matplotli</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251928186696405002">@achambertloir @altiroweey @FerroRodolfo Well it is brute force: - I save each frame in a .png file </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251920288914329602">@altiroweey @FerroRodolfo The roots of P are varying so indeed the coefficients evolve accordingly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251883814915387394">@nicoguaro @legendarybsn @tomrzah For complex polynomials? The issue is manipulatings polynomials wi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251845069772148736">@CaseyKneale Done! I have released the video on wiki commons https://t.co/CXbOaCn2Ub</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251839633706692608">@fdilke Apparently it extends to entire functions under extra hypotheses https://t.co/o72OOFt9xW</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251825964658688000">@legendarybsn @tomrzah Great question. As # zeros increase the functions degenerate and the zero of </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251501106674499585">@gregeganSF Carl de Boor even has a Matlab code to generate the basis! https://t.co/tHN6pKmxwg  http</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251499152548265985">@gregeganSF I do not think this is what you are looking for, but given n points, the de Boor basis g</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1251149281550929926">@LeechLattice Actually if you wait long enough (really long) the ears finally got resolved and the m</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@tomabangalore @tomrzah It is just minimizing int</td>
          <td>nabla f(x)</td>
          <td>^2 + (</td>
          <td>x</td>
          <td>-1)^2 dx ‚Ä¶ quite dumb but s](https://twitter.com/gabrielpeyre/status/1251059606190723073)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@omaclaren For instance, you can replace min_x</td>
          <td>Ax-y</td>
          <td><em>1 by min</em>{u,x}</td>
          <td>u-x</td>
          <td><em>1 + indicator</em>{Ax=u}, the](https://twitter.com/gabrielpeyre/status/1250759590108676097)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1250758002044555266">@omaclaren There are many convex optimization methods (eg Douglas-Rachford) where you repeatly apply</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1250050910488875010">@t_vayer I think by computing the derivative along all the singular vectors you obtain that all the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249787181226475520">@roydanroy Kempe theorem is that one can actually draw subsets of an algebraic curve (eg a line segm</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249786103370002432">@roydanroy I think this description is a bit misleading. The configuration space of all the points i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249780076830429184">@roydanroy https://t.co/AFpSBjMUhJ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249779550659239946">@roydanroy It seems that you can draw arbitrary algebraic curves. A. B. Kempe. ‚ÄúOn a General Method </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249394882084569088">@theohonohan @j_bertolotti @rolandVM And regarding the mean value, it is the same as Kernel ridge re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249381366296346625">@j_bertolotti @rolandVM I think the method is often called ‚Äúkriging‚Äù and was invented by Danie Krige</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249032553912295424">@BrunoLevy01 @sylefeb I gave a talk in the same room as George Lucas gave his keynote the day before</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1249031669102923781">@BrunoLevy01 @sylefeb I remember metting him for the first time at my first siggraph in 2005 and I w</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1248919902892818432">@BrunoLevy01 @sylefeb @BrunoLevy01 is the most amazing research centre director I know!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1248271499804454912">@keenanisalive @JustinMSolomon I think you nailed it :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247950748115185664">@FfKnighty @hhaammmaadd Yes Indeed the leading eigenvalue needs to be unique and you should randomiz</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247915946007760900">@jacquesdurden But f_a is a linear function (inner product between x and the vectorial product of th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247473204513267712">@PUNEETJ27008791 Yes exactly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247469547805966338">@david_picard Example with eigenvalues (1,1.05e^{i<em>pi/50},1.05e^{-i</em>pi/50}) so that it approaches an</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1247439071032610818">@david_picard I will show later what‚Äôs happening when there two eigenvalues with same module, in whi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1246712965673504768">@LucasVB @InertialObservr Yes exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1246419939206008835">@skoularidou @brandondamos Yes, excepted W1 on the left which is not a f-divergence (which is why th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1246173091543334912">@BenSchweitzer It is when you drop the pendulum from the top position with speed 0: it is an instabl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245991283400851456">@docmilanfar Thanks, indeed, I did a mistake ‚Ä¶ Cannot find a picture of David Lowe, who was at tha</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245819507370844161">@GuillaumeG_ @EleoBellot @roger_mansuy Pur√©e t‚Äôes en forme :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245819269084045322">@GuillaumeG_ @EleoBellot @roger_mansuy ¬´¬†Offset curve¬†¬ª/¬´¬†parallel curve¬†¬ª j‚Äôaurai dit https://t.co/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245795274750132225">@Theo_Lacombe_ Good point!! Thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245788342836494336">@Theo_Lacombe_ Yes the degree is fixed (otherwise you are correct this would be a problem). So wlog </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245779027165696003">@tonysilveti I think continuity is simple to prove by plugging one root in the other polynomial and </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245778154268110862">@achambertloir To start, I just want local 1/p holder regularity (so only if I do small enough modif</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245774879380692994">@tomabangalore That is exactly my bet. Seems obvious. But I can‚Äôt find a formal proof.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245774617224056834">@scheidegger Indeed. Maybe I am not using the correct wordings but I cannot find any reference for t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1245774214482792448">@achambertloir Yes I think so ‚Ä¶ seems obvious ‚Ä¶ any idea of the proof ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244699496732397569">@Atrix256 But did he checked before it was a subset of a topological vector space?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244699216804556800">@Atrix256 I guess he was right about cylinders having an infinite number of faces :) https://t.co/EV</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244611272676724736">@leonid_fedorov I think for any functional, when you minimize it, the triangulation will exhibit bru</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244549604424724480">@leonid_fedorov I think discontinuities in the triangulations appear when there are fusion and split</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244318965175853057">@JustinMSolomon @_AlecJacobson @keenanisalive I was not able to find a ref for this, excepted wikipe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244317976158973952">@JustinMSolomon @_AlecJacobson @keenanisalive Yes I think Delta^k needs k&gt;d/2 to have bounded Green </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244287500157431813">@thomas_laverne @keenanisalive You are correct, there was a bug, here is the updated version ‚Ä¶ htt</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244285062272647168">@pointprocesses I create each frame using matlab/python and then merge them using imagemagik.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244284872061001732">@liuyao12 @keenanisalive I think for images/surfaces biharmonic (thin plate splines) is really the b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1244279236317380608">@keenanisalive Good catch thanks (should have stick to constraint on the boundary‚Ä¶)! the green fun</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1243972388301287425">And the reference is a link to the excellent blog post by @Dirque_L</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1243597952385454081">@ValRobert974 @GuillaumeG_ <em>Gabriel</em> Cramer https://t.co/DtT7at9H43</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242545612920365058">@MasonJLegere Probably for the v2</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242506938233114631">@F_Vaggi They should give mini-Fields-medals to maths twitter accounts. Preferably made in chocolate</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242501962958061570">Also I have updated this numerical tour (on Ridge/Lasso/etc) to stick to the notations of the course</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242428302717063168">@Dirque_L @AbinavRavi I concur.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242427921157013511">@GuillaumeG_ @ParisMLgroup It is a good exercise on my 2D example to compute the ¬´¬†worse¬†¬ª starting </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242402849012813824">@bahstgwamt @68kirk My impression is that 3 or 4 steps of golden section is enough in practice for a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242397717109841920">@bahstgwamt @68kirk Beside training GANs on gigantic datasets (or maybe not) I only minimize (2D) qu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242397436129271810">@bahstgwamt @68kirk Indeed, for a generic function you would have to resort to some line search proc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242396913758068737">@bahstgwamt @68kirk It is exo3 from  https://t.co/s5aPHNLSDy https://t.co/KOy4HFm7Ng</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242395674311884805">@bahstgwamt @68kirk On such a simple example (quadratic function) there is a (actually quite nice) c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1242389643527626753">@68kirk The function changes during the animation (it becomes more and mire anisotropic) and the bla</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241988660968468483">@AbinavRavi Indeed CG would converge in 2 steps for quadratic functions in 2D (which is not what I s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241815259305979904">@brewingsense Sorry, it is just a random function ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241651617906245632">@thewarpedspace https://t.co/fDdSq7aRpa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241651445608452096">@thewarpedspace Wow, it was not done on purpose!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241488249857638401">@JustinMSolomon So maybe they are not that useless after all!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241415689153454080">@SamCMaths @octonion To clarify, what I meant is that W upper bounds TV only for discrete spaces. As</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241415285317533696">@SamCMaths @octonion Probably we are not discussing about the same thing, I do not get the issue you</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241371910476304391">@SamCMaths @octonion This is why I have put the term with alpha^bot and the recession function of ph</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1241282452632584192">The original figure is from @mathyawp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1240584120780152833">@a_picciau Not to be confused with Givens rotations :) !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1240234344230330368">@johnhearnbcn @JorisMeys @mariotelfig Sorry ‚Äì see my last tweet about the fact that even for this s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1240234097236205569">@mariotelfig I think my initial statement was misleading, should not have used the wording ‚Äúclosed f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1240225890635743232">@recifs See my last tweet, I meant x as a function of y (phase space portrait), so you are correct t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1239809026063548416">@InertialObservr Indeed, it is exactly since the solution is a sum of Gaussian radial basis function</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1239692799701848066">@BryceBesler @j_bertolotti Chap. 5 of the PhD thesis of my student Quentin Denoyelle includes a revi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1239580907549470721">@skoularidou @PyTorch @FeydyJean You are welcome :) Hope you‚Äôll join the club of Sinkhorn aficionado</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1239580608252313600">@skoularidou @PyTorch @FeydyJean And for a general purpose Python package (not only entropic regular</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1239580423602274309">@skoularidou @PyTorch @FeydyJean For a non-efficient implementation, you can have a look at the ‚ÄúOpt</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1239580151500943360">@skoularidou @PyTorch For an efficient python package, I highly recommend GeomLoss from @FeydyJean a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1238749354111926274">Also closely related is leapfrog integrator. https://t.co/MVKkciZ1dl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1238748962145910792">Same with more particles. Disclaimer: collision handling is very inaccurate ‚Ä¶ https://t.co/NL0zo9K</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1238743049569697794">@ScienceClic Yes you are correct. A better example is x^2 y / (x^2+y^2). https://t.co/HeXhHyceAV</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1238497170447360000">The wording ‚Äúin all directions‚Äù is a bit misleading since this function has zero (directional) deriv</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1238156726383378433">@GrosMtl @QasimMP Good catch, thanks ! https://t.co/ORYkYYGofD</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1238042635882856448">@xbresson And in this case on can rely on Perron Frobenius theorem to ensure existence and uniquenes</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1238021169837678592">@fakbill Indeed! https://t.co/iutl1csqsa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1237761066848980998">@jacquesdurden The paper ‚ÄúSome NP-complete problems in quadratic and nonlinear programming, Katta G.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1237758824674705410">@andrei_st_n @jacquesdurden There are convex programs which are known to be hard to solve (e.g. lead</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1237758197622022145">@jacquesdurden I think an example often cited of intractable convex problem (eg projection) are thos</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1237455204775313409">@fakbill Between 3 to 900 coefficients</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1237439946417475587">Decompressing an elephant. https://t.co/e2WcJN1B3H</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1237032716702490624">@newplatonism For Lipschitz functions error decays like 1/sqrt(n) (and this is not the optimal rate </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1236584803069308933">@ChatenayDidier @sylvaingigan Le staff de direction est en quarantaine.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1236584035570331649">@ChatenayDidier @sylvaingigan C‚Äôest tout en haut de tout en haut ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1235964028922597381">Due to popular demand (ok, nobody <em>really</em> asked me‚Ä¶) this shows the histogram of the 300 last ite</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1235934391764946944">@Patapom2 @pschwede Yes exactly !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1235901961624576001">@pschwede It displays the repartition of the iterates of the map. So black values corresponds to adh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1235642790458974208">@FfKnighty It gives this: https://t.co/wANTNxmWCK</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1235480549587906566">@NathanielVirgo I do not know‚Ä¶ Apparently the distribution is converging very slowly in this area.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1235276199829663744">@ian__manchester My french-bias forces me to cite ‚ÄúElisabeth Rouy and Agn√®s Tourin, A viscosity solu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1235150306138304514">@ian__manchester Indeed! These are the two papers to be credited for the Fast Marching.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1234855446026039296">@AustenLamacraft https://t.co/m0A6FMdB1u https://t.co/RdhoXFQPoa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1234855254493138944">@AustenLamacraft You need to replace the L2 norm on std by the so-called Bures metric btw the covari</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1234853782565113858">@pfau @KyleCranmer @Amessica Not sure if that is what you are asking but OT metric is very different</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1234853227717447683">@KyleCranmer @Amessica OT is positively curved while Fisher-Rao is Indeed negatively curved. It woul</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1234852529663598597">@AustenLamacraft @amessica1 Yes Indeed OT somehow makes (means,std) a natural coordinates.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1234575065800413191">@zzznah Like perfectly matched layer (PML)? https://t.co/KQ2M0Jo4gp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1234060674491408387">@chrisk0 Good catch!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1233552245796544512">@MehrtashH @2prime_PKU Why? And FW only rely on directional derivatives. This being said there are e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1233468471654207490">@imleslahdin They are orthogonal for the inner product defining the gradient. This follows from zero</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1233376430488248322">@JustinMSolomon @2prime_PKU Oh I See! Yes Indeed!!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1233313763631210496">@2prime_PKU GD is very different from FW. To start, GD necessitates some notion of inner product to </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1232982266793615360">@rblourenco @achambertloir The github page is here https://t.co/YR14DFNMLp https://t.co/mE7BA5ivbL</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1232980552732553217">@rblourenco @achambertloir Apparently nbviewer keeps an old version in its cache, and this was fixed</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1232918579051474951">@SmolkinSays Only u is displayed in these animations using some colormap.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1230986614949761026">@asmeurer The polar set is defined here https://t.co/iXbnVcvwho Indeed the John ellipsoid is unique </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1229429873128747009">@tonysilveti The part by William Allard is a gem! https://t.co/SMQfTVnA17</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1229417097391955968">@tonysilveti @PrasathLab \begin{enumerate} https://t.co/6IBSycJPgj</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1229333311589355520">@PrasathLab I agree that this book is quite hardcore‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1228406440429281281">@achambertloir Video mathpark d‚ÄôIrene : https://t.co/jmhTXBQINI</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227921820533317632">@Atrix256 So some kind of certification of blue noise-type behavior but for normal fields? If so, se</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227906640516653056">@Atrix256 I think for most applications just taking 3 FFT and then dealing with normalization as a p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227906159362953216">@Atrix256 Converting to spherical coord seems weird (this would depend on a choice of axes). Also th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227905869519650817">@Atrix256 Yes exactly, you could interpret the magnitude as some sort of power spectrum. Not sure wh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227880982776250376">@Atrix256 There exists a quaternionic Fourier transform (storing your 3D vector as an imaginary quat</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227636879241162753">@jjvie Todo: rename into ‚ÄúDeep Wasserstein Control of Mirror Langevin Monte Carlo‚Äù</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227636287357833216">This follows/refines an inspiring previous work of @CevherLIONS‚Äôs team https://t.co/0XF8ZM57Rl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227516283853430784">@aqsalose This review paper features many results https://t.co/UvFw5dXHCj</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227492451251675137">@Dirque_L You mean like this one ? https://t.co/gIFLjKWXAk https://t.co/SrxNxdCrrg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1227315787100229633">Two recent works showing sufficient condition for sucess of the BM approach (good news) and that thi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226898086213115905">@andrew_n_carr These are not for IPM but rather f-divergences (although technically TV is an IPM‚Ä¶)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226878379024101376">Pinsker‚Äôs inequality can be refined. This is due to Igor Vajda. https://t.co/ZtmmlZcTUt</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@skoularidou Vanilla GANs is somehow minimizing KL(mu</td>
          <td>mu+nu) (Jensen Shannon divergence) ‚Ä¶ Note th](https://twitter.com/gabrielpeyre/status/1226799698591322112)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226798337778692096">@kebabroyal_ There is a very nice review Gibbs and @mathyawp on the relations between various diverg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226797472065957888">@kebabroyal_ Also TV is (much) stronger than Wasserstein (for instance W1&lt;=TV). TV is the strong top</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226797066405515264">@kebabroyal_ KL is indeed ‚Äústronger‚Äù than TV. Converse inequality cannot be true without extra hypot</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@vnfrombucharest @johncarlosbaez Maybe a slightly better alternative is int</td>
          <td>dmu/dnu(x)-1</td>
          <td>dnu(x) to](https://twitter.com/gabrielpeyre/status/1226790371918786561)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226495373239357442">@francoisfleuret https://t.co/jQ9k3TfSFL</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226475895927529473">@roydanroy and Douady, who gave the name to this fractal.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226114240039313408">@colincui Slides: https://t.co/btRxR4P09W</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226113761406341127">@NacchoArroyo @BachFrancis Exactly. It is the same idea.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1226061326927175680">The related paper with arxiv link I gave is by @BachFrancis</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1225177790535872512">@sam_power_825 @brewingsense @LenaicCsl At epsilon=0 it becomes singular, and doing the maths become</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1225177467905826819">@sam_power_825 @brewingsense @LenaicCsl Sorry you are correct, I misunderstood what you wanted to do</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1225176733424455680">@sam_power_825 @qberthet Also there is no nice formula for discrete measures, while for continuous d</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1225176248957198336">@sam_power_825 @qberthet Indeed, but this is very specific to the discrete case. IMHO it hides under</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1225175807078883328">@sam_power_825 @brewingsense @LenaicCsl posted a picture of this. This works quite well on not too w</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1225175587561648128">@JustinMSolomon @sam_power_825 Actually doing the Taylor expansion at epsilon=0 (beyond the epsilon*</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1225174455053361152">@sam_power_825 Very good questions! The leading term is epsilon*log(epsilon) and we proved it in Sec</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224723466890498049">@qberthet @FatrasKilian The devil is in the details‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224703334721970176">@qberthet @FatrasKilian Well I guess you could decompose your approach as also introducing another d</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224676364416167937">@FatrasKilian @qberthet The paper: https://t.co/uh6GSCgi7T</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224675938279010304">@FatrasKilian This was studied before in a more general form by @qberthet who proves that wavelet es</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224675171010863104">@hronir @j_bertolotti In pratique it is simpler to proceed in another, fully discrete way, which cal</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224674736917053443">@hronir @j_bertolotti This problem has been studied and solved by Ingrid Daubechies. There is a whol</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224674447195549696">@hronir @j_bertolotti To get a ¬´ basis¬†¬ª you need to cleverly sample the set of translations and sca</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224611632854245376">@plasmaStu @j_bertolotti @imleslahdin For the continuous wavelet transform there is an explicit inve</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1224319772172660737">@samy_clementz @inria_paris Yes!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1222895535243161600">@gro_tsen Same, animated (unfortunately linearly interpolating polynomials of very different degrees</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1222586514997174278">@2prime_PKU And his book is free and amazing ! https://t.co/GnSceQvTI6</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221709582281691136">@YuhangChen13 Yes, on strongly convex problems it essentially reduces the condition number kappa (si</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221509156177432576">@Patapom2 @theohonohan @ashishsinhajsr I would love to be able to say this is the case but at some p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221508645420261377">@theohonohan @Patapom2 @ashishsinhajsr OT is definitely not a good idea to perform image registratio</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221473542820286469">@poonawalahasan @Patapom2 @Atrix256 @ashishsinhajsr For me Optimal Matching and Optimal Transport ar</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221362546206216194">@Patapom2 @Atrix256 @ashishsinhajsr TSP is an integer program that you can relax into a linear progr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221359628040253440">@Patapom2 @Atrix256 @ashishsinhajsr Indeed I don‚Äôt think there is a  connection‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221216328750641153">@Patapom2 @ashishsinhajsr Which are themselves optimal transport quantization of grayscale images of</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221216017088745476">@Patapom2 @ashishsinhajsr It is the optimal transport between the two points clouds :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221194927410991105">@ashishsinhajsr Ada Lovelace!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1221005252570353664">@lcastricato @xbresson @jwkritchie I guess @wjcook should be able to comment on this :) A detailed a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1220293324252295168">@Samuel_DavidA I am not expert enough on the question of parameter selection to have an opinion‚Ä¶ e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1220292986258493447">@Yubram11 @imleslahdin @n_keriven One of the inventors of this idea (infinite dimensional Lasso) bei</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1220292398154162176">@Yubram11 @imleslahdin There is a connexion but it is pretty far fetched ‚Ä¶ viewing the parameter a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1218882676662382593">@yureq @Mengsen It is rather common to write PDEs in divergence form. Specially to do numerics.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1218851011017805824">@Mengsen This is the derivative of f^2, ie 2<em>f‚Äô</em>f, this is a non linear PDE.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1217805558792773633">@_Vassim Oui je crois bien, j‚Äôai pris E~V.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1217465184572334080">@BruceTedesco @stephen_wolfram @stephen_wolfram is quite likely to be featured in an Oldie but Goldi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1217008048780300288">@GuillaumeG_ Ou√© mais maintenant tu aides a faire remonter l‚ÄôIdF dans le classement ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1215719310083182592">@j_bertolotti @InertialObservr https://t.co/dWtWU0yW5x</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1215718381984321537">@j_bertolotti @InertialObservr This is the lp norm unit ball indeed. For p-&gt;0 the (non convex) balls</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1215692655293759493">@julienmairal l2+l1=l3 and the dual of l3 is l1.5 ‚Ä¶ Am I correct?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1215225395374690305">@anastasiaopara @ExUtumno Sinkhorn rules :) https://t.co/wObRp0mw0I</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1215190213376532480">@Yubram11 No this is for full matrices and relies on clever algebraic properties of the matrix multi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1214581177157378050">@johncarlosbaez In approximation theory, quantitative results are often call ¬´¬†Jackson¬†¬ª inequality,</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1214519974301224960">@johncarlosbaez There is an error in my tweet, the quantitative (suboptimal) 1/sqrt(n) statement is </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1214468423503294464">@hennesseeeeee Yes by linearity. Beware that this is only true in 1D! In R^d by tensorization the Li</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1214466955870117888">@johncarlosbaez I think your intuition is correct because Bernstein guarantees also uniform approxim</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1214466070427422720">@ian__manchester Indeed! I should have added this! Bezier, splines, Nurbs ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1214443117585346566">@johncarlosbaez No the best is 1/n (using eg chebyshev) but I think you loose all the nice propertie</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213885715299344384">@CuriousMat_mx @abletterer I think understanding the property of global optimizers of the quantizati</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213778566401445888">@raghu_ugare Indeed the Ci are the Voronoi cells of the centroids.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213778263941795841">@xennygrimmato_ The dual of the segmentation diagram (ie connect two centroids when their cells inte</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213560755594911744">@loukasa_tweet 1/ indeed the mesh connectivity needs to be compressed separately‚Ä¶ (using eg EdgeBr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213492314682855424">@loukasa_tweet When you do compression you typically do not need to sort the basis vectors (you just</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213406649278439432">@loukasa_tweet This being said, I believe mesh compression has never been solved in a fully satisfyi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213406218636681216">@loukasa_tweet Indeed I think you are correct and wild behaviors can potentially happens.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213190542043025408">@alkalait I meant that they do not have a compact support. To be fair: this kind of methods are effi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213158989644681218">Although the idea was quite popular, I doubt this method really works in practice since eigenvectors</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213124196693430273">@Atrix256 Maybe ¬´¬†the surface is the graph of the scalar function f¬†¬ª ie the surface is (x,y,f(x,y))</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1213115557643460612">@Assaultarmor Now they are made in Korea.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1212016508232187907">@xaqlab @Carey23Mark https://t.co/nRbwULrsgx</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1209941439305723904">@e_d_andersen Next year I will slice non-convex cones ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1209940495738253312">@e_d_andersen This tweet was crafted specially for you :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1209877958351081473">@jasonemiller Another paper which I enjoyed a lot, defining this stratification using duality (secti</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1209875868920795136">@jasonemiller A nice account for spectral manifolds (which includes the stratification of the cone i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1209427063029014530">@FarzadQassemi It maps any triangulated surface with the topology of a disk toward a convex domain o</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1209158083018186753">@andrew_n_carr Did you checked the associated link? All the code is on github.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1209117171974131712">@lrntzrsc @fpedregosa @ENS_ULM Would you be able to survive in Paris without your dayly dose of foca</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1208458085326557184">@Coni777 @ENS_ULM You are 100% correct. We will change the wording asap.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1208421263473074180">@tradingmaniac You mean for instance on its impact on the speed of diffusion of an associated random</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1208341377723047936">@tradingmaniac In the directed case these definitions are not very useful. Either D-W is not symmetr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1208336926257950721">@HanCao9 If Z is a random vector with covariance A then the covariance of XZ is XAX.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1208119039958167557">@harshaw_tweets If f(0)&lt;=0 then f op convex is the same as f(t)/t op monotone.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1208111968651005952">@harshaw_tweets I think this is the easiest characterization, for instance to show t^p for p&lt;1. In s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1208046929885700097">@akivaw Indeed I should have used this wording.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1208011464893718533">@akivaw Sorry, no, here positive means that the eigenvalues are positive.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1207935955631558657">@akivaw Because you require that the square root itself is also positive.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1207708770316570630">@themarklstone X should be thought as a geometric means of A^-1 and B. The issue is that the formula</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1207608302445707264">@JyongHao The proof I give shows that it is unique. Also since it is solving an Optimal Transport pr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1207038065040150528">@mmdkhosravi You can consider more complicated flows of ‚Äúindistinguishable‚Äù particles. A typical exa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1206898562661003264">@siva2chinni @navOnTttr Least squares :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1206856652269510658">@jacquesdurden Automatic differentiation basically does this for you (aplying adjoint to a vector). </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1206855130093629441">@IgorCarron Yes I should have added to my tweet that it was about least squares.</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@IgorCarron I guess I could do later min</td>
          <td>Ax-y</td>
          <td>_p + lambda</td>
          <td>x</td>
          <td>_q](https://twitter.com/gabrielpeyre/status/1206838520213626882)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1206259526015635456">@elexhobby Indeed on the left when p=1 the outliers are totaly ignored and the fit is perfect with a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1206241378054881280">@elexhobby Plots are for d=10. d=0 would have produced an horizontal line.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1206229807643742208">@danilobzdok Are you thinking about a Lasso? If so, no, this is about changing the loss function fro</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1205558364316422144">@ChadScherrer Yes exactly I think these are all symplectic integrators.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1204736123353915393">@brewingsense @iugoaoj Indeed the log is missing ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1204666123209334784">@Singularitarian I guess the typical instance is that the MLE is the solution of a convex optimizati</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1204663488410505216">@iugoaoj The error is that theta2 is negative so Theta should be R x R_-</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1204471897746006016">@fakbill @fermatslibrary Yes indeed it is true for regular curves.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1204471104414334976">@t_vayer Yes v and U are the global optimum (but this assumes that the correspondances btw the x‚Äôs a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1203961273332965378">@BernardYucazzi @danilobzdok Conditionning is more tricky than marginalizing bc of potential divisio</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1203929463165276160">@BernardYucazzi @danilobzdok Are you refering to the one on conditionning?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1203640568817082368">@josegarc1979 @fespm_es Good catch! https://t.co/SubeDLfVtJ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1203240328624521216">@tomrzah Poke @mathusmassias @PierreAblin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1203045675879014400">@Juke2706 Non d√©sol√© je ne sais pas ‚Ä¶ normalement il devrait etre dans les biblioth√®ques universit</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1203032575993925632">@JustinMSolomon This one is priceless :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202910932197879810">@jayf1989 Indeed since the degree increases it is natural that roots will be forced to satisfy x^n ~</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202910132671307776">@roger_mansuy J‚Äôimagine que j‚Äôai la 1ere √©dition, je regarderai ce soir. En tout cas c‚Äôest un livre </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202906931091976194">@roger_mansuy J‚Äôadore ce livre!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202673622067879941">@jjvie You mean this right? https://t.co/2k75m30gCM</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202622660674572291">@FafaMath This is the reference I have https://t.co/dqOrF3rSuN and seems to be the first proof of th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202622460400668672">@freakonometrics Yes the theorem is still true under very weak assumptions (universality). There is </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202621925014622208">Also highly recommended (as usual) blog post by Jalil Chafai https://t.co/iCXlsYYStO https://t.co/Go</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202540732852187136">@ValRobert974 Oui mais la c‚Äôest plus simple comme polynome, au lieu du polynome caracteristique c‚Äôes</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202539808498823168">@aaronsnoswell Exactly</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1202539500662067200">@vadimkantorov I think there generalizations for eg. random quaternionic matrices but I not know any</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1201205018117988352">@RFlamary @gouypaic @nicolas_courty Congratulations Mr the habilit√© at directing les recherches.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1201117584806043648">@Dirque_L I think that if your cost is smooth you can somehow always say that the Wasserstein flow o</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1201116651372077060">@pfau Yes since the brain is well known to be equipped with a (sub)Riemannian structure everything b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1201115694521303041">@Dirque_L For a Euclidean gradient it should be the squared Euclidean norm, or any cost locally equi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1200327532379344896">@herpsonc @adantro I am using keynote.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1200167598488461312">@francoisfleuret You meant blackboard, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1200124474953273344">@RFlamary https://t.co/BnT0BcbNn3</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1200124242127474688">@RFlamary A demain üôÇ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1199278040884359168">@JeKalifa Same applies to Optimal Transport were Wavelets allows one to get around 1-Lipschitz funct</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1198675619271397377">@Patapom2 @anton_klimovsky I think the trick is that it is a random measure since the lambda_j are r</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1198231015598120960">@rajat_vd Indeed! Or rather Berry‚ÄìEsseen theorem. I do not know any precise connection. Sets replace</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1198228703815327744">@BrKloeckner It is a great question, apparently many results from convex geometry have been extended</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1197768545616220160">@imleslahdin Also Network Simplex algorithms have optimal complexity for OT problems.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1197765264676401152">@imleslahdin Yes OT is a linear program. Dantzig should have got the ¬´¬†Nobel¬†¬ª prize with Kantorovic</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1197528023504969728">@imleslahdin The important take home message is that the optimal transport btw two Gaussian is an af</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1197096336581107712">@tonysilveti I made a mistake, this one is RK4 and  the two others ones are variant of RK2 (midpoint</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1196476400909373440">@LucasVB This is the celebrated ¬´¬†pieceregular¬†¬ª function from WaveLab :) it is useful to contrast F</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1196012225380659205">@uc59 More specifically on Gromov distance btw metric space, Sapiro did a lecture for the Abel prize</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1196011504199450625">@uc59 There are some material on the Abel prize page https://t.co/K1q9hiPhf6</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1195677400895541249">@1austrartsua1 Yes exactly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1195666368441593856">@dohmatobelvis Yep, 3 years of work fit into a deceptively simple sentence :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1195009553211023360">@ismaeI__ Beware that the blue-&gt;red transition is copyrighted :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1194332670475612160">@n_keriven Haha ‚Ä¶ actually I made the break just before diving into the computation :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1194332375276089345">@vaiter Seems very hard‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1194311406168035334">@vaiter I think my graph is optimal (I hope) :) For this function R-&gt;R if you only wants df/dx, all </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1193995804039680000">@BrunoLevy01 But of course they are still going to suffer and do some MLP fun ‚Ä¶ https://t.co/tC4Au</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1193992441382260737">@BrunoLevy01 Also I think it is bad practice to only teach autodiff on feedforward graph bc it is to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1193992216307478529">@BrunoLevy01 That‚Äôs the point of my lecture! After 1h of painful computation I hope there will both </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1193978892064874499">@Atrix256 Of course :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1193479368443600897">@therealoak111 Indeed, and this distance should rather be called Monge-Kantorovitch ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1192714894405185536">@aaronsnoswell Eigenvectors are distributed uniformly on the set of orthogonal matrices.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1192334944422580224">@docmilanfar Indeed! The nice thing is that for strictly positive matrices, this recasts this partic</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1192062250351255552">@stammertescu @ChrKroer Yes exactly! The dual of the dynamic OT is a HJ type equation, and by adding</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1192054785849069568">@FatrasKilian Sorry, I meant ¬´¬†convex as a function of pi¬†¬ª. For the value of the optimization probl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1192049309262503936">@stammertescu @ChrKroer It is corrolary 5.8 of https://t.co/5MHWUaHvkq</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1192043010336247808">@stammertescu @ChrKroer Exactly! It is explained in Christian Leonard amazing work that Schodinger‚Äôs</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1191796146974461953">@ChrKroer I warmly recommend the very nice survey of Christian Leonard on the Schr√∂dinger, including</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1191795710187966465">@ChrKroer The original papers of Schr√∂dinger on the lazy gaz model:  https://t.co/ABdFtTRKvZ https:/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1187264862088499200">@FrnkNlsn (I was referring to the original paper  https://t.co/ZT6cP2VPPv)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1187264543195648000">@FrnkNlsn A rigorous analysis seems to be provided here https://t.co/0jGTgkBqx0</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1187263210598084608">@FrnkNlsn ‚Äúis guaranteed to monotonically decrease the energy E(x) as a function of time and hence t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1186674703500562433">@tchlux I meant that being a fixed point of Lloyd‚Äôs iteration is equivalent to zeroing the gradient </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1186637668047839234">@thesasho @huyhcmut1997 I agree with you. The terminology ‚Äúk means algorithm‚Äù seems quite wide sprea</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1186538664693829632">@F_Vaggi You mean it minimizes an optimal transport distance, right!?? ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1185567627315482624">@hamish_todd The case of 2 players is much simpler because it is simply linear programming which was</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1185564432711278592">@hamish_todd https://t.co/rPK4Hloox8</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1185555174905077762">@hamish_todd I think he did the extension to more than 2 players.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1185199153057554432">@BorisAlmonacid I do not think so.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1185185669183954944">@eugene_ndiaye @mathusmassias It is explained in the section ¬´Extension to subquadratic norms¬ª in th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1185138816153247744">@mathusmassias Exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1183770593776328705">@2prime_PKU @imleslahdin @ArthurGretton The gradient of this potential with respect to diracs‚Äô locat</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1183769866152697856">@2prime_PKU @imleslahdin I am not sure this is what you are looking for, but the mean field of a con</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1183451219819454475">@KyleCranmer @tammuz_ @IgorCarron For those interested, I wrote notes for the master course I will b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1183448071990067210">@KyleCranmer @tammuz_ @IgorCarron ¬´¬†Optimal Transport for the applied mathematician¬†¬ª from Flippo Sa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1183361512485916679">@mashtikhorasani If you rotate too much at some point it got stuck in a local minimum. The example I</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1183346832845529088">Actually I am cheating and I used iterated Optimal Transport in place of iterated nearest neighbors,</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1182665468844433408">@schumaml @thebiologistisn There are much better texture synthesis algorithms available now. One of </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1181565136890466305">@PicaudV There are many tricks and approximate methods to speed up the search for patch matching, so</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1180745262152716288">@BourguignonSlav @roger_mansuy @AuFilDesMaths Pour ceux qui sont int√©ress√©s par l‚Äôarticle sur le tra</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1179382945704005632">@alkalait @InertialObservr Indeed it is exactly the inverse Fourier transform of the Fourier shift t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1179372367820263424">@JasonHise64 This is an interesting question (both for 3rd order in time and space) which was addres</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177952515456950273">@nbonneel @dcoeurjo @Atrix256 @FeydyJean OMG you found the missing link :) Fourier to the rescue onc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177916816448000000">@dcoeurjo @Atrix256 @FeydyJean @nbonneel We should have picked a better name. I think the wording of</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177912888700362752">@dcoeurjo @Atrix256 I should add that I like a lot this paper! https://t.co/JgMOq3hVQU https://t.co/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177912490308571136">@dcoeurjo @Atrix256 Sinkhorn rules and keops is the way to go for large scale clouds, poke @FeydyJea</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177909993615564802">@fakbill Haha je comprends!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177909862799478784">@dcoeurjo @Atrix256 Well, indeed, when I was reading this paper on blue noise OT, I was really frust</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177905606306533377">@dcoeurjo @Atrix256 Is there some theory or analysis supporting that you should do a bit of Lloyd bu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177905177577365505">@fakbill I agree this is probably not the best visualization‚Ä¶ but this is a fairly important and n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177674904776400898">@fpedregosa Very interesting, thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177673054975135745">@fpedregosa This is the ¬´¬†2. logsumexp¬†¬ª method, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177672026389123072">@fpedregosa Why not using the usual log-sum-exp trick?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1177108825087860737">@le_roux_nicolas Anyone to turn it into Python?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1175495926217105409">@francoisfleuret What I meant: https://t.co/DedeWyqjgC</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1175440983120633856">@francoisfleuret I would sort 1:n plus some noise but I guess you look for something more theoretica</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1175336285285814272">@JeKalifa Basis pursuit :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1174234086170341376">@arturgower Indeed. It was intended to say that the approximation is obtained by keeping the M lowes</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1173607960397983744">@jjvie @IgorCarron Indeed :( heureusement qu‚Äôil y en a qui suivent!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1173288912871141376">I forgot to mention that one of the authors is @docmilanfar himself.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1173288491385589761">@docmilanfar So  now I know which article to feature for a Golden Oldie in 15 years :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1171402566556672005">@bekemax It is the orthogonal projector on the set of orthogonal matrices btw.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1171402362658988034">@bekemax It is not really Moore Penrose, it is rather simply putting the singular values of M to 1. </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1170977836016033792">@kgourg @alfcnz That was my initial reasoning but I should have used a simpler red/blue colormap. Ne</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1170936665386508288">@malga_center @rima_alaifari @joseluisromero Optimal Transport meets Focaccia!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1169551180051222529">@docmilanfar @KyleCranmer This is a great reference!! Thx! Are you aware of applications of this met</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1169509311908855808">@docmilanfar So true :) I love this algorithm so much! I am desesperatly looking for a descente pict</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1169507539031121920">@jsdenain I hope it is clear by now that convex cones is a strongly recurring theme in my tweets‚Ä¶ </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1169294660935852032">@RAnachro Indeed for n=2 these are just regular ellipses.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1169290479885856768">@tetraduzione @SimonLacosteJ @gauthier_gidel It is a shame that one cannot revise a tweet without de</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1169285952868626432">@SimonLacosteJ @gauthier_gidel Indeed it was a copy-paste typo ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1169171912402059264">@BachFrancis Sinkhorn is optimal transport with a gumbel noise on the cost. Apparently this is very </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1168611212201996293">@mblondel_ml See you tomorrow :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1168449611872256000">@dohmatobelvis It is only for cones that prox_f* is itself a projector. This is why convex cone is t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1168412984764510208">@HAFriberg For more than 2 terms this notation indicates pairwise orthogonality https://t.co/1JfLOUe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1168144282919849984">@mraginsky @optiML @csilviavr Indeed very nice. With a great picture of Seppo Linnainmaa. https://t.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1168124881910870016">@optiML @csilviavr Apparently an even older reference which makes explicit reverse mode autodiff is </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167866459185246209">@xtimv @mblondel_ml @colloquegretsi Indeed this is also a crucial idea to avoid memory explosion of </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167863163011788800">@mblondel_ml @xtimv @colloquegretsi I do agree! Understanding forward and backward differentiation i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167859964473303040">@xtimv Proof by the example (sorry in french), last Monday @colloquegretsi 48:00 https://t.co/qMOaJP</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167858060246867969">@xtimv This is <em>exactly</em> the same exact words I used in most of my talks to highlight how fundamenta</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167857284153839616">@DCasBol The theorem is ¬´¬†evaluating ‚àáf(x) is as fast as f(x)¬†¬ª.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167856408899047427">@kchonyc @xtimv @xtimv post is great and highlights the key aspect of mitigating time vs memory, whi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167554196742332416">@qberthet @GoogleAI √Ä mardi :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167173544196939776">@PLT_cheater @neuropoetic It means that in 1d the intervals on which the approximation is computed a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167095319169048576">@BrunoLevy01 @FeydyJean I guess the answer depends on the meaning of ¬´¬†work¬†¬ª :) as long as you not </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167084091839320065">@neuropoetic The multiscale nature of wavelets makes them the perfect tool to study fractals and mut</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167069182355283971">@Atrix256 @FeydyJean @nbonneel @dcoeurjo @BrunoLevy01 From LazyTensors to Lazy Thesis Evaluation :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1167061342966210560">@Atrix256 @FeydyJean In 1D OT is solved by sorting. In higher dimension OT is the gradient of a conv</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1165952015136112641">@lostella Indeed, but explicit methods diverge. The attached figure compares explicit Euler (red) ex</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1165948978002833408">@1austrartsua1 Indeed, this is not an optimization problem but min_x1 max_x2 F(x) and the goal is to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1165552184081158146">@PierreAblin Indeed!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1165316023916085248">@boahen_k It is covered in any reasonable textbook on convex optimization. I recommend the one from </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1165298148551659521">@F_Vaggi @lrntzrsc @optiML This is a smart move.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1165296943897202688">@lrntzrsc @optiML You have first to improve the constant in a result of Nesterov to be allowed to ap</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1164494839955775488">@Dirque_L Indeed, a great paper I was not aware of. Thanks a lot for the pointer.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1164494435339636736">The main author is of course @JLStarck himself.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1164248950406361094">@panlepan @geogebra This visualization is amazing.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1164136301379104768">@Dirque_L @lrntzrsc Ok, then indeed it seems to be a very good reference!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1164133722939150336">@lrntzrsc @Dirque_L I thought @Dirque_L was referring to doing classical regularization rather that </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1164131797745225729">@inversed_ru Yes indeed, 2 players zero sum games is a linear program, can be solved efficiently usi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1164102613920616449">@Dirque_L Maybe a specialist of RKHS knows some pointer on these singular kernels (inverse multiquad</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1163469881460875265">@KayVriend Indeed, edge preserving diffusions are used a lot for stereo vision and optical flow, usu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1163381318421532672">@amirvaxman_dgp Indeed my text is quite dumb ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1163026378104758273">@JustinMSolomon @roydanroy @roydanroy is reviewer 2!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162792601717751808">@roydanroy This is an updated version with hopefully better notations and a better wording. https://</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162744369037873152">@roydanroy Are they just notational issues (which are easy to fix, I will) or is it that you conside</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162739970114805762">@roydanroy I agree. I should I used different notations and use the wording densities in place of di</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162730760559894529">@CheisropheL Exactly, I generated P(x,y) from the marginals using an approximate optimal transport s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162726944309469185">@roydanroy Indeed I factored out some reference measures dx x dy. I am not sure why this is an error</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162715259746029573">@panlepan @ilarrosac @RobJLow weak* convergence :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162667752500412416">@imleslahdin No, copula are definied using a warping of the cumulative functions. I will tweet about</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162307496519831552">@mashtikhorasani It is here https://t.co/6gsD3TDsog under grad-desc-ode/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1162036730859524096">@silvascientist @lisyarus @SamuelGWalters Exactly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1161971545587245057">@silvascientist @SamuelGWalters From a 4-way tensor C_ijkl you can make 2 matrices C__{ij,kl} or C_{</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1160835221845094400">@imleslahdin If f is strictly convex the advection converges toward a dirac at the unique minimizer </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1160640979327754242">@roger_mansuy Si la beaut√© est une fonction continue on peut utiliser une approximation du nombre d‚Äô</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1160519249271676928">@A_Aspuru_Guzik Yes indeed the initial definition was given by Wigner for quantum mechanics. It has </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1160212190315630594">@DrAndreDavid @mblondel_ml @vnfrombucharest Yes exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1160162333831979009">@DrAndreDavid @mblondel_ml @vnfrombucharest A way to see it is that it nicely generalizes the way lo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1159882638263574529">@JeKalifa @mariotelfig Well ISTA is just a mutlistep wavelet-vaguelette scheme, right!?!!? And mirro</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1159422859548286977">@octonion Indeed you are correct there is no need for a variance constraint and then the solution is</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1159418880114200577">@octonion If you do not add a constraint on the variance the solution does not exist. If you add a u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1159094057450332161">@Ad_Bakir Yes exactly, that was my point!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1159051082053668864">@dangpzanco @actualmberens I was thinking one can use a l1 norm regularization (Lasso) to obtain a c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158816114555666432">@eigenhector @LenaicCsl @GSavare Yes exactly, since you have access to analytic expressions for the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158807780393988096">@eigenhector @LenaicCsl @GSavare Some references for the interested reader: https://t.co/wFg6NTsddm </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158806919970283520">@eigenhector My recommendation: win on both sides by doing both and put weights on the particles. Wi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158738374586335232">@loukasa_tweet @m_deff ML considers only Lagrangian discretization, using a grid in high dimension w</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158371828148166656">@JBlevins0 @mounialalmas I should have used the wording ¬´¬†likelihood function¬†¬ª instead of ¬´¬†likelih</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158290672803307520">@GuiseppeDon @paolagorigiorgi Indeed for large n it is either the first or the last monomial who dom</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158288122343501824">@neozero497 @datitran The video is showing how the posterior is impacted by the prior.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158269216870600704">@Quadriviuum @roger_mansuy Yes definitely! Simplest random analytic functions are indeed defined on </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158267788005269504">@imleslahdin Also, in computer graphics, people often prefer Poisson disk sampling (I guess because </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158267261951586304">@imleslahdin A related (overlapping) concept is determinantal (repulsive) point processes, which are</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1158023941262315530">@MachartPierre The codes to make the figures are here https://t.co/6gsD3TDsog</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1157976921780695040">@KrzakalaF Spoiler: Oldies but goldies: Laplace, Sur les naissances, les mariages et les morts √† Par</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1157316075450765312">@Swaarx Indeed this paper contains important theoretical results regarding the concentration of the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1157257579279593473">@gregeganSF The general solution to this problem (Babbage functional equation) is given here: J. F. </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1157240481706905600">@_onionesque @pfau @HaggaiMaron @yeewhye Indeed, I read both papers while working with @n_keriven on</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1156901954829856769">@KyleCranmer @IgorCarron There is a nice paper by Carlier et al to generalize quantile regression in</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1156901386241622017">@KyleCranmer @IgorCarron Yes this can also be used to define copulas in higher dimension, although i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1156865521272463361">@KyleCranmer Also Optimal Transport is a way to generalize sorting to higher dimensions since the OT</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1156863650294095875">@KyleCranmer Sinkhorn rules :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1156541565315235841">@francoisfleuret The other alternative is to use QR method on the companion matrix. Not super clear </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1156538143815983105">@francoisfleuret There are ways to implement this in a stable way and also accelerate convergence to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1156473148029755392">@achambertloir Writing ¬´theorem¬ª in my tweet is super misleading‚Ä¶. let‚Äôs say it is to check who is</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1156464218448826368">@achambertloir Yes definitely! Durand‚ÄìKerner can be recast as a Newton method.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1155888241083604992">@panlepan You are 100% correct. I will redo the figures with centroids instead asap.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1155421226598313985">@lrntzrsc I think you were the one who suggested I should feature this book.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1155205868406935552">@guildwyn The sphericon is another example https://t.co/k9VrsysXI2</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1154511901516324870">@lrntzrsc Mine was not involving any random process though.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1154096508914077696">@KyleCranmer These questions are out of my league ‚Ä¶ @LenaicCsl spent quite a lot of time on these </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1154094428228915200">@KyleCranmer It is this paper of Gurvits https://t.co/A3ZVuPjnxA which uses Q-sinkhorn to approximat</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1154093593692446721">@Dirque_L @GuillaumeG_ It only applies to positives matrices ‚Ä¶ but who would dare to use non posit</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1154089982023340033">@KyleCranmer Nice, I see you cite the Carlen and Maas paper :) But unfortunately there is not a nice</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1154088179391180801">@KyleCranmer And you can also generalize Optimal Transport to transport density operators in place o</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1154087678192902149">@Aufinal @katecrawford @ENS_ULM @FondationAbeona I guess Kate will explain this in her inaugural lec</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1153992200755261440">@GuillaumeG_ Another (non commutative) formula which makes lot of sense is sqrtm(x)<em>y</em>sqrtm(x). I th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1153984829983248386">@GuillaumeG_ Pro-tip: matrix product is so old fashion! You need to switch to the commutative formul</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1153927734822887425">@jeresuikkila Indeed the times have been rescaled. This is indeed a bit misleading, the time being p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1153634636704243712">@jm_alexia Unfortunately I doubt there is such a nice formula. You can compute it as L1 norm of the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1153578477469220865">@ahmaurya No, the Wasserstein distance is not the euclidean distance between the sqrt of the covaria</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1153368233543643137">@panlepan I also did the same computation :) so this is actually worse in 3D than in 2D ‚Ä¶ is there</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1153342948991483904">@panlepan The approximation you get from the area is better than the (irrational) one you get using </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1152520142754107392">@mmbronstein Probably the best reference is actually your book, chapter 7? https://t.co/3QYXQXh84n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1152254243946074112">@BrKloeckner @tomrzah And I cannot miss this opportunity to mention that it is also the way to prove</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1152233438188986368">@BrKloeckner @tomrzah Not sure if this is related to Ruelle¬¥s result: to have a quantitative estimat</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1152126224539537409">@MorphoLg Indeed they are different. I should have rather written that A^n p converges to the leadin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1151763077425242112">@kcimc @zzznah @quasimondo You should check @FeydyJean keops toolbox which computes kernel on the fl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1151506580178919424">@xvrtzn I do not know. But DC programming has been used to minimize huber loss functions. I think @j</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1151505916354863104">@drherryandmrone @nicoguaro I have to check but I think smooth functions are locally DC.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1151505531833593856">@neuropoetic @IRCSS The convex (resp. concave) part is affine on parts where the function is concave</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1151504836925501440">@R4_Unit @stubborncurias I do not think there is any theory supporting the fact that DC algorithm co</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1151398186130530304">@stubborncurias Once you know this decomposition (well this is the catch‚Ä¶) you can do a simple but</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149604116194185216">@therealoak111 Paul Appel https://t.co/IF9yNe9iU2</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149590682232713216">R√©mi pointed me to this memoire of Appel that nevertheless seems to contain a fairly large amount of</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149589489653698562">He finally slipped into a few of them, serious inaccuracies. (3/2)  https://t.co/xjBWqg2und The firs</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149588854048882692">Indeed, while some barely touch the question of the filling of volumes, which is the most difficult </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149588677191847936">Five papers were sent to the Secretariat. Almost all are interesting works and reveal a distinguishe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149416932254388224">@mraginsky https://t.co/ckrom9tqqk</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149415957061931008">@mraginsky I only work with Radon measures on metric spaces :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149410191533903872">@odakyildiz https://t.co/s8unvTgu31</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1149201704988135424">@imleslahdin I just meant that the space of all probability distributions is in general infinite dim</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1147883644230520832">@PLT_cheater @keenanisalive It is the discretisation of the Laplancian when using P1 linear finite e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1147817321706266624">@prezcannady I will tweet about # very soon. If alpha is a discrete measure, T# corresponds to movin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1146468244984868865">@kapouer Yes. As opposed to heat diffusion (which has inifinite speed of propagation), diffusion in </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1145024738982604800">@NGhoussoub @stevenstrogatz @TallPupper I could not agree more! Tools from PDEs, such as gradient fl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1143775296350564355">@PLT_cheater @GuillaumeG_ Sorry I meant the norm which has the polar ball as unit ball. It is the Le</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1143612042173194246">@PLT_cheater @GuillaumeG_ The polar of a norm f applied to singular values is the polar of f applied</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1143604881426669568">@GuillaumeG_ @PLT_cheater Well they need to be spectral balls associated to polyhdral function of th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1142335848333172736">A link to the original paper https://t.co/LFT9doS9B0</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1142173163259867137">@dohmatobelvis What is the difference with the dual formulation?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141962207489613824">@nlswrnr Yes Indeed! Fourier is periodic BC and DCT is Neumann BC (this somehow the way Strang deriv</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141952577975848960">@nlswrnr What I meant is that DCT functions do not satisfy f(0)=f(N) so unlike FFT they are efficien</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141404251912114176">@n_keriven @achambertloir @roger_mansuy @maxime_ramzi @trekkinglemon https://t.co/RJkVmTBJlI</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141374449083736066">@achambertloir @roger_mansuy @maxime_ramzi @n_keriven @trekkinglemon With extra regularity hypothese</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141316073465270275">@roger_mansuy @maxime_ramzi la preuve classique de densit√© des r√©seaux de neurones se fait par Stone</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141098953229971463">@maxime_ramzi @roger_mansuy It can be generalized in higher dimension and is the basic theorem of de</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141073172164427776">@roger_mansuy https://t.co/jF6xyTxC08</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141071427178811392">@roger_mansuy ‚ÄúKolmogorov complexity reversed‚Äù</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1141069945406705669">@roger_mansuy 1</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1140681844842799104">@PierreAlquier @ENSAEparis Tu pars o√π ??</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1140247237269831680">@NacchoArroyo It is indeed a quantitative version of the universality theorem.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1139599677383462912">@achambertloir @francoisfleuret Here is a post on the Hopf-Cole transform  https://t.co/4WZpEJ2F9v</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1139594709440110592">@achambertloir @francoisfleuret And by the change of variable exp(a/epsilon) one approximates non-li</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1139593818922205185">@PierreAblin @adamkelly2201 Same!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1139586444375265283">@francoisfleuret You might also want to check paper by @mblondel_ml and @arthurmensch that uses this</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1139183260385390593">@arthurmensch @mblondel_ml ‚Äúdo not try this at home‚Äù ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1139065771903127552">@roger_mansuy @achambertloir Oui la positivit√© de la matrice est au sens des matrices SDP, donc trac</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1139062265385299968">@achambertloir a+b&gt;=0 is lacking from the third you are right‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1138711451705794560">@salmonjsph @asmeurer I have updated the notebook https://t.co/fIpBSqbqDQ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1138374815168028672">@janfmVI @francoisfleuret It was rather to insist that if one does not put any constraints on the st</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1138374293732110337">@janfmVI Well to be fair Maiorov&amp;Pinkus is not the good old universality result, it is rather a curi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1137699307971330048">@Dirque_L Never tried, but I think you are right that doing OT between curves will give weird result</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1136360073939210240">@docmilanfar @ProbFact @KrzakalaF Sinkhorn rules üôÇ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1136337448320065537">@pflama The associated algorithm is called Fast-Marching, it can be used if you discretize on a grid</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1136336989526122498">@ProbFact @KrzakalaF Birkhoff-von-Neumann! https://t.co/pBpedt9okz</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1136335285736935424">@GuillaumeG_ https://t.co/Jexon4pCwH</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1136310710231797761">@GuillaumeG_ Total Generalized Variation ??</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1134852880157089792">@neuropoetic @sigfpe Shows how the eigenvalues of a matrix change when one linearly interpolates bet</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1134082339980660736">@tomrzah @EvpokPadding @Moins_et_mieux Les ondelettes sont fr√©quemment utilis√©es en analyse harmoniq</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1133858183196360707">@dohmatobelvis I will probably disappoint you, but it is just Matlab scripts (yes, I know I should u</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1133317926323412992">@cfevotte Great idea ! And it also connects well with Ridchardson-Lucy through multiplicative update</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1132977928386600960">@tomabangalore @gro_tsen I guess this raises the question of wether the simplex is ‚Äúabstract‚Äù or emb</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1132588702465699840">@nbonneel Yes indeed! I meant that the rate is independant.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1132301760935157761">@PopulusRe Phyllotaxis as a self organizing process https://t.co/Gsh2Oj0oiN https://t.co/xLRCcqdPQZ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1131988351358242818">@sohail__b @SebastienBubeck This is exactly what I initially thought, but unless I am wrong, this is</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1131986972568895489">@sohail__b @SebastienBubeck You are 100% correct, this an enormous typo ‚Ä¶ at least my code is corr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1131244434874621952">@salmonjsph I will redo the figure <em>after</em> the neurips deadline ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1131244244918837248">@Dirque_L @MSc_Bakir I do agree :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1131204424033079296">@GuillaumeG_ @arthurmensch Yes exactly!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1130484973318868992">@NGhoussoub The original one: https://t.co/BODHYgL7Gj</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1129389260686204928">@le_science4all I only work in Polish spaces so that I can only use the countable axiom of choice.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1129264509351288832">@omaclaren @edwardhkennedy Sorry here p=1 in my initial tweet but can be generalized to other expone</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1129263812920659968">@omaclaren @edwardhkennedy Cramer is often called ¬´¬†energy distance¬†¬ª in higher dimension. It is a M</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1129262647436881924">@omaclaren We have introduced ¬´¬†unbalanced OT¬†¬ª to aleviate problems with far away mass and be more </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1129009174384451584">@RickWicklin Indeed it is a typo‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1128995410209595393">@unsorsodicorda Yes exactly, the KL and other Cizar divergences are not smooth with respect to conve</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1128977553451102210">@ogrisel You are correct, it is Harald Cram√©r https://t.co/x7LDVHeY8U https://t.co/5Ldr8qdBm6</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1128562759770300416">@JeKalifa If you like sparse spikes deconvolution with ISTA you will love @LenaicCsl &amp; @BachFrancis </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1128256726808109058">@HaggaiMaron @n_keriven Thanks ! You paper on invariant and covariant network is amazing! It will so</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1126497546656931841">@nbonneel @icmlconf I guess it was something like ¬´¬†OT is the new electricity¬†¬ª. Or maybe ¬´¬†OT is th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1126375647914467328">@jjvie @icmlconf From @JustinMSolomon himself!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1126375404812554240">@jjvie @icmlconf But we all know the untold truth that GANs is just a special case of OT trivialized</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1126375001396064256">@SimonDeDeo @siggraph @nbonneel @dcoeurjo Exactly! A pedantic statement would be that KL is not cont</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1125410311178280961">@SaadJbabdi Well (half joking), I wrote a whole book on this question https://t.co/bszcV2GnL8 This r</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1125331162354921472">@SaadJbabdi Another idea for permutations matrices is to view them as extremal points of the polytop</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1125330731004198913">@SaadJbabdi A great idea is to view orthmatrices as eigenvector of symmetric operator and look for t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1125084924946599942">@BrunoDuchesne Il ne saurait tarder ! Surveillez aussi le prochain bulletin de l‚Äô@Apmep_Nat !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1124728555756298242">@steranhidalgo Of course!!! Also vision, graphics, ML, etc.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1124410136624234498">@james_nichols That‚Äôs fine too :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1123300632658677760">@FranckIutzeler Optimal line search should lead prospective phd students to Grenoble!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1123299793298763783">@F_Vaggi @danieldazac @kastnerkyle @PyTorch Geomloss is indeed the reference GPU implementation when</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1121321932870311936">@BrunoDuchesne It is the area of the interpolating shape. Actually there is a typo, this function is</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1120959203236093952">@lmaogh There are extrinsic flows (they depends on the way the curve is parameterized/discretized). </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1120958171646971904">@vit_tucek I will think about it. h impacts the dynamic so it does not seem obvious.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1119551038141284352">@PierreAblin This is only valid on a vector space, right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1118826768628998144">@vaiter Thanks I wanted to put this link but I shamellessly put a link to our instead ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1117357916707860481">@bobshush No, they are fractals! Global convergence of Newton algorithm is a very delicate topic :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1116983495610363904">@stubborncurias And in turn, being moments of measures can be encoded using a semi-definite matrix (</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1116982898769350656">@stubborncurias But now F(mu) is just a sum of moments, so you can replace the minimization on measu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1116982397814202371">@stubborncurias Good question! The breakthrough idea of Lasserre is to replace the finite dimensiona</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1116941375822413824">@_onionesque Yes I wanted to start the ball rolling! Which paper would you advertise for ?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1116730004719665158">@_onionesque Multiscale Representations for Manifold-Valued Data https://t.co/BszVqxbOEH It is the f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1114429033394184192">@tomrzah @swairshah @Creepazoid2 Ah yes indeed I forgot the most important‚Ä¶ this algorithm is the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1113902109089517576">@IgorCarron @daniel_bilar @mlia_lip6 @quobbe @InHenriPoincare Yes! They will be on the YouTube chann</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1113700133760380934">@lrntzrsc @InHenriPoincare Mention of a regularization parameter even started the alarm of IHP. You </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1112784323281813504">@AlexAbreuDev @InHenriPoincare Exactly :) The website of the 3rd workshop https://t.co/to7GOE3bm7 Vi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1112652471288152065">@PogrebnyakE I guess the important word is ¬´¬†illustrating¬†¬ª :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1109784092093177856">@Josh_McGee_G @danbri An Eigenvector of the laplacian (display here in red/blue) generalizes sine/co</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1106455839685132288">En version fran√ßaise : https://t.co/4B0vA58yto. Avec une pens√©e bienveillante pour les jeunes qui vo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1106268645364297728">@Michielstock I started tagging them to make Twitter moments, but the interface is so painful I neve</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1106148435030409216">@CSProfKGD @PyTorch It is back online now.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1105574968312688642">@Michielstock Let me know if you need slides of the tweets.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1104734380826787840">@cecilejanssens AUC as well as other discrimnation scores (eg Kolmogorov-Smirnov) are for 1D distrib</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102935378359664645">@gedaliap @ReluctantPotato This will be covered in a forthcoming tweet!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102919657634304001">@EricRichards22 Haha yes probably, but it is the way I best understand it :) and it generalizes nice</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102919119740039168">@janfmVI @ihHashem @Oscar_Pudding @KULeuvenBioTeC This algorithm actually solves a linear optimizati</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102902521121701889">@ReluctantPotato @arthurmensch @mblondel_ml @vnfrombucharest As a totally irrelevant side note, the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102902008598663168">@ReluctantPotato But you can backpagates dijkstra to compute the derivative of the geodesic distance</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102901474454056960">@ReluctantPotato It acts as a grassfire, it inflates outwards the set of nodes with a constant dista</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102890517283643392">@ReluctantPotato Not exactly, you need a priority list (implemented using Fibonacci heap) to explore</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102866668395597824">@MonniauxD @achambertloir @JLJulienLefevre The decision tree for quantifier elimination for existenc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102865894907219968">@vnfrombucharest https://t.co/WWSt5Xa6Zv</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102859601584840704">@vnfrombucharest It denotes the set of neighbors in the graph.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102723627760402439">@achambertloir @MonniauxD @JLJulienLefevre https://t.co/2sixv9Flt6</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1102677888405565440">@achambertloir @JLJulienLefevre @MonniauxD For a planar polygon its kernel can be computed in linear</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1100703959801626626">@PLT_cheater @3Dmattias Yes approximation theory for integral operators is very well studied. For a </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1098859252834320384">@PicaudV Yes exactly, in the case there is no constraint you are correct.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1097157357623300101">@ZhaoDeli The code is here: https://t.co/OD6URhriGo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1095729927800963072">@therealoak111 @ChrKroer Sorry I meant Csisz√°r https://t.co/BI2UFW11BB</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1095716803362414592">@ChrKroer This is what I called Cizar divergences in my tweet :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1093615464121536513">@themarklstone I will do my best. It is on my stack to implement the different existing alternative </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1093608539342426112">@themarklstone IMHO the field of OT for matrix-valued measures (or OT over a single matrix) is not y</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1092873604180643841">@ChrKroer @<strong>__kees</strong> I rather had in mind the setting of a continuous space of action, so that x an</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1092795118573309953">@ChrKroer @<strong>__kees</strong> I see that my wording is confusing everyone :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1091089699832283138">@mathematicsprof Backprop is the adjoint of the usual chain rule, so IMHO it is simple but not as ob</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1090885795060305925">@RLecamwasam A nice review with emphasis on applications to supervised learning. Of course the pract</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1089791160816750592">@Atrix256 I am not sure eg quasi random sequences converge faster. But Poisson disk is slow to gener</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1088417040707272706">@R3RT0 Oh you are right! It should be like this. #DoNotForgetTheMinusSign https://t.co/T6FuIGSwrP</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1087838662216294400">@mathematicsprof I would say it is bc of rotation invariance of the Laplacian.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1087777557364293637">@PLT_cheater @DSP_fact But you did not randomize the phase, right? After randomizing the phase you c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1087744022880403457">@PLT_cheater @DSP_fact It will sound like an audio texture, almost like a colored gaussian noise. Ra</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1086951884538236929">@0xDEAD0xBEEF Indeed! This being said, k-means and mean-shift are different (but related) algorithms</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1086951202108116992">@Dirque_L I think the mean shift is simpler, evolving particles do not interact, it is just a gradie</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1086950215561764870">@arsatiki No, it is a simple gradient ascent, particles simply moves to the closest mode (local maxi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1086167840372244480">@RobJLow Otherwise the correct notion is rather stable manifolds in place of stationary points.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1086166463331864576">@RobJLow Yes sorry you are correct my statement was in fact for gradient fields.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1085803590818717697">@docmilanfar And Catmull Pixar of course ‚Ä¶ subdivide and conquer :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1085653764894158849">@GuillaumeG_ La qualit√© d‚Äôune propal est fonction du nombre de \vspace n√©gatifs.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1083643477743812609">@GaelVaroquaux @ArthurGretton Yes this is indeed the case that MMD means kernel norms and IPM means </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1083635544448516097">@GaelVaroquaux I think you are making a mistake and mixing MMD with Integral Probability Metric (IPM</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@omaclaren You should use cost functions</td>
          <td>x-y</td>
          <td>^p with small p. You can even use p&lt;1 (concave cost), ](https://twitter.com/gabrielpeyre/status/1083480349764669440)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1083266323122188288">@paulportesi Indeed. It is Optimal transport at its best!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1083059868842512384">@themarklstone @inria_paris I could not agree more, Dantzig should also have got the Nobel for sure.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1083023742761078785">@themarklstone @inria_paris It‚Äôs a typo! It should read PDE==Partial Differential Equations.</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@PLT_cheater @DSP_fact h is a 1D high pass filter, its Fourier transform at frequency omega is</td>
          <td>omeg](https://twitter.com/gabrielpeyre/status/1082757022171447296)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1082744219167571968">@PLT_cheater @DSP_fact Almost, but you have to pre-filter each slice by h before stacking them toget</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1081308299897831424">@ThomasEWoolley Very casual simulations :) The code is here https://t.co/JLRMZahzCQ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1080880447582887936">@ddcampayo @necoleman Haha what a typo Indeed!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1080737804198559745">@abletterer ‚ÄúSo much effort is being spent on streamlining mathematics and in rendering it more effi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1079757763931193345">@PLT_cheater Sorry the legend is missing. It displays the fonction psi which defines the potential. </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1078536695681859585">@guillefix Yes this is true, but rather bc of the Fourier convolution theorem.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1077992379318190080">@JuanpaMF C‚Äôest une fa√ßon usuelle d‚Äô√©crire avec une integrale le produit de dualit√© entre une distri</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1077899962158116864">@adn_twitts I think they are roughly equivalent, but it is handy to have the same coding environneme</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1076570553866244096">@GuillaumeG_ Petit joueur üòâ https://t.co/vw2appZx8s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1075665268012171270">@francoisfleuret I couldn‚Äôt agree more. Writing maths should be like thinking about error correcting</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1074811509342130176">@RicciFlat I would not say that Ricci flow generalizes mean curvature flow. Mean curvature flow work</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1074709321093652480">@shapeoperator @dandersod Willmore is to mean-curvature what Laplacian heat is to bi-Laplacian heat </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1074694097326272512">@necoleman It is a heat equation of a parametric curve where the tangential displacement is killed t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1073119898086330369">@amirvaxman_dgp Indeed but a disk is convex :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1071693491443507205">@j_bertolotti A reference for the linear heat equation on Riemannian manifolds? Or reference on  non</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1071020808380780545">@isopteryx This one is the most recent: https://t.co/LCjZJ9Poxl Otherwise : https://t.co/s4yChh6bsT </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1068117801460740096">@StefanoConiglio i,j=1‚Ä¶n</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1067686067875254272">@Santiag72427700 @zdeborova @KrzakalaF We wrote a book: https://t.co/r59H96rRHI</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1067343222572294144">@LimeCrusher @wgeary I think it is just a Vornoi cell of a point which is not displayed. Great idea.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1066057422152830976">@kiaderouiche POT https://t.co/XIIvnr4jI2</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1065872286652268545">@360unfiltered @DynamicsSIAM This was my tweet 4 days ago on geodesic in heat (which is the stationa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1065867337243508738">@Maransadagopan Fick‚Äôs law is the linear diffusion (heat) so there is no chock. Right?</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1065712843159465990">@dohmatobelvis You can generalize Hellinger to arbitrary power a using (1-t^a)^(1/a). There are othe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1065531305193029632">@n_keriven Indeed ! If anyone has a picture of Sinkhorn, I would be highly interested. I would make </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1065240948026822656">@makc3d On a uniform grid A+B is the support of the convolution of the indicators of A and B. So it </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1065153632495128577">@BarbaraFantechi Indeed! And I guess it works not only for (R^n,+) but for general monoids.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1063883570467475457">@PLT_cheater @BraneRunner @viettran86 Yes each stage keeps the matrix of the same size. This is bc t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1063782421785645056">@dpholmes As in my tour https://t.co/EiZCJfNIHH</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1063782074446954497">@dpholmes You are correct and it is a mistake. I will repost today this tweet with my flower image.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1062367044442689536">@abletterer You are correct, the white is indeed 0!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1062270470412738560">@Mattmilladb8 This is explained here https://t.co/GUkURUmIp6</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1057946174966165504">@roger_mansuy @achambertloir You should also check this very nice video by @vihartvihart https://t.c</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1057937179333353472">@alexesquenet @DamienERNST1 Well, 1/t^2 is optimal ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1057937079890558977">@roger_mansuy @achambertloir Indeed, I did this tweet on purpose to advertise for the next MathPark!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1055142690411687936">@Dirque_L It can be decomposed as a sum of 2 rank 1 matrices: (1:n)‚Äô<em>(1:n) + (n-1:-1:0)‚Äô</em>(0:n-1)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1054442292579315714">@Dirque_L The vector [1,-2,1,0,‚Ä¶,0] is in its kernel.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1052193311241326592">@fayolle @ENS_ULM Indeed deadline is d√©c 31st 2018!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1051140592233828352">@lschmelzeisen Maybe our book ? https://t.co/r59H96rRHI</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1050451954059681792">@ddcampayo The third condition should read $sum_i lambda_i(x) x_i = x$ btw.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1046292712268460032">@IgorCarron It will!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1044286447153491968">@jonnazar The tree pruning algorithm is the bottom-up one (you first grow a full tree and then prune</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1044263726562586624">@jonnazar Cart finds the global minimum among all possible k-trees. Here it is displayed for k=4. It</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1043926798893891584">@cjordansquire The code is here https://t.co/HEP0lEZNHi, for this tweet it is in laplacian-spectrum/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1043926598901223424">@amiltonwong The code is here https://t.co/HEP0lFhp5S for this tweet it is in level_sets/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1041942556349800448">@tomabangalore It the one displayed on the bottom-left of the figure (which shows an interpolation b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1041574777155137536">@nbonneel Indeed! It should be equilateral for the inner product defined by the diffusivity tensor.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1040202577571454976">@omaclaren This 73 paper was published in Geophysics journal ;) and David Donoho‚Äôs father was geophy</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1040198709114490880">@omaclaren I think it is much older, at least 1973 with Claerbout and Muir for l1 in geophysics.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1040144972106883073">@cbouveyron It is an axiom that everything pre-AlexNet (2012) should be considered Oldie.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1038806638537441281">@BrunoLevy01 @tomabangalore It is a bit weird, I do not see how a sphere could be so different from </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1038758834569924610">@BrunoLevy01 @tomabangalore On the circle you can do the computation explicitely and it works. At th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1038745268945149952">@BrunoLevy01 @tomabangalore Apparently the definitive reference is https://t.co/WXtyNPHbZA but you h</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1038726832810610688">@BrunoLevy01 @tomabangalore From what I remember it actually works for any compact manifold (at leas</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1038393860236472320">@ThomasKealy It it is the so called universal threshold proposed by Donoho/Johnstone. It ensures for</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1038080318312534017">@JeKalifa Well of course the DCT is much older, this paper of Strang is more like a review of existi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1036858068112482305">@BrunoLevy01 @Peter_shirley I even wrote (in French sorry) some notes about this when I was studying</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1036180386060099584">@francoisfleuret @PierreAlquier Moi je ne travaille qu‚Äôavec des matrices SDP ;) Mais oui je vote pou</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1036157706384683008">@jjvie Sorry, it makes the tweet indeed a bit cryptic (but it is very convenient). It is the push-fo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1035551486984507393">@BEBischof It is the k nearest neighbor graph of a set of points. Initially the graph has two disjoi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034355381571661824">@AlexanderLyNL @le_roux_nicolas Yes thanks this was also mentionned by @ogrisel</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034130140496846850">@le_roux_nicolas @ogrisel Ok great, thx to both of you for the clarification!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034116120121430023">@GaelVaroquaux @ogrisel @le_roux_nicolas No, I think it is just that the LL needs to be twice differ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034100223289778176">@MannyDePresso @ogrisel @le_roux_nicolas I agree!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034100022902706176">@ogrisel @le_roux_nicolas Wikipedia seems to say it is always true ¬´¬†Under certain regularity condit</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034090137465090048">@le_roux_nicolas Aren‚Äôt they always equal ? It comes from the unit mass constraint I think.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034087628881182721">@AlexanderLyNL Sorry I was unclear, I actually liked the blog post you sent bc it shows that there i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034056434370142213">@AlexanderLyNL Fisher is just its restriction to a parametric sub-manifold. Then all invariances are</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1034056368171429889">@AlexanderLyNL IMHO presenting Fisher through the prism of KL (or any divergence) is a bit misleadin</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1033335820088995840">@ferranpujolca Mine but it is in french ;) https://t.co/FuM7Lzyv2Y The Fulton is great imho for fini</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1033260841917992960">@_onionesque Same for me :) @risi_kondor has many amazing papers on the use of non commutative harmo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1033260327260106752">@_onionesque I think the best illustration of this is to contrast representations of SO(2‚Ç¨ and SO(3)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1033258345019400192">@_onionesque The use of higher dimensional representations (eg matrices) is due to non-commutativity</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032733939746660353">@dohmatobelvis Also W_p is costly to compute (cubic time) whereas Euclidean distance are faster (qua</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032733498216460288">@dohmatobelvis W1 is very bad with respect to sample complexity (exponential with respect to dimensi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032729440554508288">@bdupont2 @glouppe I agree that the correct wording is ¬´¬†equivariant¬†¬ª. In signal processing people </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032728782136897541">@dohmatobelvis Not everything is about the topology. The geometry (curvature, geodesic, gradient flo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032543108226056192">@NicaoGr I can send you the sources if needed.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032533316514324480">@NicaoGr I use keynote.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032522953550905344">@mikaelrousson @glouppe Agreed! But you could do no subsampling and rather inserts 0s in the filters</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032514762012282880">@glouppe Or maybe you mean that larger receptive fields means also more ¬´¬†invariance¬†¬ª in the classi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1032513903773794304">@glouppe Naive question: is this true? Here ¬´¬†shift invariant¬†¬ª simply meant it commutes with transl</a></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[@adalisan Indeed,</td>
          <td>h</td>
          <td>_H^2 := &lt;h,h&gt;_H](https://twitter.com/gabrielpeyre/status/1032014214104272897)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1031932273782939649">@goodfellow_ian Writing f=Phi(h) with h in H, values reads (f(xi))_i = Psi(h) for Psi the ¬´¬†empirica</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1031926934496849920">@goodfellow_ian No, sorry, I just meant that the function E depends on the values of f, does not nee</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1030459485985415170">@NTourgueniev @lostanlen @lamethodeFC @franceculture Claire Mathieu : algorithmes, parcoursup Cordel</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1030213010541961220">@lrntzrsc It means the pre-GANs era.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027924826093170688">@optiML @dohmatobelvis Up to a constant it is the same expression. Was first noticed by https://t.co</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027903983824908289">@dohmatobelvis It generalizes to elliptical distributions.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027856228192538624">@Dirque_L @mathematicsprof Haha :) I guess it should be ‚ÄúOptimal Transport Inc‚Äù with our leading pro</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027855026079842304">@dohmatobelvis On Gaussians, if you contrast Fisher vs W2, it corresponds to the Poincar√© half plane</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027854287823622144">@dohmatobelvis So computing the equivalent of the Fisher metric for W2 would require solving a weigh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027853942510821377">@dohmatobelvis Infinitesimally, W_2 at a measure mu matches at 2nd order the negative Sobolev space </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027494027879477248">@mathematicsprof 3/3 Also connected to Sinkhorn approach to compute optimal transport, revitalized b</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027493349371129856">@mathematicsprof 2/3 This is tightly related to the description of the heat equation as optimal tran</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027493106613145602">@mathematicsprof 1/3 Another fascinating appearance of entropy (maths/stat-phys) is Sturm-Lott-Villa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027261581443391490">@optiML @dohmatobelvis I tried to convince people to name them MK (Monge-Kantorovitch) as suggested </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1027156079220862976">@urcum62 I could not agree more, this is one of the greatest and most inspiring article at the inter</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1025820638039236608">@tomrzah @FACS_Flow J‚Äôai un numerical tours sur PCA https://t.co/ozSAqbDOS7 ‚ÄúPCA, Nearest-Neighbors </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1025318815264124928">@NicaoGr @tomrzah @EvpokPadding If you are in Paris I guess you can ask to be ¬´¬†auditeur libre¬†¬ª.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1025318439521583104">@CreeepyJoe The link to the course notes is in the tweet. I will update them soon.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1025309896135385088">@EvpokPadding Could not agree more ‚Ä¶ I will just give some key insights about multiscale analysis </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1025309254322999297">@jhaberstro Nope unfortunately‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1024706643001573376">@GuillaumeG_ ‚Ä¶ √† faire le rebutal NIPS ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1024652814923063296">@le_roux_nicolas Indeed Figalli has tons of co-authors (and tons of papers). But the other ones also</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1024651515766689797">@optiML Indeed! Figalli was Villani‚Äôs (and Aldo Ambrosio) PhD student.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1024263418814124032">@docmilanfar Indeed, thx!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1023836477858480128">@luis_andres_v This is Indeed a nice algorithm to solve ROF. Some more background here https://t.co/</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1023482187868516353">@tomabangalore You are correct, it is the mean curvature flow (not the Ricci flow, it operates on th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1022759490721132544">@odakyildiz I am using a nearest neighbor density estimator, which is very convenient.This review pa</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1022099637862375424">@roger_mansuy @achambertloir @ENS_ULM @psl_univ @TwittFMJH Yann m‚Äôa l‚Äôair tr√®s dissip√©!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021509626846740481">@BelhalK Added!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021508949143629824">@LukePfister @fpedregosa Added! The French-force is strong with this one :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021434570103803911">@JBrooksBSI Added!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021434001842745346">@unsorsodicorda @roydanroy I credited Cauchy for GD, shameless French bias :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021432976612225032">@geofurb Added.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021431497247592448">@fnielsen Added.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021431171769602054">@yenhuan_li Added</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021429407813160960">@yenhuan_li Added!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021427999600766976">@yenhuan_li Added!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021427211285524480">@yenhuan_li Added!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021426182426906625">@nicoguaro Added!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021425929501970434">@ArnaudDoucet1 Corrected!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1021425896014729217">@tomrzah Added!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1020293513622245378">@tj_jarvis I was thinking about Golub-Reinsch.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1018442351554039808">@BraneRunner @marktmaclean Indeed. Finding global minimizers is hard. Generic methods exist for inst</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1018441821150765057">@probablyranvir The codes are here: https://t.co/HEP0lFhp5S</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1017458708639551489">@R4_Unit I used the log of the distance to the nearest neighbor to approximate entropy (the Wasserte</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1016358439604367360">@BraneRunner @TopologyFact For the mean curvature I think it is a non trivial result. For the heat e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1015330961737834496">@gokstudio @trekkinglemon These issues of Fourier modulus is discussed a bit in Section 2.1 of the s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1015330745504747520">@gokstudio @trekkinglemon Fourier atoms are too global to be useful for complicated shapes. They are</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1013736954335252481">@jdot The true power/interest/difficulty of using Optimal Transport normalization would be to deal w</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1013724657109872640">@jdot An issue is that it is not completely trivial to apply to test data the normalization done on </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1013103615827996672">@geofurb Orthogonal bases always oscillate so I would say artifacts always more or less look like ri</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1013012416525950976">@geofurb No, linear projection on a fixed set of functions is always bad for functions with steps, F</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1012010213061296128">@TheChrisMiles Indeed you are correct !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1011867537099886593">@baked_pumpkin78 The codes are here https://t.co/6gsD3TDsog</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1009852856722165760">@mblondel_ml @F_Vaggi @vnfrombucharest I want to be like @mblondel_ml and design loss functions with</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1009818027586654208">@ovchinnikov Actually another way to simulate the diffusion is to remove the random noise and replac</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1009816939114450944">@ovchinnikov Good question! You need to consider a system of N coupled SDEs. You then need to consid</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1009770659952648192">@ovchinnikov No they aren‚Äôt. This is why the method is so simple to use for instance in Bayesian inf</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1009696199098683397">@jodemaey Yes you are right, a more correct wording should be that FP is the PDE satisfied by the de</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1009493977295400960">@alveuz I recommend this one: https://t.co/dSn37DSh2e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1008391388302577664">@ruchirtewari The L2 norm somehow reflects a Gaussian-like assumption on the error distribution. But</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1007528223700381696">@GuillaumeG_ Yes indeed !!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1007326668439085056">@guillefix The Matlab code is here https://t.co/6gsD3TDsog I create the gif animation using imagemag</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1007326306441232384">@tvscherpenzeel Check also https://t.co/wbUY63I1x9 @PeterPikeSloan</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1007325875229118467">@quakephysics The Matlab code is here https://t.co/dBN2IkyOiz but you should not use it in real life</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1007325632370487296">@krajzeg @fleeboy @acid2 Indeed! The expert/pioneer is @PeterPikeSloan https://t.co/wbUY63I1x9</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1007267661137489920">@Varusabolo It is supposed to be a constant function on 6 small disks ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005882420086919169">@kastnerkyle @EricSchles Indeed, sorry about this, the Fast Marching on meshes is one of the few thi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005734241001529344">@heghbalz This is linked with my last answer I guess. A too weak metric might fail to be a distance </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005733434428403712">@heghbalz If you constrained discriminators in a set which is not dense in continuous functions then</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005726209437847552">@heghbalz Weaker metric means it is easier for maps to be smooth. For instance translation is not co</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005514254911508480">@jonnazar Yes exactly! It corresponds to viewing Radon measures as the dual of continuous functions.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005478510608216065">@ahmaurya Indeed! W1 metrizes the weak* topology (convergence in law).</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005465278275375107">@e_d_andersen https://t.co/hoYwpbMJze</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005462209265364992">@e_d_andersen I guess this is a good week then ! We have a quantum version of Wasserstein which is a</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1005370233899626496">@crude2refined Depends wether you care about the 1/dmin factor‚Ä¶ Over a continuous domain KL and W1</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1002916372194758656">@sigfpe Yes exactly! Let me know if you need help.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1002808038850514944">@mblondel_ml @julienmairal @arthurmensch K≈çsaku Yosida ;) https://t.co/j4gcbDvoKf</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1002800960341794818">@julienmairal @arthurmensch @mblondel_ml Indeed, but I would rather credit Moreau and Yosida for the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1002647127909072896">@sigfpe I would use the Wassertein distance of course :) using our sinkhorn divergence loss !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1002628106405011456">@thebasepoint Indeed you are right, this is a big typo, the definition of the Sinkhorn divergence sh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1000997517545365504">@PtaahGohan Fair enough, but I guess the goal of maze design is to put the target point as far as po</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/1000633096625147904">@tarinziyaee @usmanghani Thx for the clarification. Also ¬´¬†deconvolution¬†¬ª is a bit misleading, and </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/999672635423420417">@NGhoussoub Yann is a big fan of atan !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/997555927996928000">@Radegund @ProbFact You can have a look at chapter 8 of our book https://t.co/bszcV2GnL8 where we co</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/997554599535042566">@ProbFact @IgorCarron My 2 cents intuition is that KL is also a Bregman divergence and the rule of t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/997375770350817280">@drherryandmrone Meta-meta-take home message ihmo: there is no such thing as an ‚Äúoptimal‚Äù or ‚Äúbetter</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/997374712316596224">@drherryandmrone To sum up: if you have a medium scale problem and need high precision, use interior</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/997374097318400001">@drherryandmrone Very good question! Yes and no. Entropic barrier is not self concordant so it leads</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/996301228983414784">@strakaps Chapter 3 of our books deals with this https://t.co/bszcV2GnL8</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/994211340771262465">@Leo_mag_Mathe @diff_eq I am using P1 FEM aka the cotan weights Laplacian, see https://t.co/u7SEH1Gj</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/994210930937524229">@MannyDePresso If the manifold is a lie group then eigenspace of the laplacian are eigenspace of the</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/993957419863740416">@MannyDePresso There is no need for the manifold to be compact. Unless you need some specific proper</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/993804837304270849">@swiffydk Yes this is true, cancelling the constant and rescaling captures the second dominant eigen</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/993800728320860160">@swiffydk Yes exactly.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/993798823318360064">@swiffydk Actually no, the heat equation is without source term, but for the display I rescaled the </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/993798359063384065">@jfpas But these wikipedia pages do not cover what happens on a surface, hence the link to the Lapla</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/993403514809470976">@paglend Yes you are correct, the third one is not a physically working example ‚Ä¶ my dumb and simp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/992788808633667584">@PeterPikeSloan @erkaman2 Indeed ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/992691482149707776">@trekkinglemon Ok, I will make an animation showing effect of deformations.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/992684081656852480">@trekkinglemon Great idea! Don‚Äôt know how to make it graphical enough for a tweet‚Ä¶?‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/991405709731516416">@ylecun In particular the last paragraph of https://t.co/MA1lFN7SEM</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/991404401150525446">@ylecun The Wassertein distance should really be named Monge-Kantlorovitch ‚Ä¶ https://t.co/AJcbOo5p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/991243894636666880">@Patapom2 @antoche Well, I was rather watching @antoche coding during demo parties :) But I learned </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/990531605105135616">@heghbalz I think this is because you think of the stationary regime, in which case it is a Poisson </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/990528946637824000">@heghbalz Good question, would indeed be nice to vizualize. In vacuum, Maxwell‚Äôs equations reduce to</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/990516834272186368">Part 2/2. https://t.co/chYbul68GU</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/990252871789809669">@nomad421 @AScanLineTooFar I will tweet about the rendering equation soon !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/990210827394453505">@xtimv You are absolutely right, Lena image should not be used. I will do my best to correct the Num</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/990166453918846976">@p_ameline But the nasty part is discretization. It is far less obvious than with usual Fourier. Bas</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/990165933648990209">@p_ameline Spherical harmonics unfortunately depend on the choice of spherical coordinate system. Bu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/990150528754683906">@p_ameline For all data processing problems where the signal is defined on a sphere, for instance th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/989238752957685760">@daniel_bilar @d_r_dewhurst @DoGreatScience Check my previous tweet about ‚Äúhyperbolic vs parabolic‚Äù </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/988365462118567938">@nilspin The code is here. If you remove the display part, it is only a few lines of codes (grad/div</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/987716721178968064">@sigfpe Indeed ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/987609642619596802">@sigfpe A cool project would be to generalize this to 3D using quaternions. For sure will be painful</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/987607183012581376">@sigfpe Sorry didn‚Äôt read the end of the thread. Thx.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/987606788034957312">@sigfpe This seems similar (same?) as interpolating coefficients of polynomuals (the caracteristic p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/986244110452477953">@L_badikho I think this is rather kernel SVM, right? I credited Cortes for the soft margin formulati</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/986160738837368832">@Michielstock Indeed!!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/985592880562606080">@MannyDePresso @roydanroy @bguedj The PhD thesis is here https://t.co/CvS4ogWWPq</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/985592604489404422">@MannyDePresso @roydanroy @bguedj Also, the PhD thesis of Emmanuel Candes (pre-curvelets and pre-com</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/985586710116470784">@roydanroy @MannyDePresso @bguedj Non-linear approximation theory (rates and maxisets) of ridge func</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/985458142807982080">@jjvie Indeed, it is the simplest MLP.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/985448752709341185">@arimorcos I could not agree more!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/985127836750176256">@ArtificialMindX Yes the kernel has to be positive definite https://t.co/lE66B0VaCG</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/985127488664887296">@mere_mortise @jacquesdurden Indeed. There has been quite some works about reconstruction from sift </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/985109404562481157">@ArtificialMindX Of course. But thin plate is one of the most widely used for low dimensional interp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/984923526074380288">@sigfpe Tao/Vu result is an amazing tour de force, it is the culmination of years of efforts from ma</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/984922457508335616">@sigfpe Indeed, determinantal point processes are starting to be used to produce high sampling quali</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/983974526957621251">@SketchpunkLabs It is here: https://t.co/HEP0lFhp5S</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/982940932273209345">@mere_mortise @jacquesdurden There are some works of Joachim Weickert‚Äôs group that do compression by</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/982925907202625537">@mere_mortise @jacquesdurden Indeed this is what most coders do. Except that they do not quantize pi</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/982896575704895488">@jacquesdurden Eulerian vs Lagrangian üôÇ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/982896449900990464">@JeKalifa Indeed ‚Ä¶ designing state of the art coders is only about finding the good balance betwee</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/981230087386423296">@DavidStansby I think 3rd order in time is unstable and diverges exponentially. You should check, bu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/981201386200330240">@drherryandmrone Yes!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/981200223941877760">@actualmberens On periodic domains, these equations are trivial to solve using FFTs ‚Ä¶ no need for </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/981196092892176386">@DavidStansby 3rd order is not very meaningful bc it cannot be rotation invariant. For 4th order, th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/979263491507761153">@cas_group @MissionVillani @BachFrancis It‚Äôs a (admittedly private) joke ‚Ä¶ Villani is a world expe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/977897478308712453">@guillefix These are geodesics for a very irregular Riemannian metric (proportional to the image gra</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/977079393444130821">@volkuleshov @gerardbiau The paper https://t.co/z21VsfJhCI clearly makes this point, see also https:</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/977079271029137409">@volkuleshov @gerardbiau Many papers (incl. mine‚Ä¶) motivate GANs as doing density fitting with som</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/975655839976624128">@rphlypo Indeed the ‚Äú0‚Äù should be a ‚Äú1‚Äù ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/975654631962529792">@jonnazar The gauge can indeed be discontinuous and have values +inf, for instance for an hyperplane</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/975034195113279489">@Laurent_Daudet De 10^0 √† 10^3! Je vais montrer des pixels et calculer des moyennes en fait. Peut √™t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/969956836546883586">@vmbotwin It is a really basic and standard method, so probably coded in most image segmentation lib</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/969956354743066624">@PaulBalanca Well if you use a deep net to classify the pixels, you parametrize phi by a deep net :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/969506155616788480">@Dirque_L @BorweinJ @HerreWiersma Indeed this was the meaning of ¬´¬†the two extreme cases¬†¬ª at the bo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/969329662651486208">@StillFuturr @GuillaumeG_ @goodfellow_ian Of course I understood this. It is important to educate pe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/969300990292844544">@StillFuturr @GuillaumeG_ @goodfellow_ian I think it is not a good thing to rename all min/max probl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/968858131265720320">@learningML532 Indeed !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/968543848048848896">@xtimv The same idea leads to a very simple scheme for SVM with a ridge squared l2  regularization.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/968543527419498496">@xtimv For more intricate functional, you cannot solve the primal or the dual alone, you have to go </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/968542881580494848">@xtimv This NT explains the main idea for TV (this idea was first used in a famous paper of Antonin </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/968542456949198848">@xtimv The canonical examples are total variation regularization (where A is the gradient operator w</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/967074934919921664">@kmett I could not agree more!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/966683004155330560">@lostella You are correct, the equivalence was really for the ¬´¬†nabla¬†¬ª of g and g* being mutually i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/965684597953650693">@guillefix Indeed! Thx</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/965555647948427264">@Ptaah_LSG @kevd42 Sorry I meant ¬´¬†in 3d¬†¬ª‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/965553109945737216">@Ptaah_LSG @kevd42 I agree with you that the skeleton wording is not so good in 2d.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/965552790062993408">@kevd42 @Ptaah_LSG Yes exactly. It defines hypersurfaces (points with 2 equal neighbors) meeting alo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/964558171581898752">@GCLinderman Indeed, sample complexity only tells about the behavor of the mean of the empirical dis</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/964459142806167553">@mere_mortise The ¬´¬†static¬†¬ª optimal transport is the optimal assignement which is just the sorting </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/964414249656184833">@R3RT0 alpha denotes the histograms of the pixels of the displayed images. So alpha_0 is the histogr</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/964227050428157957">@goodfellow_ian @GuillaumeG_ Nope ‚Ä¶ I guess one needs to restort to more trivial averaging strateg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/964111672859340800">@Dirque_L The paper of Bach and Weed exposes in details how the geometry of support impacts the rate</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/964078749712035840">@dtsbourg @JustinMSolomon Indeed! More details in our book https://t.co/9ZlWm6KI9p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/963683624544587777">@davidjayharris @GuillaumeG_ Not really. Implicit Euler requires to solve a non-linear equation at e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/962799181575442432">@ogrisel This is more or less what is done in this paper https://t.co/UZJhQPYgaP Extrapolation someh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/962758926965071873">@loretoparisi For convex problems you can look at my https://t.co/9xi15J0h40 in the Optimization sec</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/962741024744460288">@loretoparisi For the convex case, primal-dual first order solvers provably work (among which ‚ÄúChamb</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/962352959941218304">@ogrisel I will check but most certainly it will.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/962352507434557440">@fpedregosa Do not spoil my forthcoming tweets ;)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/961658917779632132">@trekkinglemon And also to perform planar graph embedding for surface parameterization (Tutte embedd</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/960141352683098112">@Dirque_L Haha indeed ‚Ä¶ I mixed 2 slides ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/958374974527549440">@ogrisel Indeed one can use extrapolation to speedup the local rates. But one might loose global con</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/956937074204127232">@F_Vaggi @JuliaLanguage Well, coding a Runge-Kutta 4/3 is simple in comparison to implementing by ha</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/956935422868967431">@F_Vaggi Ah this is a great idea for future tweets! If everyone doing adjoint state methods for opti</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/955021385549668352">@trekkinglemon Indeed. Actually the legend is flawed ‚Ä¶ In think the first image is psi_3 is then t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/954404237173297152">@Philippe_Ciuciu You mean considering tetrahedral mesh in place of triangulation? I guess the theore</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/953006022641242114">@framrus An important application is phase retrieval where the contraints are affine and circles (he</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/952541046747037696">@vakibs @StphTphsn Not really. ICA or PCA would corresponds to looking for both the coefficients and</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/952499632151646208">@vakibs @StphTphsn Since the basis is assumed to be orthogonal, the coefficients are simply the inne</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/952491696843055104">@StphTphsn The blue part is the expression with inner products of the ortho projection on a linear s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/952138816466444288">@roger_mansuy Indeed! Algorithms operating on planar triangulations (eg computing Delaunay triangula</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/951863989935529984">@Lumen_AI Check @JeKalifa replying on my Sobolev tweet with a Besov one!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/949672271840530433">@optiML I fully agree! We used it to show linear rate of Sinkhorn for unbalanced OT.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/949639288559423490">@dohmatobelvis Nice review paper of Lemmens and Nussbaum https://t.co/W8HTqujnuY</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/949549212253020166">@tarinziyaee @mtyka I will feature complex unitary matrices in another tweet ! Rainbow colors to be </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/949369033840840704">@mere_mortise https://t.co/Wx9Od8OJid</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/948627829926883328">@ogrisel @dohmatobelvis Actually 2 lines are enough :) Just depends on how much you are willing to s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/946392456576323584">@drherryandmrone @SMAI_media Yes indeed, otherwise look for ¬´¬†pointed GH convergence¬†¬ª.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/945689715147202560">@kiaderouiche It is a model of crowd motion with congestion proposed by Bertrand Maury, Aude Roudnef</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/944609444075360258">@Dirque_L Great, thanks!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/944593410001915905">@Dirque_L I am curious ‚Ä¶ I think there is not scheme with 1/k^2 rate and linear rate for strongly </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/943936793942032384">@fpedregosa @kysucix Ahaha ‚Ä¶ you are stochastic and I am batch!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/943896400395489281">@TheShubhanshu @fpedregosa https://t.co/dSn37DSh2e</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/943896043955073024">@e_devijver @melinaGALLOPIN @julienmairal https://t.co/9ZlWm6KI9p</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/943445336948924416">@inversed_ru For the quadratic Renyi there is however a fast algorithm bc one can compute the orthog</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/943229557682909184">@inversed_ru You can use any strictly convex entropy functional (including eg Renyi‚Äôs) to define reg</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/943131939661328384">@Michielstock Yes its is exactly a maxent on couplings with prescribed marginals. It is the so-calle</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/942327554693844992">@yablak Very good question, I have not idea!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/942019601680011264">@OlivierVerdier @ogrisel My mistake, you are correct, Fisher metric does not depend on the actual re</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941678833475997696">@srchvrs If phi is strongly convex then D2phi is positive definite so it defines a proper Euclidean </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941675342036721665">@dcoeurjo K-S is not so nice in 2d, I do not think it even defines a distance ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941669883397492737">@dcoeurjo This is great! You could also report the Wassertein distance to the uniform density, using</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941668840999669760">@fromagedebrebis Convergence proof works for any p, the argument relies on the Kurdyka-≈Åojasiewicz i</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941663126273093633">@jonnazar But at least the algorithms (proximal point, forward bacwkard, conditional gradient, etc) </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941662589523808256">@jonnazar Yes you can define prox theory for any bregman divergence and in fact even for generic met</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941584845075025920">@srchvrs What I meant: https://t.co/DrAwLHZ2Us</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941357980074889217">@OlivierVerdier @ogrisel Which just made it to Inventiones today !! This is amazing, congratulation.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941357035093987328">@OlivierVerdier @ogrisel A much more refined analysis was done by @GSavare in their landmark paper h</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941356230437691393">@OlivierVerdier @ogrisel Also we did a series of papers to define a proper way to interpolate betwee</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941355525509468160">@OlivierVerdier @ogrisel A video to illustrate what I meant by horizontal vs vertical ! https://t.co</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941354692340273152">@OlivierVerdier @ogrisel It corresponds to dealing with measures using ¬´¬†vertical¬†¬ª (for Fisher) vs </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941354301921820672">@OlivierVerdier @ogrisel Fisher depends on a choice of a reference measure to compute the density. U</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/941299957163839488">@fromagedebrebis It is used a lot. For p=0, look for ¬´¬†iterative hard thresholding¬†¬ª which is Forwar</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/940967441118658560">@roydanroy @ogrisel @RogerGrosse Works of Yann Olivier are really great. Funny he used to work on Op</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/940966298871386113">@TheShubhanshu The source codes and the theoretical explanations can be found at https://t.co/5O7mrZ</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/940897017714356224">@ogrisel So transport can handle evolutions of measures which are mutually singular (eg a curve that</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/940896313595580417">@ogrisel Also Fisher depends on a choice of a reference measure (typically Lebesgue) whereas transpo</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/940895805921251328">@ogrisel The issue with Fisher geometry is with degenerate (rank d√©ficient) covariance. This can be </a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/939907513000054784">@artix41 All the codes to reproduce OT related figures are on the github of our book project. https:</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/939801502109093888">@jonnazar Great question, will generate the animation of the copula asap.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/938077039650852864">@dohmatobelvis Of course, it is just that sometimes you are lucky and your function is DC. I will tw</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/937973151065083904">@dohmatobelvis You just take the positive/negative part of f‚Äô‚Äô. This is most likely in standard text</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/936970739189714944">@TheShubhanshu Yes exactly. It is an orthogonal projector on some level set of the function. The FnT</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/936566289543385088">@hrheydarian It gives you an estimator of the entropy of the underlying density from which the point</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/936284329466003456">@hrheydarian Just 1 is enough to get a consistent estimator. Increasing the number decreases varianc</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/935580822970818561">@jonnazar Yes exactly, this is a proximal step for a generic distance. Proximal algorithm are genera</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/933579266457001984">@PierreAlquier Indeed :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/930521185644838912">@LenaicCsl No, it just that the dual potentials are moving crazy at the beginning, when the step siz</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/930061426709934080">@fxcoudert I think this documented in the litterature. This accumulation vanishes when increasing #s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/929754371444928512">@jacquesdurden @Laurent_Daudet The proof caries over verbatim for subgaussian. If the entries are ii</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/929676203774144512">@Laurent_Daudet Eq (25) in https://t.co/HXUDEzRr8f</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/929675830757863424">@Laurent_Daudet Optimality conditions for l1 require control on the l^inf norm of the correlation an</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/929338376871927808">@hrheydarian @MathsParis Just fixed it. Thanks.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/929336992281833474">@mblondel_ml @LenaicCsl Thanks! Forgot the link ‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/927506385868349440">@Dirque_L This basically why entropy is useful in high dimension, bc of the favorable 1/sqrt(n) samp</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/927505951615324160">@Dirque_L We use it here https://t.co/soS8taGO2l to show that Sinkhorn divergences interpolate btw O</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/927437688831127552">@Maciej_Kula @chrisemoody @xbresson @mblondel_ml It is a general scheme, an energy f(m) over measure</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/927437477714968576">@Maciej_Kula @chrisemoody @xbresson @mblondel_ml Ah I am not an expert, but maybe :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/927427742307459072">@Maciej_Kula @chrisemoody @xbresson @mblondel_ml This is what we do in practice when differentiating</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/927427335451578369">@Maciej_Kula @chrisemoody @xbresson @mblondel_ml If you parametrize the cost matrix parametrizing th</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/927183298400522240">@mblondel_ml Apparently it was never published! Thanks for the pointer!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/926085132917919744">@IntuitMachine We will release a book on Computational Optimal Transport soon, including discussions</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/923587249438121984">@daniel_bilar I am putting the codes to reproduce the figures of my tweets here https://t.co/HEP0lFh</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/923172027011346432">@odakyildiz We will ask to PGMO, but I doubt‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/922521124260532224">@nueluno Yes indeed. OT is the right way to manipulate the color distribution. But not really to war</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/922470379884597248">@nueluno Diffeomorphism-based registration methods work better for images and shapes.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/922470119711952897">@nueluno OT works best for histrograms and distributions. Treating images as histograms is bad pract</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/922469787413950467">@kiaderouiche Yes there are quantum (matrix-valued) generalizations of OT. Send me an email for refs</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/922060796166893569">@Michielstock You might also want to ask @BrunoLevy01 who is the expert. He has amazing videos.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/922057779048828928">@Michielstock Several papers by Merigot, de Goes, Levy, Mirebeau, Galouet, etc. Landmark theory pape</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/922043821868929025">@mblondel_ml Log-sum-exp rules !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/922043478846164992">@mblondel_ml Indeed‚Ä¶ but ressembles was less catchy !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/918542129655435265">@jacquesdurden Heat (Gauss convol) would become inf-convol with x^2. They both regularize. Gauss and</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/915857536619859968">@IgorCarron @Laurent_Daudet @franceisai @nvidia @LightOnIO @GDRBioComp @JoDureau @Ourghanlian @Ludov</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/912928627871948801">@cbouveyron Non d√©sol√©‚Ä¶ ceci dit c‚Äôest tr√®s simple‚Ä¶</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/905917716523081728">@chrisemoody For the general case, it is the Bures metric on the covariances, I tweeted about it a w</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/905917584037605376">@chrisemoody Yes for diagonal covariances it is the same, each dimension gets interpolated independe</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/905850183279415296">@dohmatobelvis So now you are ready to study Lasserre hierarchy and SOS relaxation !</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/905550460744081409">@dohmatobelvis A <em>single</em> iteration of Sinkhorn with bandwidth eps is similar to RKHS of width sigma</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/905549458854879232">@dohmatobelvis I tried to explain here https://t.co/I3y69GhtuS the differences btw primal/dual formu</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/904072986248044544">@sigfpe @IgorCarron Ah sorry, I meant the Fenchel‚ÄìLegendre transform from convex analysis, not the o</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/904067434218446849">@HamzaGauss For instance, this is Prop. 2.1 of https://t.co/ZqTZ9kxWeq</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/904055581996191744">@SimonBillouet Claude Shannon, Leonid Kantorovitch, Alan Turing</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/904054803751473152">@SimonBillouet Ingrid Daubechies, Claire Voisin, Maryam Mirzakhani</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/893910053069213696">@Philippe_Ciuciu @miccai2017 No, but if it is only shape data integrating an OT loss should be fairl</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/891681054746177540">@wittawatj These dual RKHS norms are also called Maximum Mean Discrepency https://t.co/5Kg3mbYuT4</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/891679752020426752">@wittawatj In this case the dual norm is also an RKHS norm using the ‚Äúinverse‚Äù kernel. An exemple is</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/891654141470179329">@wittawatj But dual norms are much more expensive to compute (no closed form, need to solve a concav</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/891653843955658753">@wittawatj Weak topology takes into account the geometry of the ground space X. So dual norms can ha</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/878682854347223040">@IgorCarron @InHenriPoincare @SocMathFr They will most likely be added on Monday by IHP. I guess.</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/872355659144155136">@rphlypo Thanks!!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/870937984224428032">@docmilanfar Thx for the pointer. We will update the preprint to mention works on information theore</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/870528901453406210">@F_Vaggi Yes we will do it asap on https://t.co/p5KiI67gV0</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/867100889437401092">@PierreAlquier Haha indeed Monge-Cuturi :)</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/860885925139820546">@F_Vaggi Of course, no problem!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/836248320125321219">@KloudStrife indeed, Marco Cuturi is working in this direction. You should contact him if you are in</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/831773085242159104">@andisspam A description of the method with reference is provided in my https://t.co/rEcHTxAk4X : ht</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/831772900260769792">@andisspam sorry for late reply. Thanks for the reference, will read it!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/831120582322356225">@andisspam its dynamic is interesting, a function becoming singular (diracs) and is not fully unders</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/831120273860608000">@andisspam its a classical algorithm for deconvolution (recovering high frequencies from a blurred s</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/827261716744396802">@Laurent_Daudet @jacquesdurden make Optimal Transport great again!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/793935684503478272">@roger_mansuy l‚Äôexpos√© de Buc n‚Äôa pas √©t√© film√©, des √©l√®ves de seconde incroyables avec des question</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/793935195762294784">@roger_mansuy @_CIRM La conf√©rence du CIRM est retransmise en direct sur la chaine du CIRM https://t</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/782685601317388288">@InHenriPoincare @IgorCarron Indeed, my mistake, sorry for the trouble!</a></li>
  <li><a href="https://twitter.com/gabrielpeyre/status/782671915911671808">@IgorCarron I meant 14:00. It is next Thursday, the 6th.</a></li>
</ul>
:ET